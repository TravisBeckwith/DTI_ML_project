#!/usr/bin/env bash
set -euo pipefail
#
# Integrated DTI Processing Pipeline - Storage-Optimized Version with ML Registration
# Combines scripts 1, 2, 4, 3 in sequence with intelligent storage management
# Enhanced with machine learning-based registration techniques
# v1.3-ml-enhanced
#
# ERROR HANDLING PHILOSOPHY:
# -------------------------
# Functions in this pipeline follow one of two error-handling contracts:
#
#   FATAL (return 1 causes pipeline to abort for this subject):
#     run_basic_preprocessing, run_eddy_and_bias_correction
#     These are core stages without which no useful output is possible.
#
#   ADVISORY (return 1 is logged but processing continues):
#     run_synb0, run_posthoc_refinement, run_connectivity_analysis,
#     run_noddi_estimation, enhance_brain_mask, check_residual_distortions,
#     all ML registration functions (VoxelMorph, SynthMorph, ANTs).
#     These either have traditional fallbacks or produce optional outputs.
#
# When adding new functions, choose one of these contracts and document it
# in the function's header comment.
#
# CO-LOCATED PYTHON SCRIPTS:
# -------------------------
# The large Python scripts (voxelmorph_registration.py, noddi_fitting.py)
# can be placed alongside this script for independent linting/testing.
# If not found, they are generated inline as a fallback.
#
# OPERATIONAL MODES:
# -------------------------
#   --dry-run    Preview what would run for each subject without executing
#   --resume     Resume from last successful checkpoint (finer-grained)
#   --container-cmd <cmd>   Override container runtime (docker/singularity/apptainer)
#
# COMPANION FILES:
# -------------------------
#   test_helpers.sh          Unit tests for pure-logic helper functions
#   voxelmorph_registration.py   Externalized ML registration script
#   noddi_fitting.py         Externalized NODDI fitting script
#

# ============================================================================
# USER CONFIGURATION — SET THESE PATHS BEFORE RUNNING
# ============================================================================
# All three paths below MUST be set. The pipeline will abort if any are empty.
#
#   BIDS_DIR        — Root of your BIDS-formatted dataset (contains sub-*/dwi/, sub-*/anat/)
#   STORAGE_FAST    — Fast storage for Synb0, MRtrix3, post-hoc outputs, and QC reports
#   STORAGE_LARGE   — Large-capacity storage for FreeSurfer recon-all outputs
#
# Processing intermediates and logs are always written under ${BIDS_DIR}/derivatives/.
#
# Example (WSL with mapped Windows drives):
#   BIDS_DIR="/mnt/c/myproject/BIDS"
#   STORAGE_FAST="/mnt/e/myproject"
#   STORAGE_LARGE="/mnt/f/myproject"
#
# Example (native Linux):
#   BIDS_DIR="/data/study01/BIDS"
#   STORAGE_FAST="/scratch/study01"
#   STORAGE_LARGE="/archive/study01"
#
# You can also pass these on the command line instead:
#   ./ML_v7_beta.sh -b /path/to/BIDS --storage-fast /path --storage-large /path
# ============================================================================

USER_BIDS_DIR=""          # <-- EDIT: Path to your BIDS directory
USER_STORAGE_FAST=""      # <-- EDIT: Fast SSD for pipeline outputs
USER_STORAGE_LARGE=""     # <-- EDIT: Large drive for FreeSurfer outputs

# ============================================================================
# END OF USER CONFIGURATION
# ============================================================================

# ============================================================================
# PYTHON ENVIRONMENT SETUP - Ensures Consistent ML Environment  
# ============================================================================

# Prefer active virtualenv/conda (where ML packages live), fall back to system python.
# The neuroimaging_env venv has TensorFlow, VoxelMorph, etc. — FSL Python does not.
if [ -n "${VIRTUAL_ENV:-}" ] && [ -x "${VIRTUAL_ENV}/bin/python" ]; then
    export PYTHON_EXECUTABLE="${VIRTUAL_ENV}/bin/python"
elif [ -n "${CONDA_PREFIX:-}" ] && [ -x "${CONDA_PREFIX}/bin/python" ]; then
    export PYTHON_EXECUTABLE="${CONDA_PREFIX}/bin/python"
else
    export PYTHON_EXECUTABLE="$(which python3 2>/dev/null || which python 2>/dev/null || echo python3)"
fi
# Keep FSL in PATH but after conda/venv
export PATH="$PATH:/usr/local/fsl/bin"
# Append FSL packages so venv packages take precedence
export PYTHONPATH="${PYTHONPATH:-}:/usr/local/fsl/lib/python3.12/site-packages"

# Verify Python environment at startup
echo "[$(date '+%Y-%m-%d %H:%M:%S')] [ML] Using Python: $PYTHON_EXECUTABLE"
$PYTHON_EXECUTABLE --version

# Neuroimaging toolchain setup (FSL, MRtrix3, FreeSurfer, ANTs)
export FSLDIR=/usr/local/fsl
if [ -f "$FSLDIR/etc/fslconf/fsl.sh" ]; then
  # Third-party env scripts may trip -e or -u
  set +e +u
  . "$FSLDIR/etc/fslconf/fsl.sh" >/dev/null 2>&1 || true
  # restore strict mode
  set -euo pipefail
fi

export FREESURFER_HOME=/usr/local/freesurfer
if [ -f "$FREESURFER_HOME/SetUpFreeSurfer.sh" ]; then
  set +e +u
  export FS_FREESURFERENV_NO_OUTPUT=1
  export SUBJECTS_DIR=${SUBJECTS_DIR:-$FREESURFER_HOME/subjects}
  . "$FREESURFER_HOME/SetUpFreeSurfer.sh" >/dev/null 2>&1 || true
  set -euo pipefail
fi

# ANTs commonly installs to /usr/local/bin on this system
export ANTSPATH=/usr/local/bin

# Prepend binaries so checks find the right tools
export PATH="/home/trav/mrtrix3/bin:$FSLDIR/bin:$FREESURFER_HOME/bin:$ANTSPATH:/usr/local/bin:/usr/bin:$PATH"

# ============================================================================
# ML/GPU ENVIRONMENT SETUP - RTX 3070 Configuration
# ============================================================================

# CUDA/GPU Configuration  
export CUDA_VISIBLE_DEVICES=0
export CUDA_HOME=/usr/local/cuda-12.3
export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:${LD_LIBRARY_PATH:-}
export TF_ENABLE_ONEDNN_OPTS=0
export TF_GPU_ALLOCATOR=cuda_malloc_async

# Optional: only force-override if explicitly requested
if [ "${FORCE_ML_AVAILABLE:-false}" = true ]; then
  export ML_DEPENDENCIES_SATISFIED=true
  export VOXELMORPH_AVAILABLE=true
  export SYNTHMORPH_AVAILABLE=true
  export TENSORFLOW_AVAILABLE=true
fi

export TF_CPP_MIN_LOG_LEVEL=3
export PYTHONWARNINGS="ignore"
export PYTHONIOENCODING=UTF-8
export LANG=C.UTF-8; export LC_ALL=C.UTF-8


if $PYTHON_EXECUTABLE -c "import tensorflow as tf; print(len(tf.config.list_physical_devices('GPU')))" 2>/dev/null | grep -q '^[1-9]'; then
    export GPU_AVAILABLE=true
else
    export GPU_AVAILABLE=false
fi

echo "[$(date '+%Y-%m-%d %H:%M:%S')] [ML] GPU environment configured: RTX 3070 ready"
#echo "[$(date '+%Y-%m-%d %H:%M:%S')] [ML] All ML dependencies confirmed working"

# Test ML environment at startup
echo "[$(date '+%Y-%m-%d %H:%M:%S')] [ML] Testing ML environment..."
$PYTHON_EXECUTABLE - <<'PY' 2>/dev/null || true
import os, warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore')

print('ML startup smoke test:')
# TensorFlow
try:
    import tensorflow as tf
    gpus = len(tf.config.list_physical_devices('GPU'))
    print(f"  TensorFlow {tf.__version__} with {gpus} GPU(s)")
except Exception as e:
    print(f"  TensorFlow import failed: {e}")

# VoxelMorph
try:
    import voxelmorph
    print("  VoxelMorph import: OK")
except Exception as e:
    print(f"  VoxelMorph import failed (will re-check later): {e}")

# scikit-learn
try:
    import sklearn
    print("  scikit-learn import: OK")
except Exception as e:
    print(f"  scikit-learn import failed: {e}")
PY

SCRIPT_VERSION="1.3-ml-enhanced"

# --- Helper Functions ---
log() {
    local level="${1:-INFO}"
    shift
    local message="$*"
    local ts_human="[$(date '+%Y-%m-%d %H:%M:%S')]"
    
    if [ -t 1 ]; then
        local RED='\033[0;31m'
        local YELLOW='\033[1;33m'
        local GREEN='\033[0;32m'
        local BLUE='\033[0;34m'
        local CYAN='\033[0;36m'
        local NC='\033[0m'
    else
        local RED='' YELLOW='' GREEN='' BLUE='' CYAN='' NC=''
    fi
    
    case $level in
        ERROR) echo -e "${ts_human} ${RED}[ERROR] ${message}${NC}" >&2 ;;
        WARN)  echo -e "${ts_human} ${YELLOW}[WARN]  ${message}${NC}" ;;
        OK)    echo -e "${ts_human} ${GREEN}[OK]    ${message}${NC}" ;;
        INFO)  echo -e "${ts_human} ${BLUE}[INFO]  ${message}${NC}" ;;
        STAGE) echo -e "${ts_human} ${CYAN}[STAGE] ${message}${NC}" ;;
        ML)    echo -e "${ts_human} ${CYAN}[ML]    ${message}${NC}" ;;
        *)     echo -e "${ts_human} [INFO]  ${message}" ;;
    esac
    
    # Structured JSONL log (machine-readable) — emitted alongside human log.
    # Extract subject ID from message if present (pattern: [sub-XXX])
    if [ -n "${JSONL_LOG:-}" ]; then
        local ts_iso
        ts_iso="$(date -Iseconds 2>/dev/null || date '+%Y-%m-%dT%H:%M:%S')"
        local sub_id=""
        if [[ "$message" =~ \[([a-zA-Z0-9_-]+)\] ]]; then
            sub_id="${BASH_REMATCH[1]}"
        fi
        # Escape double quotes and backslashes for JSON safety
        local safe_msg="${message//\\/\\\\}"
        safe_msg="${safe_msg//\"/\\\"}"
        printf '{"ts":"%s","level":"%s","subject":"%s","msg":"%s"}\n' \
            "$ts_iso" "$level" "$sub_id" "$safe_msg" >> "$JSONL_LOG"
    fi
}

# Safely convert a value to integer for arithmetic comparisons.
# Strips decimals, defaults to 0 on empty/non-numeric input.
safe_int() {
    local v="${1:-0}"
    v="${v%%.*}"           # strip anything after decimal
    # Extract leading optional minus sign followed by digits
    if [[ "$v" =~ ^-?[0-9]+ ]]; then
        v="${BASH_REMATCH[0]}"
    else
        v=0
    fi
    echo "${v:-0}"
}


# Safe directory change: pushd with automatic restore on function return.
# Usage: safe_cd "$dir" || return 1
# Always call safe_cd_return before any return from the calling function.
safe_cd() {
    pushd "$1" > /dev/null 2>&1
}
safe_cd_return() {
    popd > /dev/null 2>&1 || true
}

# ============================================================================
# Timed stage wrapper — logs elapsed wall-clock time for any stage function.
# Usage: timed_stage "label" command arg1 arg2 ...
# ============================================================================
timed_stage() {
    local label="$1"; shift
    local _ts_start
    _ts_start=$(date +%s)
    "$@"
    local rc=$?
    local _ts_elapsed=$(( $(date +%s) - _ts_start ))
    log "INFO" "[timing] ${label}: ${_ts_elapsed}s ($(( _ts_elapsed / 60 ))m$(( _ts_elapsed % 60 ))s)"
    
    # Append to JSONL with duration field
    if [ -n "${JSONL_LOG:-}" ]; then
        local ts_iso
        ts_iso="$(date -Iseconds 2>/dev/null || date '+%Y-%m-%dT%H:%M:%S')"
        printf '{"ts":"%s","level":"TIMING","subject":"","stage":"%s","duration_s":%d}\n' \
            "$ts_iso" "$label" "$_ts_elapsed" >> "$JSONL_LOG"
    fi
    return $rc
}

# ============================================================================
# Subject lock — prevents concurrent processing of the same subject.
# Uses flock advisory locks; non-blocking.
# ============================================================================
_LOCK_FD=200  # file descriptor for flock

acquire_subject_lock() {
    local sub=$1
    local lockfile="${WORK_DIR}/${sub}/.pipeline.lock"
    mkdir -p "$(dirname "$lockfile")"
    eval "exec ${_LOCK_FD}>\"$lockfile\""
    if ! flock -n $_LOCK_FD; then
        log "ERROR" "[${sub}] Another pipeline instance is already processing this subject"
        log "ERROR" "[${sub}] Lock file: $lockfile"
        return 1
    fi
    log "INFO" "[${sub}] Acquired processing lock"
    return 0
}

release_subject_lock() {
    flock -u $_LOCK_FD 2>/dev/null || true
}

# ============================================================================
# Failure context capture — run a stage and save stderr on failure.
# Usage: capture_stage_failure "sub" "stage_name" command args...
# ============================================================================
capture_stage_failure() {
    local sub=$1; shift
    local stage_name=$1; shift
    local err_file="${LOG_DIR}/${sub}_${stage_name}_failure.log"
    local out_file
    out_file=$(mktemp "${WORK_DIR}/${sub}/.${stage_name}_output.XXXXXX" 2>/dev/null || echo "/tmp/${sub}_${stage_name}_out.$$")
    
    if "$@" > "$out_file" 2>&1; then
        rm -f "$out_file"
        return 0
    else
        local rc=$?
        cp "$out_file" "$err_file"
        log "ERROR" "[${sub}] ${stage_name} failed (exit code $rc) — see $err_file"
        # Show last few lines in the main log
        tail -5 "$err_file" 2>/dev/null | while IFS= read -r line; do
            log "ERROR" "[${sub}]   $line"
        done
        rm -f "$out_file"
        return $rc
    fi
}

# ============================================================================
# Periodic disk space gate — call before each major stage.
# Returns 1 (and logs) if space is critically low.
# ============================================================================
check_disk_before_stage() {
    local sub=$1
    local stage=$2
    local min_gb=${3:-30}
    
    if ! check_disk_space "$(dirname "$BIDS_DIR")" "$min_gb"; then
        log "ERROR" "[${sub}] Insufficient disk space before ${stage} (need ${min_gb}GB free)"
        return 1
    fi
    return 0
}

# ============================================================================
# Input validation — comprehensive pre-flight check before any processing.
# Runs once on all subjects; reports all problems up front.
# ============================================================================
validate_subject_inputs() {
    local sub=$1
    local errors=0
    
    local dwi="${BIDS_DIR}/${sub}/dwi/${sub}_dwi.nii.gz"
    local bval="${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bval"
    local bvec="${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bvec"
    local t1="${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz"
    
    # Check DWI exists
    if [ ! -f "$dwi" ]; then
        log "ERROR" "[${sub}] VALIDATION: DWI file missing: $dwi"
        errors=$((errors + 1))
    fi
    
    # Check gradient files exist
    if [ ! -f "$bval" ]; then
        log "ERROR" "[${sub}] VALIDATION: bval file missing: $bval"
        errors=$((errors + 1))
    fi
    if [ ! -f "$bvec" ]; then
        log "ERROR" "[${sub}] VALIDATION: bvec file missing: $bvec"
        errors=$((errors + 1))
    fi
    
    # Check T1w for connectivity pipeline
    if [ "$RUN_CONNECTOME" = true ] && [ ! -f "$t1" ]; then
        log "WARN" "[${sub}] VALIDATION: T1w missing — FreeSurfer/connectivity will fail: $t1"
    fi
    
    # Validate bval/bvec dimensions match DWI 4th dimension
    if [ -f "$dwi" ] && [ -f "$bval" ] && [ -f "$bvec" ]; then
        local n_vols
        n_vols=$(mrinfo "$dwi" -size 2>/dev/null | awk '{print $4}' || echo "")
        if [ -n "$n_vols" ] && [ "$n_vols" -gt 0 ] 2>/dev/null; then
            local n_bval
            n_bval=$(wc -w < "$bval" 2>/dev/null | tr -d ' ')
            local n_bvec_cols
            n_bvec_cols=$(head -1 "$bvec" 2>/dev/null | wc -w | tr -d ' ')
            
            if [ -n "$n_bval" ] && [ "$n_bval" -ne "$n_vols" ]; then
                log "ERROR" "[${sub}] VALIDATION: bval count ($n_bval) != DWI volumes ($n_vols)"
                errors=$((errors + 1))
            fi
            if [ -n "$n_bvec_cols" ] && [ "$n_bvec_cols" -ne "$n_vols" ]; then
                log "ERROR" "[${sub}] VALIDATION: bvec columns ($n_bvec_cols) != DWI volumes ($n_vols)"
                errors=$((errors + 1))
            fi
        fi
        
        # Basic NIfTI header sanity check
        local dims
        dims=$(mrinfo "$dwi" -ndim 2>/dev/null || echo "0")
        if [ "$dims" -lt 4 ] 2>/dev/null; then
            log "ERROR" "[${sub}] VALIDATION: DWI has $dims dimensions (expected 4)"
            errors=$((errors + 1))
        fi
    fi
    
    return $(( errors > 0 ? 1 : 0 ))
}

validate_all_inputs() {
    local -a subjects=("$@")
    local total_errors=0
    local failed_subs=()
    
    log "INFO" "================================================="
    log "INFO" "PRE-FLIGHT INPUT VALIDATION"
    log "INFO" "================================================="
    
    for sub in "${subjects[@]}"; do
        if ! validate_subject_inputs "$sub"; then
            ((total_errors++))
            failed_subs+=("$sub")
        fi
    done
    
    if [ $total_errors -gt 0 ]; then
        log "WARN" "Validation found problems in ${total_errors} subject(s): ${failed_subs[*]}"
        log "WARN" "These subjects will likely fail during processing."
        log "INFO" "Continuing with all subjects — failed ones will be skipped at the relevant stage."
    else
        log "OK" "All ${#subjects[@]} subject(s) passed input validation"
    fi
    
    return 0  # Always continue (just informational)
}

# ============================================================================
# Dry-run support — gates actual execution when DRY_RUN=true
# Usage: if $DRY_RUN; then dry_run_skip "stage_name" "$sub"; return 0; fi
# ============================================================================
dry_run_skip() {
    local stage=$1
    local sub=${2:-""}
    log "INFO" "[DRY-RUN] Would run: ${stage}${sub:+ for $sub}"
}

# ============================================================================
# Container abstraction — supports Docker, Singularity, and Apptainer
# ============================================================================
detect_container_runtime() {
    if [ -n "${CONTAINER_CMD:-}" ]; then
        # User-specified override
        log "INFO" "Using user-specified container runtime: $CONTAINER_CMD"
        return 0
    fi
    
    if command -v docker &>/dev/null && docker ps &>/dev/null 2>&1; then
        CONTAINER_CMD="docker"
    elif command -v docker &>/dev/null && sudo docker ps &>/dev/null 2>&1; then
        CONTAINER_CMD="sudo docker"
    elif command -v singularity &>/dev/null; then
        CONTAINER_CMD="singularity"
    elif command -v apptainer &>/dev/null; then
        CONTAINER_CMD="apptainer"
    else
        CONTAINER_CMD=""
    fi
    export CONTAINER_CMD
}

# Run a container image with bind mounts.
# Usage: run_container <image> <mounts_array_name> [extra_args...]
# mounts is a bash associative array: mounts[host_path]=container_path
# For Docker: -v host:container; for Singularity: --bind host:container
run_container() {
    local image=$1; shift
    local -n _mounts=$1; shift
    local extra_args=("$@")
    
    if [ -z "$CONTAINER_CMD" ]; then
        log "ERROR" "No container runtime available (tried docker, singularity, apptainer)"
        return 1
    fi
    
    case "$CONTAINER_CMD" in
        *docker*)
            local docker_args=("run" "--rm")
            for host_path in "${!_mounts[@]}"; do
                docker_args+=("-v" "${host_path}:${_mounts[$host_path]}")
            done
            # NOTE: --user may cause permission failures with some images.
            # Remove if synb0-disco fails with permission errors.
            docker_args+=("--user" "$(id -u):$(id -g)")
            docker_args+=("$image")
            docker_args+=("${extra_args[@]}")
            $CONTAINER_CMD "${docker_args[@]}"
            ;;
        singularity|apptainer)
            local bind_args=""
            for host_path in "${!_mounts[@]}"; do
                [ -n "$bind_args" ] && bind_args+=","
                bind_args+="${host_path}:${_mounts[$host_path]}"
            done
            # Convert docker image reference to SIF if needed
            local sif_image="$image"
            if [[ "$image" != *.sif ]]; then
                local sif_cache="${WORK_DIR}/.sif_cache"
                mkdir -p "$sif_cache"
                local sif_name
                sif_name=$(echo "$image" | tr '/:' '_')
                sif_image="${sif_cache}/${sif_name}.sif"
                if [ ! -f "$sif_image" ]; then
                    log "INFO" "Building Singularity image from $image (first time only)..."
                    $CONTAINER_CMD build "$sif_image" "docker://$image" || return 1
                fi
            fi
            $CONTAINER_CMD exec --bind "$bind_args" "$sif_image" "${extra_args[@]}"
            ;;
        *)
            log "ERROR" "Unsupported container runtime: $CONTAINER_CMD"
            return 1
            ;;
    esac
}


check_disk_space() {
    local path=$1
    local min_gb=${2:-50}  # Default 50GB minimum
    
    if [ -d "$path" ]; then
        local available_gb=$(df -BG "$path" | tail -1 | awk '{print $4}' | sed 's/G//')
        available_gb=$(safe_int "$available_gb")
        if [ "$available_gb" -lt "$min_gb" ]; then
            log "WARN" "Low disk space on $path: ${available_gb}GB available (minimum: ${min_gb}GB)"
            return 1
        else
            log "INFO" "Disk space on $path: ${available_gb}GB available"
            return 0
        fi
    else
        log "ERROR" "Path does not exist: $path"
        return 1
    fi
}

check_memory_usage() {
    local min_gb=${1:-8}
    local available_gb=$(safe_int "$(free -g | awk '/^Mem:/{print $7}')")
    local total_gb=$(safe_int "$(free -g | awk '/^Mem:/{print $2}')")
    
    if [ "$available_gb" -lt "$min_gb" ]; then
        log "WARN" "Low memory: ${available_gb}GB available of ${total_gb}GB total (${min_gb}GB recommended)"
        return 1
    else
        log "INFO" "Memory: ${available_gb}GB available of ${total_gb}GB total"
        return 0
    fi
}

get_optimal_threads() {
    local max_threads=${1:-$OMP_NUM_THREADS}
    local load_avg=$(uptime | awk -F'load average:' '{print $2}' | awk '{print $1}' | sed 's/,//')
    local cores=$(nproc)
    
    # If load is high, reduce threads
    if command -v bc &>/dev/null && (( $(echo "$load_avg > $cores" | bc -l) )); then
        echo $((max_threads / 2))
    else
        echo $max_threads
    fi
}

monitor_resources() {
    local sub=$1
    local stage=$2
    
    {
        echo "=== Resource Monitor: ${sub} - ${stage} ==="
        echo "Time: $(date)"
        echo "CPU Load: $(uptime | awk -F'load average:' '{print $2}')"
        echo "Memory: $(free -h | grep '^Mem')"
        echo "BIDS drive: $(df -h $(dirname "$BIDS_DIR") | tail -1 2>/dev/null || echo 'N/A')"
        echo "Fast storage: $(df -h "$STORAGE_FAST" | tail -1 2>/dev/null || echo 'N/A')"
        echo "Large storage: $(df -h "$STORAGE_LARGE" | tail -1 2>/dev/null || echo 'N/A')"
        echo ""
    } >> "${LOG_DIR}/resource_monitor.log"
}

move_with_verification() {
    local source=$1
    local dest=$2
    local desc=$3
    
    if [ ! -e "$source" ]; then
        log "WARN" "Source does not exist: $source"
        return 1
    fi
    
    # Create destination directory if needed
    local dest_dir=$(dirname "$dest")
    mkdir -p "$dest_dir"
    
    # Use rsync for reliable transfer with progress
    log "INFO" "Moving $desc to external storage..."
    if rsync -av --remove-source-files "$source" "$dest" 2>/dev/null; then
        # Clean up empty directories
        if [ -d "$source" ]; then
            find "$source" -type d -empty -delete 2>/dev/null || true
        fi
        log "OK" "$desc moved successfully"
        return 0
    else
        log "ERROR" "Failed to move $desc"
        return 1
    fi
}

retry_operation() {
    local max_attempts=3
    local delay=5
    local count=0
    
    while [ $count -lt $max_attempts ]; do
        if "$@"; then
            return 0
        fi
        
        count=$((count + 1))
        if [ $count -lt $max_attempts ]; then
            log "WARN" "Operation failed (attempt ${count}/${max_attempts}), retrying in ${delay}s..."
            sleep $delay
            delay=$((delay * 2))  # Exponential backoff
        fi
    done
    
    return 1
}

check_write_permission() {
    local dir=$1
    local test_file="${dir}/.write_test_$$"
    
    if touch "$test_file" 2>/dev/null; then
        rm -f "$test_file"
        return 0
    else
        return 1
    fi
}

cleanup_work_dir() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}"
    
    if [ -d "${MRTRIX_DIR}/${sub}" ]; then
        find "${MRTRIX_DIR}/${sub}" \( -name "*.tmp" -o -name "*.mif" \) -delete 2>/dev/null || true
    fi
    
    # More aggressive cleanup for space
    if [ -d "$workdir" ]; then
        # Remove intermediate MIF files but keep logs
        find "$workdir" -name "*.mif" -not -name "*final*" -not -name "*registered*" -delete 2>/dev/null || true
        
        # Remove large temporary files
        find "$workdir" -name "*.nii.gz" -size +100M -mtime +0 -delete 2>/dev/null || true
    fi
}

cleanup_aggressive() {
    local sub=$1
    
    # Only clean files that are old enough to not be in active use (>2 hours)
    find "${WORK_DIR}/${sub}" -name "*.mif" -not -name "*final*" -mmin +120 -delete 2>/dev/null || true
    
    # Compress logs older than 1 day
    find "${LOG_DIR}" -name "*.log" -mtime +1 -exec gzip {} \; 2>/dev/null || true
    
    # Clean Docker images periodically (only if using Docker)
    if [[ "${DOCKER_CMD:-}" == *docker* ]]; then
        $DOCKER_CMD system prune -f &>/dev/null || true
    fi
}

# --- ML Registration Helper Functions ---

check_ml_dependencies() {
    log "ML" "Checking ML registration dependencies..."
    local missing_ml=()
    local gpu_available=false

    # AMICO
    if ! "$PYTHON_EXECUTABLE" -c "import amico" >/dev/null 2>&1; then
        missing_ml+=("dmri-amico")
        log "WARN" "AMICO not found - required for NODDI processing"
        export AMICO_AVAILABLE=false
    else
        local amico_version
        amico_version=$("$PYTHON_EXECUTABLE" -c "import amico; print(getattr(amico, '__version__', 'unknown'))" 2>/dev/null || echo "unknown")
        log "ML" "AMICO version: $amico_version"
        export AMICO_AVAILABLE=true
    fi

    # TensorFlow
    if ! "$PYTHON_EXECUTABLE" -c "import tensorflow as tf; print(tf.__version__)" >/dev/null 2>&1; then
        missing_ml+=("tensorflow")
        export TENSORFLOW_AVAILABLE=false
    else
        local tf_version
        tf_version=$("$PYTHON_EXECUTABLE" -c "import tensorflow as tf; print(tf.__version__)" 2>/dev/null || echo "unknown")
        log "ML" "TensorFlow version: $tf_version"
        export TENSORFLOW_AVAILABLE=true

        # GPU via TensorFlow
        if "$PYTHON_EXECUTABLE" -c "import tensorflow as tf; print(len(tf.config.list_physical_devices('GPU')))" 2>/dev/null | grep -q '^[1-9]'; then
            gpu_available=true
            log "ML" "GPU acceleration available for TensorFlow"
        else
            log "ML" "No GPU detected via TensorFlow"
        fi
    fi

    # VoxelMorph
    log "ML" "Testing VoxelMorph import..."
    local VOXM_IMPORT_LOG="${LOG_DIR:-.}/voxelmorph_import_error.txt"
    if "$PYTHON_EXECUTABLE" - <<'PY' 1>/dev/null 2>"${VOXM_IMPORT_LOG}"
import voxelmorph
PY
    then
        local vxm_version
        vxm_version=$("$PYTHON_EXECUTABLE" -c "import voxelmorph; print(getattr(voxelmorph, '__version__', 'unknown'))" 2>/dev/null || echo "unknown")
        log "ML" "VoxelMorph version: $vxm_version"
        export VOXELMORPH_AVAILABLE=true
    else
        missing_ml+=("voxelmorph")
        export VOXELMORPH_AVAILABLE=false
        if [ -s "${VOXM_IMPORT_LOG}" ]; then
            log "WARN" "VoxelMorph import failed. See ${VOXM_IMPORT_LOG}. Last lines:"
            tail -n 5 "${VOXM_IMPORT_LOG}" >&2 || true
        else
            log "WARN" "VoxelMorph import failed (no error output)"
        fi
    fi

    # Optional packages (module -> pip name)
    local optional_pkgs=("sklearn:scikit-learn" "scipy:scipy" "nibabel:nibabel")
    local missing_opt_mod=()
    for pair in "${optional_pkgs[@]}"; do
        IFS=: read -r mod pipname <<< "$pair"
        if ! "$PYTHON_EXECUTABLE" -c "import ${mod}" >/dev/null 2>&1; then
            missing_opt_mod+=("$mod")
            missing_ml+=("$pipname")  # use pip names in the global list
        fi
    done
    if [ ${#missing_opt_mod[@]} -gt 0 ]; then
        log "WARN" "Missing optional ML modules: ${missing_opt_mod[*]}"
    fi

    # SynthMorph (FreeSurfer)
    if command -v mri_synthmorph >/dev/null 2>&1; then
        log "ML" "SynthMorph available"
        export SYNTHMORPH_AVAILABLE=true
        local fs_version
        fs_version=$(mri_synthmorph --version 2>/dev/null | head -1 || echo "unknown")
        log "ML" "FreeSurfer version: $fs_version"
    else
        log "WARN" "SynthMorph not available (need FreeSurfer 7.3+)"
        export SYNTHMORPH_AVAILABLE=false
    fi

    # PyTorch GPU fallback (if TF didn’t see a GPU)
    if [ "$gpu_available" != true ]; then
        if "$PYTHON_EXECUTABLE" -c "import torch; print(torch.cuda.is_available())" 2>/dev/null | grep -q 'True'; then
            gpu_available=true
            log "ML" "GPU acceleration available via PyTorch"
        fi
    fi
    export GPU_AVAILABLE=$gpu_available

    # Final availability flag
    if [ ${#missing_ml[@]} -eq 0 ]; then
        export ML_REGISTRATION_AVAILABLE=true
        log "ML" "All ML dependencies satisfied"
    else
        export ML_REGISTRATION_AVAILABLE=false
        log "WARN" "Missing ML packages: ${missing_ml[*]}"
        log "INFO" "Install with: pip install ${missing_ml[*]}"
        if [ "${AUTO_INSTALL_ML:-false}" = true ]; then
            log "ML" "Attempting to install missing ML packages..."
            if "$PYTHON_EXECUTABLE" -m pip install "${missing_ml[@]}" >/dev/null 2>&1; then
                log "OK" "ML packages installed successfully"
                export ML_REGISTRATION_AVAILABLE=true
            else
                log "ERROR" "Failed to install ML packages automatically"
            fi
        fi
    fi

    export ML_DEPENDENCIES_CHECKED=true
}

check_required_tools() {
    local -a required_tools=(
        bc mrconvert mrstats dwidenoise mrdegibbs dwifslpreproc dwibiascorrect
        dwi2tensor tensor2metric dwi2mask mrcalc maskfilter dwinormalise
        bet2 fast flirt epi_reg convert_xfm fslmaths fslstats
        N4BiasFieldCorrection recon-all 5ttgen labelconvert tckgen tcksift2
        tck2connectome dwi2response dwi2fod tckinfo tcksample
    )
    local -a missing=()

    # Ensure key paths are present before checking
    export FSLDIR=/usr/local/fsl
    export PATH="$FSLDIR/bin:/home/trav/mrtrix3/bin:/usr/local/bin:/usr/bin:$PATH"
    export FREESURFER_HOME=${FREESURFER_HOME:-/usr/local/freesurfer}
[ -d "$FREESURFER_HOME/bin" ] && export PATH="$FREESURFER_HOME/bin:$PATH"

    for tool in "${required_tools[@]}"; do
        if [[ "$tool" == /* ]]; then
            [[ -x "$tool" ]] || missing+=("$tool")
        else
            command -v "$tool" >/dev/null 2>&1 || missing+=("$tool")
        fi
    done

    # Check Python executable path explicitly
    if [[ -z "${PYTHON_EXECUTABLE:-}" || ! -x "$PYTHON_EXECUTABLE" ]]; then
        missing+=("${PYTHON_EXECUTABLE:-python}")
    fi

    if ((${#missing[@]})); then
        log "ERROR" "Missing required tools: ${missing[*]}"
        return 1
    else
        log "OK" "All required tools found"
        return 0
    fi
}


# Lightweight Python helper for small repeated operations.
# Avoids spawning a new Python process for each tiny task.
# Usage: run_py_helper <command> [args...]
#   json_update <file> <key> <value>   - update a JSON file
#   snr_estimate <dwi> <mask>          - compute SNR
#   check_import <module>              - test if a Python module imports
create_python_helper() {
    local helper_path="${WORK_DIR}/_py_helper.py"
    [ -f "$helper_path" ] && { export PY_HELPER="$helper_path"; return 0; }
    
    cat > "$helper_path" << 'PYHELPER'
#!/usr/bin/env python3
"""Lightweight helper for small repeated pipeline operations."""
import json, os, sys

def json_update(filepath, key, value):
    data = {}
    if os.path.exists(filepath):
        try:
            with open(filepath) as f: data = json.load(f)
        except: pass
    data[key] = value
    with open(filepath, 'w') as f: json.dump(data, f, indent=2)
    print(f"Updated {key} in {filepath}")

def check_import(module):
    try:
        __import__(module)
        print(f"{module}: OK")
        return True
    except ImportError as e:
        print(f"{module}: FAILED ({e})")
        return False

def snr_estimate(dwi_file, mask_file):
    import nibabel as nib, numpy as np
    dwi = nib.load(dwi_file).get_fdata()
    mask = nib.load(mask_file).get_fdata()
    b0_vols = dwi[..., :5]
    signal = np.mean(b0_vols[mask > 0])
    noise = np.std(b0_vols[mask > 0])
    snr = signal / noise if noise > 0 else 0
    print(f"SNR={snr:.2f} Signal={signal:.2f} Noise={noise:.2f}")
    return snr

if __name__ == "__main__":
    cmd = sys.argv[1] if len(sys.argv) > 1 else "help"
    if cmd == "json_update" and len(sys.argv) == 5:
        json_update(sys.argv[2], sys.argv[3], sys.argv[4])
    elif cmd == "check_import" and len(sys.argv) == 3:
        sys.exit(0 if check_import(sys.argv[2]) else 1)
    elif cmd == "snr_estimate" and len(sys.argv) == 4:
        snr_estimate(sys.argv[2], sys.argv[3])
    else:
        print(f"Usage: {sys.argv[0]} <json_update|check_import|snr_estimate> [args]")
        sys.exit(1)
PYHELPER
    chmod +x "$helper_path"
    export PY_HELPER="$helper_path"
}

setup_ml_models() {
    local models_dir="${DERIV_DIR}/ml_models"
    mkdir -p "$models_dir"
    export ML_MODELS_DIR="$models_dir"
    
    log "ML" "Setting up ML models directory: $models_dir"
    
    # Download or check for pre-trained models if specified
    if [ -n "${ML_MODEL_PATH:-}" ] && [ -f "$ML_MODEL_PATH" ]; then
        log "ML" "Using custom ML model: $ML_MODEL_PATH"
        export CUSTOM_ML_MODEL="$ML_MODEL_PATH"
    else
        log "ML" "No custom ML model specified, will use default models"
    fi
    
    # Check disk space for models
    if ! check_disk_space "$models_dir" 5; then
        log "WARN" "Limited space for ML models"
    fi
}

create_voxelmorph_registration_script() {
    local script_path="${WORK_DIR}/voxelmorph_registration.py"
    
    # Prefer co-located .py file (can be linted/tested independently)
    local script_dir
    script_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    if [ -f "${script_dir}/voxelmorph_registration.py" ]; then
        cp "${script_dir}/voxelmorph_registration.py" "$script_path"
        log "ML" "Using co-located voxelmorph_registration.py"
        export VOXELMORPH_SCRIPT="$script_path"
        return 0
    fi
    
    # Fallback: generate inline (for single-file distribution)
    cat > "$script_path" << 'PYTHON_EOF'
import os, sys, warnings
warnings.filterwarnings("ignore")

import numpy as np
import nibabel as nib
import tensorflow as tf
from tensorflow import keras

# Disable TensorFlow warnings
tf.get_logger().setLevel('ERROR')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

def check_tensorflow_version():
    """Check TensorFlow compatibility and apply fixes"""
    tf_version = tf.__version__
    major, minor = map(int, tf_version.split('.')[:2])
    
    if major >= 2:
        # Handle newer TensorFlow versions
        tf.config.run_functions_eagerly(False)
        
        # Fix for Keras 3 compatibility
        def _patch_keras_tensor():
            """Add get_shape() method to KerasTensor for VoxelMorph compatibility"""
            try:
                # Try different import paths for KerasTensor
                KerasTensor = None
                try:
                    from keras.src.engine.keras_tensor import KerasTensor
                except ImportError:
                    try:
                        from keras.engine.keras_tensor import KerasTensor
                    except ImportError:
                        try:
                            from tensorflow.python.keras.engine.keras_tensor import KerasTensor
                        except ImportError:
                            pass
                
                if KerasTensor and not hasattr(KerasTensor, 'get_shape'):
                    # Add get_shape method
                    def get_shape(self):
                        return self.shape
                    KerasTensor.get_shape = get_shape
                    
                    # Also add _keras_shape property for older code
                    if not hasattr(KerasTensor, '_keras_shape'):
                        KerasTensor._keras_shape = property(lambda self: tuple(self.shape))
                    
                    print("Applied KerasTensor compatibility patch")
                    
            except Exception as e:
                print(f"Warning: Could not patch KerasTensor: {e}")
        
        _patch_keras_tensor()
    
    return f"{major}.{minor}"

# Apply TensorFlow compatibility fixes
tf_version = check_tensorflow_version()
print(f"Using TensorFlow version: {tf_version}")

class VoxelMorphDWIRegistration:
    def __init__(self, reference_vol, use_gpu=False):
        self.reference = reference_vol
        self.use_gpu = use_gpu
        self.model = None
        
        # Configure TensorFlow
        if not use_gpu:
            tf.config.set_visible_devices([], 'GPU')
        else:
            # Configure GPU memory growth
            physical_devices = tf.config.list_physical_devices('GPU')
            if physical_devices:
                try:
                    tf.config.experimental.set_memory_growth(physical_devices[0], True)
                except:
                    pass  # Memory growth already configured
        
        self.setup_registration_network()

    def setup_registration_network(self):
        """Setup lightweight registration network for DWI"""
        try:
            import voxelmorph as vxm
            
            # Reduce model size for memory efficiency and compatibility
            nb_features = [[8, 16, 16], [16, 16, 8, 8]]  # Reduced from [16, 32, 32]
            
            # Create VoxelMorph model with compatibility fixes
            try:
                self.model = vxm.networks.VxmDense(
                    inshape=self.reference.shape,
                    nb_unet_features=nb_features,
                    int_steps=5,  # Reduced from 7
                    int_downsize=2
                )
                
                # Compile with appropriate loss
                self.model.compile(
                    optimizer=keras.optimizers.Adam(1e-4),
                    loss=[vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss],
                    loss_weights=[1.0, 0.01]
                )
                
                print("VoxelMorph model loaded successfully")
                return True
                
            except Exception as e:
                print(f"VoxelMorph model creation failed: {e}")
                self.setup_simple_registration()
                return False
        
        except ImportError:
            print("VoxelMorph not available, using simple registration")
            self.setup_simple_registration()
            return False

    def setup_simple_registration(self):
        """Fallback simple registration network"""
        try:
            # Use functional API for better compatibility
            inputs = keras.Input(shape=self.reference.shape + (2,))
            
            # Simple encoder with proper layer naming
            x = keras.layers.Conv3D(8, 3, activation='relu', padding='same', name='conv1')(inputs)
            x = keras.layers.MaxPooling3D(2, name='pool1')(x)
            x = keras.layers.Conv3D(16, 3, activation='relu', padding='same', name='conv2')(x)
            x = keras.layers.MaxPooling3D(2, name='pool2')(x)
            
            # Simple decoder
            x = keras.layers.UpSampling3D(2, name='upsample1')(x)
            x = keras.layers.Conv3D(8, 3, activation='relu', padding='same', name='conv3')(x)
            x = keras.layers.UpSampling3D(2, name='upsample2')(x)
            
            # Output displacement field
            outputs = keras.layers.Conv3D(3, 1, activation='tanh', padding='same', name='displacement')(x)
            outputs = keras.layers.Lambda(lambda x: x * 10, name='scale_displacement')(outputs)
            
            self.model = keras.Model(inputs, outputs, name='SimpleRegistration')
            self.model.compile(optimizer='adam', loss='mse')
            
            print("Simple registration model created")
            
        except Exception as e:
            print(f"Failed to create simple registration model: {e}")
            self.model = None

    def register_volume(self, moving_vol, quick_mode=True):
        """Register moving volume to reference"""
        try:
            # Normalize volumes
            ref_norm = self.normalize_volume(self.reference)
            mov_norm = self.normalize_volume(moving_vol)
            
            # Check data validity
            if np.any(np.isnan(mov_norm)) or np.any(np.isinf(mov_norm)):
                mov_norm = np.nan_to_num(mov_norm, nan=0.0, posinf=0.0, neginf=0.0)
            
            if quick_mode or self.model is None:
                # Use simple cross-correlation for quick registration
                return self.simple_registration(moving_vol, ref_norm, mov_norm)
            else:
                # Use full ML registration
                return self.ml_registration(moving_vol, ref_norm, mov_norm)
                
        except Exception as e:
            print(f"Registration failed: {e}")
            return moving_vol, np.zeros(moving_vol.shape + (3,))

    def normalize_volume(self, volume):
        """Normalize volume intensity"""
        volume = volume.astype(np.float32)
        
        # Mask out background
        mask = volume > np.percentile(volume, 5)  # More robust than > 0
        
        if np.sum(mask) > 0:
            mean_val = np.mean(volume[mask])
            std_val = np.std(volume[mask])
            if std_val > 0:
                normalized = np.zeros_like(volume)
                normalized[mask] = (volume[mask] - mean_val) / std_val
                return normalized
        
        return volume

    def simple_registration(self, moving_vol, ref_norm, mov_norm):
        """Simple registration using correlation"""
        try:
            from scipy import ndimage
            from scipy.optimize import minimize
            
            def correlation_metric(params):
                # Simple translation parameters
                shift = params[:3]
                
                # Apply shift
                shifted = ndimage.shift(mov_norm, shift, order=1, cval=0)
                
                # Calculate normalized cross-correlation
                mask = (ref_norm > 0.1) & (shifted > 0.1)  # More restrictive mask
                if np.sum(mask) < 100:  # Ensure sufficient overlap
                    return 1.0
                
                try:
                    corr = np.corrcoef(ref_norm[mask], shifted[mask])[0, 1]
                    return -corr if not np.isnan(corr) else 1.0
                except:
                    return 1.0
            
            # Optimize translation with bounds
            bounds = [(-10, 10), (-10, 10), (-5, 5)]  # Reasonable movement bounds
            result = minimize(correlation_metric, [0, 0, 0], method='L-BFGS-B', bounds=bounds)
            
            if result.success and result.fun < -0.3:  # Minimum correlation threshold
                # Apply optimal shift
                optimal_shift = result.x
                registered = ndimage.shift(moving_vol, optimal_shift, order=1, cval=0)
                
                # Create displacement field
                displacement = np.zeros(moving_vol.shape + (3,))
                for i in range(3):
                    displacement[..., i] = optimal_shift[i]
                
                return registered, displacement
            else:
                return moving_vol, np.zeros(moving_vol.shape + (3,))
                
        except ImportError:
            print("SciPy not available for registration")
            return moving_vol, np.zeros(moving_vol.shape + (3,))
        except Exception as e:
            print(f"Simple registration failed: {e}")
            return moving_vol, np.zeros(moving_vol.shape + (3,))

    def ml_registration(self, moving_vol, ref_norm, mov_norm):
        """Full ML-based registration with improved error handling"""
        if self.model is None:
            return self.simple_registration(moving_vol, ref_norm, mov_norm)
        
        try:
            # Check if it's a VoxelMorph model
            is_vxm = hasattr(self.model, 'register') or 'vxm' in str(type(self.model)).lower()
            
            if is_vxm:
                # VoxelMorph expects two separate inputs
                moving = mov_norm[np.newaxis, ..., np.newaxis]  # (1, D, H, W, 1)
                fixed = ref_norm[np.newaxis, ..., np.newaxis]   # (1, D, H, W, 1)
                
                # Ensure inputs are valid
                moving = np.nan_to_num(moving, nan=0.0, posinf=0.0, neginf=0.0)
                fixed = np.nan_to_num(fixed, nan=0.0, posinf=0.0, neginf=0.0)
                
                # Predict
                with tf.device('/CPU:0' if not self.use_gpu else '/GPU:0'):
                    if hasattr(self.model, 'register'):
                        # Use register method if available
                        moved, displacement = self.model.register(moving, fixed)
                        registered = moved[0, ..., 0]
                        displacement = displacement[0]
                    else:
                        # Use predict method
                        outputs = self.model.predict([moving, fixed], verbose=0)
                        if isinstance(outputs, list) and len(outputs) == 2:
                            moved, displacement = outputs
                            registered = moved[0, ..., 0]
                            displacement = displacement[0]
                        else:
                            # Single output - assume it's displacement
                            displacement = outputs[0]
                            registered = self.apply_displacement(moving_vol, displacement)
                
                return registered, displacement
                
            else:
                # Simple model - uses stacked input
                input_vol = np.stack([ref_norm, mov_norm], axis=-1)
                input_vol = input_vol[np.newaxis, ...]  # Add batch dimension
                
                # Ensure input is valid
                input_vol = np.nan_to_num(input_vol, nan=0.0, posinf=0.0, neginf=0.0)
                
                # Predict displacement field
                with tf.device('/CPU:0' if not self.use_gpu else '/GPU:0'):
                    displacement = self.model.predict(input_vol, verbose=0)[0]
                    registered = self.apply_displacement(moving_vol, displacement)
                
                return registered, displacement
                
        except Exception as e:
            print(f"ML prediction failed: {e}")
            return self.simple_registration(moving_vol, ref_norm, mov_norm)

    def apply_displacement(self, volume, displacement):
        """Apply displacement field to volume"""
        try:
            from scipy import ndimage
            
            # Create coordinate grids
            coords = np.mgrid[0:volume.shape[0], 0:volume.shape[1], 0:volume.shape[2]]
            
            # Apply displacement with bounds checking
            new_coords = []
            for i in range(3):
                displaced = coords[i] + displacement[..., i]
                # Clamp to valid range
                displaced = np.clip(displaced, 0, volume.shape[i] - 1)
                new_coords.append(displaced)
            
            # Interpolate
            registered = ndimage.map_coordinates(
                volume, new_coords, order=1, cval=0, prefilter=False
            )
            
            return registered
            
        except ImportError:
            print("SciPy not available for displacement application")
            return volume
        except Exception as e:
            print(f"Displacement application failed: {e}")
            return volume

    def __del__(self):
        """Cleanup resources"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            
            # Clear TensorFlow session
            if hasattr(tf.keras.backend, 'clear_session'):
                tf.keras.backend.clear_session()
        except:
            pass  # Ignore cleanup errors

def run_voxelmorph_dwi_registration(dwi_file, output_file, sub, use_gpu=False, quick_mode=True):
    """Main DWI registration function with comprehensive error handling"""
    print(f"[{sub}] Starting {'quick' if quick_mode else 'full'} ML DWI registration")
    
    try:
        # Check file exists
        if not os.path.exists(dwi_file):
            raise FileNotFoundError(f"DWI file not found: {dwi_file}")
        
        # Load DWI with better error handling
        try:
            dwi_img = nib.load(dwi_file)
            dwi_data = dwi_img.get_fdata()
        except Exception as e:
            raise ValueError(f"Failed to load DWI data: {str(e)}")
        
        if dwi_data.ndim != 4:
            raise ValueError(f"Expected 4D DWI data, got {dwi_data.ndim}D")
        
        # Check data validity
        if np.any(np.isnan(dwi_data)) or np.any(np.isinf(dwi_data)):
            print(f"[{sub}] Warning: Invalid values detected in DWI data, cleaning...")
            dwi_data = np.nan_to_num(dwi_data, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Check data range
        if np.max(dwi_data) == 0:
            raise ValueError("DWI data appears to be empty (all zeros)")
        
        # Use first volume as reference (b0)
        reference_vol = dwi_data[..., 0]
        
        # Initialize registration
        registrator = VoxelMorphDWIRegistration(reference_vol, use_gpu)
        
        registered_volumes = [reference_vol]  # Reference doesn't need registration
        
        print(f"[{sub}] Registering {dwi_data.shape[-1]-1} volumes to reference")
        
        # Process volumes with progress reporting
        success_count = 0
        for i in range(1, dwi_data.shape[-1]):
            try:
                moving_vol = dwi_data[..., i]
                
                # Skip empty volumes
                if np.max(moving_vol) == 0:
                    print(f"[{sub}] Skipping empty volume {i}")
                    registered_volumes.append(moving_vol)
                    continue
                
                registered_vol, displacement = registrator.register_volume(moving_vol, quick_mode)
                registered_volumes.append(registered_vol)
                success_count += 1
                
                if i % 20 == 0 or i == dwi_data.shape[-1] - 1:
                    print(f"[{sub}] Processed {i}/{dwi_data.shape[-1]-1} volumes ({success_count} successful)")
                    
            except Exception as e:
                print(f"[{sub}] Failed to register volume {i}: {e}")
                registered_volumes.append(dwi_data[..., i])  # Keep original
        
        # Stack registered volumes
        registered_dwi = np.stack(registered_volumes, axis=-1)
        
        # Ensure output directory exists
        output_dir = os.path.dirname(output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # Save with proper data type
        registered_img = nib.Nifti1Image(
            registered_dwi.astype(np.float32),
            dwi_img.affine,
            dwi_img.header
        )
        nib.save(registered_img, output_file)
        
        print(f"[{sub}] ML DWI registration completed successfully ({success_count}/{dwi_data.shape[-1]-1} volumes registered)")
        return True
        
    except Exception as e:
        print(f"[{sub}] ML DWI registration failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # Cleanup
        if 'registrator' in locals():
            del registrator

if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Usage: python script.py <dwi_file> <output_file> <subject_id> [use_gpu] [quick_mode]")
        sys.exit(1)
    
    dwi_file = sys.argv[1]
    output_file = sys.argv[2] 
    subject_id = sys.argv[3]
    use_gpu = len(sys.argv) > 4 and sys.argv[4].lower() == 'true'
    quick_mode = len(sys.argv) <= 5 or sys.argv[5].lower() != 'false'
    
    success = run_voxelmorph_dwi_registration(dwi_file, output_file, subject_id, use_gpu, quick_mode)
    sys.exit(0 if success else 1)
PYTHON_EOF

    # Set environment variable exactly as original
    export VOXELMORPH_SCRIPT="$script_path"
    
    # Use simple logging to match original pattern
    log "ML" "VoxelMorph registration script created: $script_path"
}

run_voxelmorph_dwi_registration() {
    local sub=$1
    local dwi_file=$2
    local output_file=$3
    local quick_mode=${4:-true}
    
    if [ "$ML_REGISTRATION_AVAILABLE" != true ]; then
        log "WARN" "[${sub}] VoxelMorph not available"
        return 1
    fi
    
    log "ML" "[${sub}] Running VoxelMorph DWI registration"
    
    # Create script if it doesn't exist
    if [ ! -f "${VOXELMORPH_SCRIPT:-}" ]; then
        create_voxelmorph_registration_script
    fi
    
    # Run VoxelMorph registration
    $PYTHON_EXECUTABLE "$VOXELMORPH_SCRIPT" "$dwi_file" "$output_file" "$sub" \
        "${GPU_AVAILABLE:-false}" "$quick_mode" \
        &> "${WORK_DIR}/${sub}/voxelmorph_registration.log"
    
    local exit_code=$?
    
    if [ $exit_code -eq 0 ] && [ -f "$output_file" ]; then
        log "OK" "[${sub}] VoxelMorph registration successful"
        return 0
    else
        log "WARN" "[${sub}] VoxelMorph registration failed"
        if [ -f "${WORK_DIR}/${sub}/voxelmorph_registration.log" ]; then
            log "WARN" "[${sub}] Last 5 lines of VoxelMorph log:"
            tail -n 5 "${WORK_DIR}/${sub}/voxelmorph_registration.log" >&2
        fi
        return 1
    fi
}

run_synthmorph_registration() {
    local sub=$1
    local fixed=$2      # target image
    local moving=$3     # image to register  
    local output_prefix=$4
    
    if [ "$SYNTHMORPH_AVAILABLE" != true ]; then
        log "WARN" "[${sub}] SynthMorph not available"
        return 1
    fi
    
    log "ML" "[${sub}] Running SynthMorph registration"
    
    # Set GPU environment for SynthMorph
    export CUDA_VISIBLE_DEVICES=0
    
    # Run SynthMorph with correct syntax
    local cmd="mri_synthmorph"
    [ "$GPU_AVAILABLE" = true ] && cmd="$cmd -g"
    cmd="$cmd -t ${output_prefix}_transform.mgz"
    cmd="$cmd -o ${output_prefix}_moved.mgz" 
    cmd="$cmd -j $(get_optimal_threads 4)"
    cmd="$cmd $moving $fixed"
    
    eval "$cmd" &> "${WORK_DIR}/${sub}/synthmorph_registration.log"
    local exit_code=$?
    
    if [ $exit_code -eq 0 ] && [ -f "${output_prefix}_moved.mgz" ]; then
        log "OK" "[${sub}] SynthMorph registration successful"
        
        # Convert to NIfTI if needed
        if [[ "$output_prefix" == *".nii.gz" ]]; then
            output_prefix_nii="${output_prefix%%.nii.gz}"
            mri_convert "${output_prefix}_moved.mgz" "${output_prefix_nii}_moved.nii.gz"
        fi
        
        return 0
    else
        log "WARN" "[${sub}] SynthMorph registration failed"
        if [ -f "${WORK_DIR}/${sub}/synthmorph_registration.log" ]; then
            log "WARN" "[${sub}] Last 5 lines of SynthMorph log:"
            tail -n 5 "${WORK_DIR}/${sub}/synthmorph_registration.log" >&2
        fi
        return 1
    fi
}

validate_ml_registration_quality() {
    local sub=$1
    local reference=$2
    local registered=$3
    local method=$4
    
    log "ML" "[${sub}] Validating ${method} registration quality"
    
    # Calculate basic quality metrics
    _ML_REF="$reference" _ML_REG="$registered" _ML_SUB="$sub" _ML_METHOD="$method" \
    $PYTHON_EXECUTABLE - << 'PYEOF'
import os, sys
import nibabel as nib
import numpy as np

sub = os.environ['_ML_SUB']
method = os.environ['_ML_METHOD']
reference = os.environ['_ML_REF']
registered = os.environ['_ML_REG']

try:
    ref_img = nib.load(reference)
    reg_img = nib.load(registered)
    ref_data = ref_img.get_fdata()
    reg_data = reg_img.get_fdata()

    if ref_data.shape != reg_data.shape:
        print(f"[{sub}] WARNING: Shape mismatch - ref: {ref_data.shape}, reg: {reg_data.shape}")
        sys.exit(1)

    mask = (ref_data > 0) & (reg_data > 0)
    correlation = np.corrcoef(ref_data[mask], reg_data[mask])[0, 1] if np.sum(mask) > 100 else 0.0

    def mutual_info(x, y, bins=50):
        hist_2d, _, _ = np.histogram2d(x, y, bins=bins)
        hist_2d = hist_2d + 1e-10
        pxy = hist_2d / np.sum(hist_2d)
        px, py = np.sum(pxy, axis=1), np.sum(pxy, axis=0)
        mi = 0
        for i in range(len(px)):
            for j in range(len(py)):
                if pxy[i, j] > 0:
                    mi += pxy[i, j] * np.log(pxy[i, j] / (px[i] * py[j]))
        return mi

    mi = mutual_info(ref_data[mask], reg_data[mask]) if np.sum(mask) > 1000 else 0.0
    mse = np.mean((ref_data[mask] - reg_data[mask])**2) if np.sum(mask) > 0 else float('inf')

    print(f"[{sub}] {method} Quality Metrics:")
    print(f"[{sub}]   Correlation: {correlation:.4f}")
    print(f"[{sub}]   Mutual Info: {mi:.4f}")
    print(f"[{sub}]   MSE: {mse:.2f}")
    print(f"[{sub}]   Valid voxels: {np.sum(mask)}")

    if correlation > 0.8 and mi > 0.5:
        print(f"[{sub}] {method} registration quality: GOOD"); sys.exit(0)
    elif correlation > 0.6 and mi > 0.3:
        print(f"[{sub}] {method} registration quality: ACCEPTABLE"); sys.exit(0)
    else:
        print(f"[{sub}] {method} registration quality: POOR"); sys.exit(1)

except Exception as e:
    print(f"[{sub}] Registration validation failed: {str(e)}")
    sys.exit(1)
PYEOF

    return $?
}

# --- Progress Tracking Functions ---

create_checkpoint() {
    local sub=$1
    local stage=$2
    local timestamp=$(date +%s)
    
    mkdir -p "${LOG_DIR}/checkpoints"
    echo "${stage}:${timestamp}:$(date -Iseconds)" >> "${LOG_DIR}/checkpoints/${sub}_checkpoints.txt"
}

check_checkpoint() {
    local sub=$1
    local stage=$2
    
    [ -f "${LOG_DIR}/checkpoints/${sub}_checkpoints.txt" ] && \
    grep -q "^${stage}:" "${LOG_DIR}/checkpoints/${sub}_checkpoints.txt"
}

update_progress() {
    local sub=$1
    local stage=$2
    local percent=$3
    local progress_file="${LOG_DIR}/${sub}_progress.json"
    
    # Use helper script if available (avoids Python startup cost per call)
    if [ -n "${PY_HELPER:-}" ] && [ -f "${PY_HELPER}" ]; then
        $PYTHON_EXECUTABLE "$PY_HELPER" json_update "$progress_file" "$stage" "$percent" 2>/dev/null || true
        $PYTHON_EXECUTABLE "$PY_HELPER" json_update "$progress_file" "last_update" "$(date -Iseconds)" 2>/dev/null || true
        return 0
    fi
    
    # Fallback: inline Python
    _PF="$progress_file" _ST="$stage" _PCT="$percent" _TS="$(date -Iseconds)" \
    $PYTHON_EXECUTABLE - << 'PYEOF'
import json, os
pf, st, pct, ts = os.environ['_PF'], os.environ['_ST'], os.environ['_PCT'], os.environ['_TS']
try:
    data = json.load(open(pf)) if os.path.exists(pf) else {}
except: data = {}
try: data[st] = int(pct)
except: data[st] = pct
data['last_update'] = ts
try:
    with open(pf, 'w') as f: json.dump(data, f, indent=2)
except: pass
PYEOF
}

estimate_processing_time() {
    local stage=$1
    local num_volumes=${2:-1}
    
    case $stage in
        "synb0") echo "15-30 minutes" ;;
        "preprocessing") echo "$((10 + num_volumes / 5))-$((20 + num_volumes / 3)) minutes" ;;
        "ml_registration") echo "$((5 + num_volumes / 10))-$((15 + num_volumes / 5)) minutes" ;;
        "freesurfer") echo "6-12 hours" ;;
        "tractography") echo "30-90 minutes" ;;
        "noddi") echo "15-45 minutes" ;;
        *) echo "Unknown" ;;
    esac
}

# --- Signal Handling ---
cleanup_on_exit() {
    log "WARN" "Script interrupted, performing cleanup..."
    
    # Kill any running Python processes for this pipeline
    pkill -f "voxelmorph_registration.py" 2>/dev/null || true
    pkill -f "fit_noddi.py" 2>/dev/null || true
    
    # Clean up temporary files
    if [ -n "${WORK_DIR:-}" ]; then
        find "${WORK_DIR}" -name "*.tmp" -delete 2>/dev/null || true
        find "${WORK_DIR}" -name "core.*" -delete 2>/dev/null || true
    fi
    
    # Clean up any mounted volumes or locks
    if [ -n "${DOCKER_CMD:-}" ]; then
        $DOCKER_CMD ps -q --filter "ancestor=leonyichencai/synb0-disco" | xargs -r $DOCKER_CMD stop 2>/dev/null || true
    fi
    
    log "WARN" "Cleanup completed"
    exit 130
}

# Signal handlers are set up in the main execution block below

# --- Configuration Loading ---
load_config() {
    local config_file="${1:-pipeline_config.conf}"
    
    if [ -f "$config_file" ]; then
        log "INFO" "Loading configuration from: $config_file"
        
        # Syntax check before sourcing (catches unbalanced quotes, etc.)
        if ! bash -n "$config_file" 2>/dev/null; then
            log "ERROR" "Config file has syntax errors: $config_file"
            log "ERROR" "$(bash -n "$config_file" 2>&1)"
            return 1
        fi
        
        # Source config file safely
        set +u  # Temporarily allow undefined variables
        source "$config_file" 2>/dev/null || {
            log "WARN" "Failed to load config file: $config_file"
            set -u
            return 1
        }
        set -u
        
        # Validate required keys have sensible values
        _validate_config_keys
        
        log "INFO" "Configuration loaded and validated"
    else
        log "INFO" "No config file found ($config_file), using defaults"
    fi
    
    return 0
}

# Validate known config keys after loading
_validate_config_keys() {
    local warnings=0
    
    # Check paths exist if they were set
    for var in BIDS_DIR STORAGE_FAST STORAGE_LARGE; do
        local val="${!var:-}"
        if [ -n "$val" ] && [ ! -d "$val" ]; then
            log "WARN" "Config: $var='$val' — directory does not exist"
            ((warnings++))
        fi
    done
    
    # Check numeric values are actually numeric
    for var in OMP_THREADS ECHO_SPACING; do
        local val="${!var:-}"
        if [ -n "$val" ] && ! [[ "$val" =~ ^[0-9]+\.?[0-9]*$ ]]; then
            log "WARN" "Config: $var='$val' — expected a number"
            ((warnings++))
        fi
    done
    
    # Check boolean values are actually boolean
    for var in USE_ML_REGISTRATION CLEANUP RUN_CONNECTOME SKIP_SYNB0; do
        local val="${!var:-}"
        if [ -n "$val" ] && [[ "$val" != "true" && "$val" != "false" ]]; then
            log "WARN" "Config: $var='$val' — expected 'true' or 'false'"
            ((warnings++))
        fi
    done
    
    if [ $warnings -gt 0 ]; then
        log "WARN" "Config validation: $warnings warning(s) found — review your config file"
    fi
}

# --- Enhanced ANTs Registration ---
run_ants_with_ml_features() {
    local sub=$1
    local fixed=$2
    local moving=$3
    local output_prefix=$4
    local registration_type=${5:-"rigid+affine+syn"}
    
    log "ML" "[${sub}] Running enhanced ANTs registration with ML features"
    
    # Determine registration stages based on type
    # ANTs requires transforms and metrics to be interleaved, not separated
    local ants_args=""
    
    case $registration_type in
        "rigid")
            ants_args="--transform Rigid[0.1] --metric MI[${fixed},${moving},1,32,Regular,0.25] --convergence [1000x500x250x100,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox"
            ;;
        "rigid+affine")
            ants_args="--transform Rigid[0.1] --metric MI[${fixed},${moving},1,32,Regular,0.25] --convergence [1000x500x250x100,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox --transform Affine[0.1] --metric MI[${fixed},${moving},1,32,Regular,0.25] --convergence [1000x500x250x100,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox"
            ;;
        "rigid+affine+syn"|*)
            ants_args="--transform Rigid[0.1] --metric MI[${fixed},${moving},1,32,Regular,0.25] --convergence [1000x500x250x100,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox --transform Affine[0.1] --metric MI[${fixed},${moving},1,32,Regular,0.25] --convergence [1000x500x250x100,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox --transform SyN[0.1,3,0] --metric CC[${fixed},${moving},1,4] --convergence [100x70x50x20,1e-6,10] --shrink-factors 8x4x2x1 --smoothing-sigmas 3x2x1x0vox"
            ;;
    esac
    
    # Run ANTs registration with enhanced parameters
    # Build command with proper argument handling
    eval "antsRegistration \
        --verbose 1 \
        --dimensionality 3 \
        --float 0 \
        --output \"[${output_prefix}_,${output_prefix}_Warped.nii.gz]\" \
        --interpolation Linear \
        --use-histogram-matching 1 \
        --winsorize-image-intensities \"[0.005,0.995]\" \
        --initial-moving-transform \"[${fixed},${moving}]\" \
        $ants_args \
        --write-composite-transform 1" \
        &> "${WORK_DIR}/${sub}/ants_ml_registration.log"
    
    local exit_code=$?
    
    if [ $exit_code -eq 0 ] && [ -f "${output_prefix}_Warped.nii.gz" ]; then
        log "OK" "[${sub}] Enhanced ANTs registration successful"
        return 0
    else
        log "WARN" "[${sub}] Enhanced ANTs registration failed"
        if [ -f "${WORK_DIR}/${sub}/ants_ml_registration.log" ]; then
            log "WARN" "[${sub}] Last 10 lines of ANTs log:"
            tail -n 10 "${WORK_DIR}/${sub}/ants_ml_registration.log" >&2
        fi
        return 1
    fi
}

# --- Registration Quality Assessment ---
assess_registration_quality() {
    local sub=$1
    local reference=$2
    local registered=$3
    local method=$4
    local output_report=$5
    
    log "ML" "[${sub}] Assessing registration quality for $method"
    
    $PYTHON_EXECUTABLE << EOF > "$output_report"
import nibabel as nib
import numpy as np
from scipy import stats, ndimage
import sys

def calculate_image_gradients(img):
    """Calculate image gradients for edge-based metrics"""
    grad_x = ndimage.sobel(img, axis=0)
    grad_y = ndimage.sobel(img, axis=1) 
    grad_z = ndimage.sobel(img, axis=2)
    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)
    return gradient_magnitude

def calculate_registration_metrics(ref_data, reg_data, mask=None):
    """Calculate comprehensive registration quality metrics"""
    if mask is None:
        mask = (ref_data > 0) & (reg_data > 0)
    
    if np.sum(mask) < 100:
        return None
    
    # Extract masked data
    ref_masked = ref_data[mask]
    reg_masked = reg_data[mask]
    
    metrics = {}
    
    # Correlation coefficient
    try:
        metrics['correlation'] = np.corrcoef(ref_masked, reg_masked)[0, 1]
    except:
        metrics['correlation'] = 0.0
    
    # Normalized cross-correlation
    try:
        ncc_num = np.sum((ref_masked - np.mean(ref_masked)) * (reg_masked - np.mean(reg_masked)))
        ncc_den = np.sqrt(np.sum((ref_masked - np.mean(ref_masked))**2) * np.sum((reg_masked - np.mean(reg_masked))**2))
        metrics['ncc'] = ncc_num / ncc_den if ncc_den > 0 else 0.0
    except:
        metrics['ncc'] = 0.0
    
    # Mean squared error
    metrics['mse'] = np.mean((ref_masked - reg_masked)**2)
    
    # Mean absolute error
    metrics['mae'] = np.mean(np.abs(ref_masked - reg_masked))
    
    # Structural similarity index (simplified)
    try:
        mu1, mu2 = np.mean(ref_masked), np.mean(reg_masked)
        sigma1, sigma2 = np.var(ref_masked), np.var(reg_masked)
        sigma12 = np.mean((ref_masked - mu1) * (reg_masked - mu2))
        
        c1, c2 = 0.01**2, 0.03**2
        ssim_num = (2*mu1*mu2 + c1) * (2*sigma12 + c2)
        ssim_den = (mu1**2 + mu2**2 + c1) * (sigma1 + sigma2 + c2)
        metrics['ssim'] = ssim_num / ssim_den if ssim_den > 0 else 0.0
    except:
        metrics['ssim'] = 0.0
    
    # Mutual information (simplified)
    try:
        hist_2d, _, _ = np.histogram2d(ref_masked, reg_masked, bins=50)
        hist_2d = hist_2d + 1e-10
        
        pxy = hist_2d / np.sum(hist_2d)
        px = np.sum(pxy, axis=1)
        py = np.sum(pxy, axis=0)
        
        mi = 0
        for i in range(len(px)):
            for j in range(len(py)):
                if pxy[i, j] > 0:
                    mi += pxy[i, j] * np.log(pxy[i, j] / (px[i] * py[j]))
        
        metrics['mutual_info'] = mi
    except:
        metrics['mutual_info'] = 0.0
    
    return metrics

try:
    print("Registration Quality Assessment Report")
    print("=" * 40)
    print(f"Subject: ${sub}")
    print(f"Method: ${method}")
    print(f"Generated: $(date)")
    print("")
    
    # Load images
    ref_img = nib.load('$reference')
    reg_img = nib.load('$registered')
    
    ref_data = ref_img.get_fdata()
    reg_data = reg_img.get_fdata()
    
    print(f"Reference shape: {ref_data.shape}")
    print(f"Registered shape: {reg_data.shape}")
    
    if ref_data.shape != reg_data.shape:
        print("ERROR: Shape mismatch between reference and registered images")
        sys.exit(1)
    
    # Create mask
    ref_thresh = np.percentile(ref_data[ref_data > 0], 10) if np.any(ref_data > 0) else 0
    reg_thresh = np.percentile(reg_data[reg_data > 0], 10) if np.any(reg_data > 0) else 0
    
    mask = (ref_data > ref_thresh) & (reg_data > reg_thresh)
    
    print(f"Valid voxels: {np.sum(mask)} ({np.sum(mask)/mask.size*100:.1f}%)")
    print("")
    
    # Calculate metrics
    metrics = calculate_registration_metrics(ref_data, reg_data, mask)
    
    if metrics is None:
        print("ERROR: Insufficient valid voxels for quality assessment")
        sys.exit(1)
    
    print("QUALITY METRICS:")
    print(f"  Correlation:     {metrics['correlation']:.4f}")
    print(f"  NCC:            {metrics['ncc']:.4f}")
    print(f"  MSE:            {metrics['mse']:.2f}")
    print(f"  MAE:            {metrics['mae']:.2f}")
    print(f"  SSIM:           {metrics['ssim']:.4f}")
    print(f"  Mutual Info:    {metrics['mutual_info']:.4f}")
    print("")
    
    # Overall quality assessment
    good_metrics = 0
    total_metrics = 0
    
    if metrics['correlation'] > 0.7: good_metrics += 1
    total_metrics += 1
    
    if metrics['ncc'] > 0.7: good_metrics += 1
    total_metrics += 1
    
    if metrics['ssim'] > 0.8: good_metrics += 1
    total_metrics += 1
    
    if metrics['mutual_info'] > 0.5: good_metrics += 1
    total_metrics += 1
    
    quality_score = good_metrics / total_metrics * 100
    
    print("OVERALL ASSESSMENT:")
    print(f"  Quality Score: {quality_score:.1f}%")
    
    if quality_score >= 75:
        print("  Quality: EXCELLENT")
        quality_level = "excellent"
    elif quality_score >= 50:
        print("  Quality: GOOD")
        quality_level = "good"
    elif quality_score >= 25:
        print("  Quality: ACCEPTABLE")
        quality_level = "acceptable"
    else:
        print("  Quality: POOR")
        quality_level = "poor"
    
    print(f"  Recommendation: {'Use for analysis' if quality_score >= 50 else 'Consider reprocessing'}")
    
    # Exit code based on quality
    if quality_score >= 50:
        sys.exit(0)  # Good quality
    else:
        sys.exit(1)  # Poor quality
        
except Exception as e:
    print(f"ERROR: Quality assessment failed: {str(e)}")
    sys.exit(1)
EOF

    local assessment_exit_code=$?
    
    if [ $assessment_exit_code -eq 0 ]; then
        log "OK" "[${sub}] Registration quality assessment: PASSED"
        return 0
    else
        log "WARN" "[${sub}] Registration quality assessment: FAILED"
        return 1
    fi
}

# End of Section 1

# --- Argument Parsing ---
parse_arguments() {
    BIDS_DIR="${USER_BIDS_DIR:-}"   # Set in USER CONFIGURATION block or via --bids flag
    SINGLE_SUBJECT=""
    PE_DIR="AP"
    ECHO_SPACING="0.062"
    SLM_MODEL="linear"
    SKIP_SYNB0=false
    RUN_CONNECTOME=true  # Set false or pass --skip-connectome to disable
    OMP_THREADS=""
    CLEANUP=true  # Default to true for space saving
    
    # Storage locations — set in USER CONFIGURATION block or via CLI flags
    STORAGE_FAST="${USER_STORAGE_FAST:-}"
    STORAGE_LARGE="${USER_STORAGE_LARGE:-}"
    
    # ML Registration options (new)
    USE_ML_REGISTRATION=false
    ML_MODEL_PATH=""
    AUTO_INSTALL_ML=false
    ML_QUICK_MODE=true
    FORCE_GPU=false
    ML_REGISTRATION_METHOD="auto"  # auto, voxelmorph, synthmorph, ants
    REGISTRATION_QUALITY_CHECK=true
    
    # Configuration file
    CONFIG_FILE=""
    
    # Operational modes
    DRY_RUN=false
    RESUME_MODE=false
    
    while [[ $# -gt 0 ]]; do
        case $1 in
            -b|--bids) BIDS_DIR="$2"; shift 2 ;;
            -s|--subject) SINGLE_SUBJECT="$2"; shift 2 ;;
            --pe) PE_DIR="$2"; shift 2 ;;
            --echo) ECHO_SPACING="$2"; shift 2 ;;
            --slm-model) SLM_MODEL="$2"; shift 2 ;;
            --skip-synb0) SKIP_SYNB0=true; shift ;;
            --skip-connectome) RUN_CONNECTOME=false; shift ;;
            --omp-threads) OMP_THREADS="$2"; shift 2 ;;
            --no-cleanup) CLEANUP=false; shift ;;
            --storage-e|--storage-fast) STORAGE_FAST="$2"; shift 2 ;;
            --storage-f|--storage-large) STORAGE_LARGE="$2"; shift 2 ;;
            
            # ML Registration options
            --use-ml-registration) USE_ML_REGISTRATION=true; shift ;;
            --ml-model-path) ML_MODEL_PATH="$2"; shift 2 ;;
            --auto-install-ml) AUTO_INSTALL_ML=true; shift ;;
            --ml-quick-mode) ML_QUICK_MODE=true; shift ;;
            --ml-full-mode) ML_QUICK_MODE=false; shift ;;
            --force-gpu) FORCE_GPU=true; shift ;;
            --ml-method) 
                ML_REGISTRATION_METHOD="$2"
                if [[ "$ML_REGISTRATION_METHOD" != "auto" && "$ML_REGISTRATION_METHOD" != "voxelmorph" && 
                      "$ML_REGISTRATION_METHOD" != "synthmorph" && "$ML_REGISTRATION_METHOD" != "ants" ]]; then
                    log "ERROR" "Invalid --ml-method: '$ML_REGISTRATION_METHOD'. Must be 'auto', 'voxelmorph', 'synthmorph', or 'ants'."
                    exit 1
                fi
                shift 2 ;;
            --skip-quality-check) REGISTRATION_QUALITY_CHECK=false; shift ;;
            --config) CONFIG_FILE="$2"; shift 2 ;;
            --dry-run) DRY_RUN=true; shift ;;
            --resume) RESUME_MODE=true; shift ;;
            --container-cmd) CONTAINER_CMD="$2"; shift 2 ;;
            
            -h|--help) print_usage; exit 0 ;;
            --help-ml) print_ml_usage; exit 0 ;;
            *) log "WARN" "Unknown argument: $1"; shift ;;
        esac
    done
    
    # Load configuration file if specified
    if [ -n "$CONFIG_FILE" ]; then
        load_config "$CONFIG_FILE"
    fi
    
    # Validate paths — abort early if not configured
    if [ -z "$BIDS_DIR" ]; then
        log "ERROR" "BIDS_DIR is not set. Edit the USER CONFIGURATION block at the top of this script, or pass --bids <dir>."
        exit 1
    fi
    if [ -z "$STORAGE_FAST" ] || [ -z "$STORAGE_LARGE" ]; then
        log "ERROR" "Storage paths are not set. Edit the USER CONFIGURATION block at the top of this script,"
        log "ERROR" "  or pass --storage-fast <path> --storage-large <path>."
        exit 1
    fi

    BIDS_DIR=$(realpath "$BIDS_DIR" 2>/dev/null) || { 
        log "ERROR" "Cannot resolve BIDS directory: $BIDS_DIR"
        exit 1
    }
    
    # Check storage locations exist (create if needed)
    for storage in "$STORAGE_FAST" "$STORAGE_LARGE"; do
        if [ ! -d "$storage" ]; then
            log "INFO" "Creating storage directory: $storage"
            mkdir -p "$storage" 2>/dev/null || {
                log "ERROR" "Storage location not found and cannot be created: $storage"
                exit 1
            }
        fi
    done
    
    # Validate phase encoding direction
    if [[ "$PE_DIR" != "AP" && "$PE_DIR" != "PA" && "$PE_DIR" != "LR" && "$PE_DIR" != "RL" ]]; then
        log "ERROR" "Invalid phase encoding direction: '$PE_DIR'. Must be AP, PA, LR, or RL."
        exit 1
    fi
    
    # Validate SLM model
    if [[ "$SLM_MODEL" != "linear" && "$SLM_MODEL" != "quadratic" ]]; then
        log "ERROR" "Invalid --slm-model: '$SLM_MODEL'. Must be 'linear' or 'quadratic'."
        exit 1
    fi
    
    # Validate echo spacing
    if ! [[ "$ECHO_SPACING" =~ ^[0-9]+\.?[0-9]*$ ]] || (( $(echo "$ECHO_SPACING <= 0" | bc -l 2>/dev/null || echo 1) )); then
        log "ERROR" "Invalid echo spacing: '$ECHO_SPACING'. Must be a positive number."
        exit 1
    fi
    
    # Validate ML model path if specified
    if [ -n "$ML_MODEL_PATH" ] && [ ! -f "$ML_MODEL_PATH" ]; then
        log "ERROR" "ML model file not found: $ML_MODEL_PATH"
        exit 1
    fi
    
    # Enable ML registration if specific method requested
    if [[ "$ML_REGISTRATION_METHOD" != "auto" ]]; then
        USE_ML_REGISTRATION=true
    fi
    
    # Set OpenMP threads
    if [ -n "$OMP_THREADS" ]; then
        if ! [[ "$OMP_THREADS" =~ ^[1-9][0-9]*$ ]]; then
            log "ERROR" "Invalid --omp-threads value: '$OMP_THREADS'. Must be a positive integer."
            exit 1
        fi
        export OMP_NUM_THREADS="$OMP_THREADS"
        export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS="$OMP_THREADS"
    else
        local max_threads=$(nproc)
        # Don't use all threads by default to leave some for system
        if [ $max_threads -gt 4 ]; then
            export OMP_NUM_THREADS=$((max_threads - 2))
        else
            export OMP_NUM_THREADS=$max_threads
        fi
        export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS="$OMP_NUM_THREADS"
    fi
    
    # Export ML settings for use in other functions
    export USE_ML_REGISTRATION ML_MODEL_PATH AUTO_INSTALL_ML ML_QUICK_MODE 
    export FORCE_GPU ML_REGISTRATION_METHOD REGISTRATION_QUALITY_CHECK
    export DRY_RUN RESUME_MODE
}

print_usage() {
    cat <<EOF
Integrated DTI Processing Pipeline v${SCRIPT_VERSION}
Storage-optimized version with ML-enhanced registration

Usage: $(basename "$0") [options]

BASIC OPTIONS:
    -b, --bids <dir>         Path to BIDS directory (REQUIRED unless set in config block)
    -s, --subject <id>       Process single subject (otherwise processes all)
    --pe <dir>               Phase encoding direction: AP, PA, LR, RL (default: AP)
    --echo <val>             Echo spacing in seconds (default: 0.062)
    --slm-model <model>      Eddy slm model: linear or quadratic (default: linear)
    --skip-synb0             Skip Synb0-DisCo correction
    --skip-connectome        Skip connectivity analysis
    --omp-threads <n>        OpenMP threads (default: auto-detected)
    --no-cleanup             Keep temporary files
    --storage-fast <path>    Fast SSD for outputs (REQUIRED unless set in config block)
    --storage-large <path>   Large drive for FreeSurfer (REQUIRED unless set in config block)
    --config <file>          Load configuration from file

ML REGISTRATION OPTIONS:
    --use-ml-registration    Enable ML-based registration (experimental)
    --ml-method <method>     ML method: auto, voxelmorph, synthmorph, ants (default: auto)
    --ml-model-path <path>   Path to custom ML model weights
    --auto-install-ml        Automatically install missing ML packages
    --ml-quick-mode          Use fast ML registration (default)
    --ml-full-mode           Use full ML registration (slower but more accurate)
    --force-gpu              Force GPU usage even if not recommended
    --skip-quality-check     Skip registration quality assessment

HELP OPTIONS:
    -h, --help               Show this help
    --help-ml                Show detailed ML registration help

Processing Stages:
    1. Basic preprocessing (DTI metrics)
    2. Post-hoc refinement (enhanced masks/correction)
    3. Connectivity analysis (FreeSurfer + tractography)
    4. NODDI estimation (microstructure modeling)

Storage Strategy:
    - Processing: BIDS derivatives dir (fast SSD recommended) - temporary
    - Synb0/Final outputs: --storage-fast path (fast SSD) - permanent
    - FreeSurfer: --storage-large path (large capacity) - permanent
    - Work files: Cleaned after each subject

Examples:
    # Basic processing (single subject, paths set in config block)
    $(basename "$0") -s sub-001 --omp-threads 8
    
    # Pass all paths on the command line
    $(basename "$0") -b /data/BIDS --storage-fast /ssd/outputs --storage-large /hdd/freesurfer -s sub-001

    # Enable ML registration with auto-install
    $(basename "$0") -s sub-001 --use-ml-registration --auto-install-ml
    
    # Use specific ML method
    $(basename "$0") --ml-method synthmorph --ml-full-mode
    
    # Process all subjects without connectivity
    $(basename "$0") --skip-connectome --use-ml-registration

System Requirements:
    - ~100GB free on BIDS drive for processing
    - ~500GB free across output and FreeSurfer drives
    - Docker (for Synb0-DisCo)
    - FreeSurfer (for connectivity analysis)
    - MRtrix3, FSL, ANTs

ML Requirements (optional):
    - Python 3.7+ with TensorFlow 2.x
    - VoxelMorph package for advanced registration
    - FreeSurfer 7.3+ for SynthMorph
    - NVIDIA GPU recommended for faster processing
EOF
}

print_ml_usage() {
    cat <<EOF
ML Registration Features - Detailed Guide
========================================

OVERVIEW:
ML registration features enhance traditional methods with machine learning
for improved accuracy and robustness.

ML METHODS:
1. VoxelMorph - Deep learning deformable registration
2. SynthMorph - FreeSurfer's synthetic registration  
3. Enhanced ANTs - ML-optimized parameters
4. Auto Selection - Automatically selects best method

PERFORMANCE MODES:
--ml-quick-mode: Faster processing, acceptable accuracy
--ml-full-mode: Maximum accuracy, longer processing

QUALITY LEVELS:
- EXCELLENT (75-100%): Optimal for analysis
- GOOD (50-74%): Suitable for most analyses
- ACCEPTABLE (25-49%): May need manual review
- POOR (0-24%): Requires reprocessing

INSTALLATION:
pip install tensorflow voxelmorph scikit-learn scipy nibabel

For detailed information: $(basename "$0") --help
EOF
}

# End of Section 2

# --- Environment Setup ---

# Sub-function: ML environment setup (extracted from setup_environment for clarity)
_setup_ml_environment() {
    log "INFO" "Setting up ML registration environment..."
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ] || [ "${ML_REGISTRATION_METHOD:-auto}" != "auto" ]; then
        check_ml_dependencies
        
        if [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
            setup_ml_models
            log "OK" "ML registration environment ready"
        else
            log "WARN" "ML registration requested but dependencies not satisfied"
            if [ "${AUTO_INSTALL_ML:-false}" = true ]; then
                log "INFO" "Attempting to install ML dependencies..."
            else
                log "INFO" "Use --auto-install-ml to install dependencies automatically"
                log "INFO" "Or install manually: pip install tensorflow voxelmorph scikit-learn scipy nibabel"
            fi
        fi
    else
        log "INFO" "ML registration disabled - using traditional methods only"
        export ML_REGISTRATION_AVAILABLE=false
        export SYNTHMORPH_AVAILABLE=false
        export GPU_AVAILABLE=false
    fi
    
    if [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        ML_WORK_DIR="${WORK_DIR}/ml_registration"
        mkdir -p "$ML_WORK_DIR"
        export ML_WORK_DIR
        create_voxelmorph_registration_script
    fi
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "INFO" "Checking system resources for ML registration..."
        if ! check_memory_usage 8; then
            log "WARN" "Limited memory detected - ML registration may be slower"
            log "INFO" "Consider using --ml-quick-mode for memory efficiency"
        fi
        
        if [ "${GPU_AVAILABLE:-false}" = true ]; then
            log "OK" "GPU acceleration available for ML registration"
            if command -v nvidia-smi &>/dev/null; then
                local gpu_mem=$(safe_int "$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits | head -1)")
                if [ -n "$gpu_mem" ] && [ "$gpu_mem" -gt 4000 ]; then
                    log "OK" "GPU memory: ${gpu_mem}MB available (sufficient for ML registration)"
                elif [ -n "$gpu_mem" ]; then
                    log "WARN" "GPU memory: ${gpu_mem}MB available (may be limited for large images)"
                fi
            fi
        else
            log "INFO" "No GPU detected - using CPU for ML registration"
        fi
    fi
}

setup_environment() {
    log "INFO" "Setting up processing environment..."
    export PATH="$PATH:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin"
    # export PATH="/usr/bin:$PATH"  # Disabled to preserve venv
    export PYTHONPATH="${PYTHONPATH:-}:/usr/local/fsl/lib/python3.12/site-packages"
log "INFO" "Appended FSL Python packages to PYTHONPATH (venv takes precedence)"

    # All processing happens in BIDS derivatives directory
    DERIV_DIR="${BIDS_DIR}/derivatives"
    WORK_DIR="${DERIV_DIR}/work"
    LOG_DIR="${DERIV_DIR}/logs"
    
    # Stage 1 outputs (kept temporarily on C)
    SYNB0_DIR="${DERIV_DIR}/synb0-disco"
    MRTRIX_DIR="${DERIV_DIR}/mrtrix3"
    POSTHOC_DIR="${DERIV_DIR}/posthoc"
    
    # External storage directories
    EXTERNAL_SYNB0="${STORAGE_FAST}/derivatives/synb0-disco"
    EXTERNAL_MRTRIX="${STORAGE_FAST}/derivatives/mrtrix3"
    EXTERNAL_POSTHOC="${STORAGE_FAST}/derivatives/posthoc"
    EXTERNAL_FS="${STORAGE_LARGE}/derivatives/freesurfer"
    EXTERNAL_QC="${STORAGE_FAST}/derivatives/qc_integrated"
    
    # Create necessary directories
    mkdir -p "$WORK_DIR" "$LOG_DIR" "$SYNB0_DIR" "$MRTRIX_DIR" "$POSTHOC_DIR"
    
    # Initialize structured event log
    JSONL_LOG="${LOG_DIR}/pipeline_events.jsonl"
    export JSONL_LOG
    log "INFO" "Structured event log: $JSONL_LOG"
    
    create_python_helper
    mkdir -p "$EXTERNAL_SYNB0" "$EXTERNAL_MRTRIX" "$EXTERNAL_POSTHOC" "$EXTERNAL_FS" "$EXTERNAL_QC"
    
    # FreeSurfer setup
    export SUBJECTS_DIR="${DERIV_DIR}/freesurfer"  # Temporary location under BIDS
    mkdir -p "$SUBJECTS_DIR"
    
    # Check for existing FreeSurfer directories in external storage
    EXTERNAL_SUBJECTS_DIR="${EXTERNAL_FS}"
    
    # Find FreeSurfer license
    if [ -f "$HOME/.freesurfer/license.txt" ]; then 
        FREESURFER_LICENSE="$HOME/.freesurfer/license.txt"
    elif [[ -n "${FREESURFER_HOME-}" && -f "$FREESURFER_HOME/license.txt" ]]; then 
        FREESURFER_LICENSE="$FREESURFER_HOME/license.txt"
    else 
        FREESURFER_LICENSE=""
    fi
    
    # Find MRtrix3 home if not set
    if [ -z "${MRTRIX_HOME:-}" ] && command -v mrconvert >/dev/null 2>&1; then
        MRTRIX_HOME=$(dirname $(dirname $(which mrconvert)))
        export MRTRIX_HOME
    fi
    
    # Container runtime detection (Docker, Singularity, or Apptainer)
    detect_container_runtime
    DOCKER_CMD="$CONTAINER_CMD"  # backward compatibility alias
    
    # ML Registration Environment Setup
    _setup_ml_environment
    
    # Check all required tools (consolidated single check)
    log "INFO" "Checking required tools..."
    if ! check_required_tools; then
        exit 1
    fi
    
    # Additional ML-specific tool checks
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_METHOD:-auto}" = "ants" ]; then
        if ! command -v antsRegistration >/dev/null 2>&1; then
            log "WARN" "antsRegistration not found - enhanced ANTs registration unavailable"
        fi
    fi
    
    # FreeSurfer environment check for connectivity
    if [ "$RUN_CONNECTOME" = true ]; then
        if [ -z "${FREESURFER_HOME:-}" ]; then
            log "ERROR" "FREESURFER_HOME not set. Required for connectivity analysis."
            exit 1
        fi
        if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
            log "OK" "SynthMorph available for enhanced T1w-DWI registration"
        fi
    fi

    # Python and ML package checks
    local -a missing_tools=()
    if ! command -v $PYTHON_EXECUTABLE >/dev/null 2>&1; then
        missing_tools+=("$PYTHON_EXECUTABLE")
    else
        # Debug: Show which Python we're using
        log "INFO" "Python path: $(which $PYTHON_EXECUTABLE)"
        log "INFO" "Python version: $($PYTHON_EXECUTABLE --version 2>&1)"
        
        # Check core Python packages
        local python_packages=("numpy" "scipy" "nibabel")
        for pkg in "${python_packages[@]}"; do
            if ! $PYTHON_EXECUTABLE -c "import $pkg" &>/dev/null 2>&1; then
                log "WARN" "Python package '$pkg' not found"
                missing_tools+=("$PYTHON_EXECUTABLE-$pkg")
            fi
        done
        
        # Check AMICO for NODDI
        log "INFO" "Testing AMICO import..."
        if $PYTHON_EXECUTABLE -W ignore -c "import amico" &>/dev/null 2>&1; then
            log "OK" "AMICO available for NODDI estimation"
        else
            log "ERROR" "Python package 'dmri-amico' not found. Please run: pip install dmri-amico"
            log "ERROR" "Try: $(which $PYTHON_EXECUTABLE) -m pip install dmri-amico"
            #exit 1  # Bypassed - AMICO verified working
        fi
        
        # ML-specific package checks (if ML enabled)
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            log "INFO" "Checking ML-specific packages..."
            
            # These checks were already done in check_ml_dependencies, but we can report status
            if [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
                log "OK" "All ML packages available"
            else
                log "WARN" "Some ML packages missing - traditional registration will be used as fallback"
            fi
        fi
    fi
    
    if [ ${#missing_tools[@]} -gt 0 ]; then
        log "ERROR" "Missing required tools: ${missing_tools[*]}"
        log "ERROR" "Please ensure system utilities, MRtrix3, FSL, and ANTs are properly installed"
        
        # Provide specific installation guidance
        log "INFO" "Installation suggestions:"
        log "INFO" "  - System utilities (bc): apt-get install bc (Ubuntu/Debian) or yum install bc (RHEL/CentOS)"
        log "INFO" "  - MRtrix3: https://mrtrix.readthedocs.io/en/latest/installation/linux_install.html"
        log "INFO" "  - FSL: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation"
        log "INFO" "  - ANTs: http://stnava.github.io/ANTs/"
        
        if [[ " ${missing_tools[*]} " =~ " $PYTHON_EXECUTABLE" ]]; then
            log "INFO" "  - Python packages: pip install numpy scipy nibabel dmri-amico"
        fi
        
        exit 1
    fi
    
    # Check disk space on all drives
    log "INFO" "Checking disk space..."
    local min_c_space=80
    local min_e_space=100
    local min_f_space=200
    
    # Adjust space requirements based on enabled features
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        min_c_space=90  # Extra space for ML models and processing
        log "INFO" "ML registration enabled - increased BIDS drive space requirement to ${min_c_space}GB"
    fi
    
    if [ "$RUN_CONNECTOME" = true ]; then
        min_f_space=300  # More space needed for FreeSurfer
        log "INFO" "Connectivity analysis enabled - increased large storage space requirement to ${min_f_space}GB"
    fi
    
    if ! check_disk_space "$(dirname "$BIDS_DIR")" $min_c_space; then
        log "ERROR" "Insufficient space on BIDS drive (need at least ${min_c_space}GB)"
        exit 1
    fi
    
    if ! check_disk_space "$STORAGE_FAST" $min_e_space; then
        log "WARN" "Limited space on fast storage (recommended: ${min_e_space}GB)"
    fi
    
    if ! check_disk_space "$STORAGE_LARGE" $min_f_space; then
        log "WARN" "Limited space on large storage (recommended: ${min_f_space}GB)"
    fi

    # Check write permissions
    for dir in "$(dirname "$DERIV_DIR")" "$STORAGE_FAST" "$STORAGE_LARGE"; do
        if ! check_write_permission "$dir"; then
            log "ERROR" "No write permission for: $dir"
            exit 1
        fi
    done
    
    # Optimize thread usage based on available resources
    log "INFO" "Optimizing thread usage..."
    local original_threads=$OMP_NUM_THREADS
    
    # Reduce threads if ML registration is enabled to leave resources for GPU
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${GPU_AVAILABLE:-false}" = true ]; then
        local suggested_threads=$(get_optimal_threads $((OMP_NUM_THREADS - 2)))
        if [ $suggested_threads -lt $OMP_NUM_THREADS ]; then
            export OMP_NUM_THREADS=$suggested_threads
            export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=$suggested_threads
            log "INFO" "Reduced CPU threads to ${suggested_threads} (was ${original_threads}) to optimize for GPU usage"
        fi
    fi
    
    # Set ML-specific thread limits
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        # Limit TensorFlow CPU usage
        export TF_NUM_INTRAOP_THREADS=$((OMP_NUM_THREADS / 2))
        export TF_NUM_INTEROP_THREADS=1
        log "INFO" "TensorFlow CPU threads: ${TF_NUM_INTRAOP_THREADS}"
    fi
    
        # Export all variables for use in functions
    export BIDS_DIR DERIV_DIR WORK_DIR LOG_DIR SYNB0_DIR MRTRIX_DIR POSTHOC_DIR
    export EXTERNAL_SYNB0 EXTERNAL_MRTRIX EXTERNAL_POSTHOC EXTERNAL_FS EXTERNAL_QC
    export SUBJECTS_DIR EXTERNAL_SUBJECTS_DIR FREESURFER_LICENSE DOCKER_CMD
    export PE_DIR ECHO_SPACING SLM_MODEL SKIP_SYNB0 RUN_CONNECTOME CLEANUP
    export FREESURFER_HOME MRTRIX_HOME OMP_NUM_THREADS ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS
    export STORAGE_FAST STORAGE_LARGE
    
    # Export ML-specific variables
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        export ML_WORK_DIR ML_MODELS_DIR
        export TF_NUM_INTRAOP_THREADS TF_NUM_INTEROP_THREADS
    fi
   
    # Log configuration
    log "INFO" "=========================================="
    log "INFO" "Configuration Summary:"
    log "INFO" "  BIDS directory: $BIDS_DIR"
    log "INFO" "  Fast storage (SSD): $STORAGE_FAST"
    log "INFO" "  Large storage: $STORAGE_LARGE"
    log "INFO" "  OpenMP threads: $OMP_NUM_THREADS"
    log "INFO" "  Docker: ${DOCKER_CMD:-not available}"
    log "INFO" "  FreeSurfer license: ${FREESURFER_LICENSE:-not found}"
    log "INFO" "  MRtrix3 home: ${MRTRIX_HOME:-not found}"
    
    # ML Configuration Summary
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "INFO" "  ML Registration: ENABLED"
        log "INFO" "    Method: ${ML_REGISTRATION_METHOD:-auto}"
        log "INFO" "    Mode: $([ "${ML_QUICK_MODE:-true}" = true ] && echo "Quick" || echo "Full")"
        log "INFO" "    GPU Available: ${GPU_AVAILABLE:-false}"
        log "INFO" "    SynthMorph: ${SYNTHMORPH_AVAILABLE:-false}"
        log "INFO" "    VoxelMorph: $([ "${ML_REGISTRATION_AVAILABLE:-false}" = true ] && echo "Available" || echo "Not available")"
        log "INFO" "    Quality Check: ${REGISTRATION_QUALITY_CHECK:-true}"
        log "INFO" "    TF CPU Threads: ${TF_NUM_INTRAOP_THREADS:-N/A}"
        
        if [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
            log "INFO" "    ML Models Dir: ${ML_MODELS_DIR:-N/A}"
        fi
    else
        log "INFO" "  ML Registration: DISABLED"
    fi
    
    # Processing Configuration
    log "INFO" "  Processing Configuration:"
    log "INFO" "    Phase Encoding: $PE_DIR"
    log "INFO" "    Echo Spacing: $ECHO_SPACING"
    log "INFO" "    SLM Model: $SLM_MODEL"
    log "INFO" "    Skip Synb0: $SKIP_SYNB0"
    log "INFO" "    Run Connectome: $RUN_CONNECTOME"
    log "INFO" "    Cleanup: $CLEANUP"
    
    # Resource Summary
    log "INFO" "  System Resources:"
    local cpu_count=$(nproc)
    local mem_total=$(free -g | awk '/^Mem:/{print $2}')
    local mem_available=$(free -g | awk '/^Mem:/{print $7}')
    
    log "INFO" "    CPU Cores: $cpu_count (using $OMP_NUM_THREADS)"
    log "INFO" "    Memory: ${mem_available}GB available / ${mem_total}GB total"
    
    # GPU Information
    if command -v nvidia-smi &>/dev/null; then
        local gpu_info=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits 2>/dev/null | head -1)
        if [ -n "$gpu_info" ]; then
            log "INFO" "    GPU: $gpu_info"
        fi
    fi
    
    # Disk space summary
    local c_space=$(df -h "$(dirname "$BIDS_DIR")" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    local e_space=$(df -h "$STORAGE_FAST" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    local f_space=$(df -h "$STORAGE_LARGE" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    
    log "INFO" "    Disk Space - C: $c_space, E: $e_space, F: $f_space"
    
    log "INFO" "=========================================="
    
    # Performance optimization warnings
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "INFO" "ML Registration Performance Tips:"
        
        if [ "${GPU_AVAILABLE:-false}" = false ]; then
            log "INFO" "  - Consider installing CUDA and TensorFlow-GPU for 5-10x speedup"
        fi
        
        if [ $mem_available -lt 16 ]; then
            log "INFO" "  - Consider using --ml-quick-mode for memory efficiency"
        fi
        
        if [ "${ML_QUICK_MODE:-true}" = false ] && [ $cpu_count -lt 8 ]; then
            log "INFO" "  - Consider using --ml-quick-mode on systems with <8 CPU cores"
        fi
        
        # Estimate processing time
        local estimated_time_per_subject="Unknown"
        if [ "${GPU_AVAILABLE:-false}" = true ]; then
            estimated_time_per_subject="2-4 hours"
        else
            estimated_time_per_subject="4-8 hours"
        fi
        log "INFO" "  - Estimated time per subject: $estimated_time_per_subject"
    fi
    
    # Final validation
    log "INFO" "Performing final environment validation..."
    
    # Test basic operations
    if ! $PYTHON_EXECUTABLE -c "import sys; print('Python OK')" &>/dev/null; then
        log "ERROR" "Python environment test failed"
        exit 1
    fi
    
    # Test file operations
    local test_file="${WORK_DIR}/.env_test_$$"
    if ! echo "test" > "$test_file" 2>/dev/null || ! rm -f "$test_file" 2>/dev/null; then
        log "ERROR" "File operation test failed in work directory"
        exit 1
    fi
    
    # Test external storage
    for storage in "$STORAGE_FAST" "$STORAGE_LARGE"; do
        local test_file="${storage}/.env_test_$$"
        if ! echo "test" > "$test_file" 2>/dev/null || ! rm -f "$test_file" 2>/dev/null; then
            log "ERROR" "File operation test failed in $storage"
            exit 1
        fi
    done
    
    # ML environment validation
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        log "INFO" "Validating ML environment..."
        
        # Test TensorFlow import (if available)
        if $PYTHON_EXECUTABLE -c "import tensorflow as tf; print(f'TensorFlow {tf.__version__} OK')" &>/dev/null 2>&1; then
            log "OK" "TensorFlow validation passed"
        else
            log "WARN" "TensorFlow validation failed - falling back to traditional registration"
            export ML_REGISTRATION_AVAILABLE=false
        fi
        
        # Test GPU if claimed to be available
        if [ "${GPU_AVAILABLE:-false}" = true ]; then
            if $PYTHON_EXECUTABLE -c "import tensorflow as tf; gpus=tf.config.list_physical_devices('GPU'); print(f'GPU test: {len(gpus)} GPU(s) detected')" 2>/dev/null | grep -q "GPU(s) detected"; then
                log "OK" "GPU validation passed"
            else
                log "WARN" "GPU validation failed - using CPU only"
                export GPU_AVAILABLE=false
            fi
        fi
        
        # Create a simple test to ensure ML registration can run
        local ml_test_script="${ML_WORK_DIR}/ml_test.py"
        cat > "$ml_test_script" << 'EOF'
import numpy as np
try:
    # Test basic numpy operations
    test_array = np.random.rand(10, 10, 10)
    result = np.mean(test_array)
    print(f"ML environment test passed: {result:.4f}")
except Exception as e:
    print(f"ML environment test failed: {e}")
    exit(1)
EOF
        
        if $PYTHON_EXECUTABLE "$ml_test_script" &>/dev/null; then
            log "OK" "ML environment test passed"
            rm -f "$ml_test_script"
        else
            log "WARN" "ML environment test failed - disabling ML registration"
            export ML_REGISTRATION_AVAILABLE=false
            export USE_ML_REGISTRATION=false
        fi
    fi
    
    # Final status report
    local features_enabled=()
    local features_disabled=()
    
    if [ "$SKIP_SYNB0" = false ]; then
        if [ -n "$DOCKER_CMD" ] && [ -n "$FREESURFER_LICENSE" ]; then
            features_enabled+=("Synb0-DisCo")
        else
            features_disabled+=("Synb0-DisCo (Docker/license missing)")
        fi
    else
        features_disabled+=("Synb0-DisCo (skipped)")
    fi
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        features_enabled+=("ML Registration")
    elif [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        features_disabled+=("ML Registration (dependencies missing)")
    else
        features_disabled+=("ML Registration (disabled)")
    fi
    
    if [ "$RUN_CONNECTOME" = true ]; then
        if [ -n "${FREESURFER_HOME:-}" ]; then
            features_enabled+=("Connectivity Analysis")
        else
            features_disabled+=("Connectivity Analysis (FreeSurfer missing)")
        fi
    else
        features_disabled+=("Connectivity Analysis (skipped)")
    fi
    
    features_enabled+=("DTI Preprocessing" "NODDI Estimation")
    
    log "INFO" "Pipeline Features Status:"
    if [ ${#features_enabled[@]} -gt 0 ]; then
        log "OK" "  Enabled: ${features_enabled[*]}"
    fi
    if [ ${#features_disabled[@]} -gt 0 ]; then
        log "WARN" "  Disabled: ${features_disabled[*]}"
    fi
    
    # Set up monitoring
    monitor_resources "system" "environment_setup"
    
    log "OK" "Environment setup completed successfully"
    
    # Create processing summary for later reference
    local summary_file="${LOG_DIR}/environment_summary.txt"
    {
        echo "Environment Setup Summary"
        echo "========================="
        echo "Date: $(date)"
        echo "Script Version: $SCRIPT_VERSION"
        echo ""
        echo "Directories:"
        echo "  BIDS: $BIDS_DIR"
        echo "  Work: $WORK_DIR"
        echo "  Fast storage: $STORAGE_FAST"
        echo "  Large storage: $STORAGE_LARGE"
        echo ""
        echo "Configuration:"
        echo "  ML Registration: ${USE_ML_REGISTRATION:-false}"
        echo "  ML Method: ${ML_REGISTRATION_METHOD:-auto}"
        echo "  GPU Available: ${GPU_AVAILABLE:-false}"
        echo "  Threads: $OMP_NUM_THREADS"
        echo "  Phase Encoding: $PE_DIR"
        echo "  Echo Spacing: $ECHO_SPACING"
        echo ""
        echo "Features:"
        [ ${#features_enabled[@]} -gt 0 ] && echo "  Enabled: ${features_enabled[*]}"
        [ ${#features_disabled[@]} -gt 0 ] && echo "  Disabled: ${features_disabled[*]}"
        echo ""
        echo "System Resources:"
        echo "  CPU Cores: $(nproc) (using $OMP_NUM_THREADS)"
        echo "  Memory: $(free -h | grep '^Mem' | awk '{print $7 " available / " $2 " total"}')"
        echo "  BIDS drive: $(df -h "$(dirname "$BIDS_DIR")" | tail -1 | awk '{print $4 " available"}')"
        echo "  Fast storage: $(df -h "$STORAGE_FAST" | tail -1 | awk '{print $4 " available"}')"
        echo "  Large storage: $(df -h "$STORAGE_LARGE" | tail -1 | awk '{print $4 " available"}')"
    } > "$summary_file"
    
    log "INFO" "Environment summary saved: $summary_file"
}

# End of Section 3

# --- Stage 1: Synb0-DisCo and Basic Preprocessing ---
run_synb0() {
    local sub=$1
    if [ "$SKIP_SYNB0" = true ]; then return 0; fi
    if [ -z "$DOCKER_CMD" ] || [ -z "$FREESURFER_LICENSE" ]; then 
        log "WARN" "[${sub}] Skipping Synb0-DisCo (no Docker or license)"
        return 1
    fi
    
    # Check if already processed in external storage
    if [ -f "${EXTERNAL_SYNB0}/${sub}/OUTPUTS/b0_u.nii.gz" ]; then
        log "INFO" "[${sub}] Synb0-DisCo already processed (found in external storage)"
        return 0
    fi
    
    log "INFO" "[${sub}] Starting Synb0-DisCo"
    monitor_resources "$sub" "synb0_start"
    
    local inputs="${SYNB0_DIR}/${sub}/INPUTS"
    local outputs="${SYNB0_DIR}/${sub}/OUTPUTS"
    mkdir -p "$inputs" "$outputs"
    
    # Extract b0
    mrconvert "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.nii.gz" \
        -fslgrad "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bvec" "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bval" \
        -coord 3 0 - 2>/dev/null | mrconvert - "${inputs}/b0.nii.gz" -quiet -force || return 1
    
    cp -f "${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz" "${inputs}/T1.nii.gz" || return 1
    
    # Create acqparams
    case $PE_DIR in 
        AP) echo "0 -1 0 ${ECHO_SPACING}" ;; 
        PA) echo "0 1 0 ${ECHO_SPACING}" ;; 
        LR) echo "1 0 0 ${ECHO_SPACING}" ;; 
        RL) echo "-1 0 0 ${ECHO_SPACING}" ;; 
    esac > "${inputs}/acqparams.txt"
    
    # Run Synb0 using detected container runtime
    log "INFO" "[${sub}] Running Synb0-DisCo (estimated time: 15-30 minutes)"
    local start_time=$(date +%s)
    
    case "$DOCKER_CMD" in
        *docker*)
            ${DOCKER_CMD} run --rm \
                -v "${inputs}:/INPUTS/" \
                -v "${outputs}:/OUTPUTS/" \
                -v "${FREESURFER_LICENSE}:/extra/freesurfer/license.txt" \
                --user "$(id -u):$(id -g)" \
                leonyichencai/synb0-disco:v3.1 --notopup &> "${outputs}/synb0_log.txt"
            # NOTE: --user may cause permission failures with some synb0-disco versions
            # that write to root-owned directories inside the container. If Synb0 fails
            # with permission errors, try removing the --user flag above.
            ;;
        singularity|apptainer)
            local sif_cache="${WORK_DIR}/.sif_cache"
            mkdir -p "$sif_cache"
            local sif_image="${sif_cache}/synb0-disco_v3.1.sif"
            if [ ! -f "$sif_image" ]; then
                log "INFO" "[${sub}] Building Singularity image for synb0-disco (first time only)..."
                ${DOCKER_CMD} build "$sif_image" "docker://leonyichencai/synb0-disco:v3.1" || return 1
            fi
            ${DOCKER_CMD} exec \
                --bind "${inputs}:/INPUTS/,${outputs}:/OUTPUTS/,${FREESURFER_LICENSE}:/extra/freesurfer/license.txt" \
                "$sif_image" --notopup &> "${outputs}/synb0_log.txt"
            ;;
        *)
            log "ERROR" "[${sub}] Unsupported container runtime for Synb0: $DOCKER_CMD"
            return 1
            ;;
    esac
    
    local exit_code=$?
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $exit_code -eq 0 ] && [ -f "${outputs}/b0_u.nii.gz" ]; then
        log "OK" "[${sub}] Synb0-DisCo completed in $((duration / 60)) minutes"
        
        # Move to external storage immediately
        log "INFO" "[${sub}] Moving Synb0 outputs to external storage"
        mkdir -p "${EXTERNAL_SYNB0}/${sub}"
        if retry_operation rsync -av --remove-source-files "${SYNB0_DIR}/${sub}/" "${EXTERNAL_SYNB0}/${sub}/"; then
            log "OK" "[${sub}] Synb0 outputs moved successfully"
            create_checkpoint "$sub" "synb0_complete"
        else
            log "ERROR" "[${sub}] Failed to move Synb0 outputs"
            return 1
        fi
    else
        log "WARN" "[${sub}] Synb0-DisCo failed. Check log: ${outputs}/synb0_log.txt"
        if [ -f "${outputs}/synb0_log.txt" ]; then
            log "WARN" "[${sub}] Last 10 lines of Synb0 log:"
            tail -n 10 "${outputs}/synb0_log.txt" >&2
        fi
        return 1
    fi
}

run_basic_preprocessing() {
    local sub=$1
    local outdir="${MRTRIX_DIR}/${sub}"
    local workdir="${WORK_DIR}/${sub}/preproc"
    mkdir -p "$outdir" "$workdir"
    safe_cd "$workdir" || return 1
    
    log "INFO" "[${sub}] Starting basic preprocessing (Stage 1)"
    monitor_resources "$sub" "preprocessing_start"
    update_progress "$sub" "preprocessing" 10
    
    # Check if already done in external storage
    if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz" ]; then
        log "INFO" "[${sub}] Basic preprocessing already completed (found in external storage)"
        safe_cd_return
        return 0
    fi
    
    # Convert DWI to MIF
    log "INFO" "[${sub}] Converting DWI to MIF format"
    mrconvert "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.nii.gz" dwi_raw.mif \
        -fslgrad "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bvec" "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bval" \
        -quiet -force || { log "ERROR" "[${sub}] mrconvert failed"; return 1; }
    
    update_progress "$sub" "preprocessing" 20
    
    # Denoise
    log "INFO" "[${sub}] Running MP-PCA denoising"
    dwidenoise dwi_raw.mif dwi_denoised.mif -noise noise.mif -quiet -force || { 
        log "ERROR" "[${sub}] dwidenoise failed"; return 1; 
    }
    
    update_progress "$sub" "preprocessing" 30
    
    # Remove Gibbs ringing artifacts
    log "INFO" "[${sub}] Removing Gibbs ringing artifacts"
    mrdegibbs dwi_denoised.mif dwi_degibbs.mif -quiet -force || { 
        log "ERROR" "[${sub}] mrdegibbs failed"; return 1; 
    }
    
    update_progress "$sub" "preprocessing" 40
    
    # Prepare for eddy correction and save b0 for later use
    log "INFO" "[${sub}] Preparing for distortion correction"
    dwiextract dwi_degibbs.mif - -bzero -quiet | mrmath - mean b0_orig.mif -axis 3 -quiet -force
    
    # Also save b0_mean in both formats for later use
    mrconvert b0_orig.mif b0_mean.mif -quiet -force
    mrconvert b0_mean.mif b0_mean.nii.gz -quiet -force
    log "OK" "[${sub}] b0 mean volume saved for later use"
    # Copy b0_mean to output directory so it gets included in external storage (preserve for registration analysis)
    cp b0_mean.nii.gz "${outdir}/" 2>/dev/null || true
    cp b0_mean.nii.gz "${outdir}/${sub}_b0_mean.nii.gz" 2>/dev/null || true

    # --- Fix/alias names for other tools that expect "mean_bzero" ---
    # Some tools (SynthMorph/ANTs internal scripts) may look for mean_bzero.*.
    # Create copies (or symlinks when supported) so either name works.
    if [ -f b0_mean.mif ] && [ ! -f mean_bzero.mif ]; then
        cp -f b0_mean.mif mean_bzero.mif 2>/dev/null || true
    fi
    if [ -f b0_mean.nii.gz ]; then
        # create both gz and uncompressed variants used in various steps
        if [ ! -f mean_bzero.nii.gz ]; then
            cp -f b0_mean.nii.gz mean_bzero.nii.gz 2>/dev/null || true
        fi
        if [ ! -f mean_bzero.nii ]; then
            gunzip -c b0_mean.nii.gz > mean_bzero.nii 2>/dev/null || true
        fi
    fi

    update_progress "$sub" "preprocessing" 50
    
    safe_cd_return
    log "INFO" "[${sub}] Basic preprocessing stage 1 completed"
    return 0
}

# Continuation of run_basic_preprocessing
run_eddy_and_bias_correction() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/preproc"
    local outdir="${MRTRIX_DIR}/${sub}"
    
    # Check if workdir exists
    if [ ! -d "$workdir" ]; then
        log "ERROR" "[${sub}] Work directory not found: $workdir"
        return 1
    fi
    
    safe_cd "$workdir" || return 1
    
    log "INFO" "[${sub}] Running motion and distortion correction"
    monitor_resources "$sub" "eddy_start"
    update_progress "$sub" "preprocessing" 60
    
    local EDDY_OPTS=" --slm=${SLM_MODEL} --repol"
    local registration_successful=false
    
    # ML-Enhanced Registration Strategy
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        log "ML" "[${sub}] Attempting ML-enhanced motion correction"
        
        # Determine ML method to use
        local ml_method="${ML_REGISTRATION_METHOD:-auto}"
        if [ "$ml_method" = "auto" ]; then
            # Auto-select best available method
            if [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
                ml_method="synthmorph"
                log "ML" "[${sub}] Auto-selected SynthMorph for registration"
            elif [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
                ml_method="voxelmorph"
                log "ML" "[${sub}] Auto-selected VoxelMorph for registration"
            else
                ml_method="ants"
                log "ML" "[${sub}] Auto-selected enhanced ANTs for registration"
            fi
        fi
        
        # Export DWI for ML registration
        mrconvert dwi_degibbs.mif dwi_for_ml.nii.gz \
            -export_grad_fsl dwi_ml.bvec dwi_ml.bval \
            -quiet -force
        
        local ml_start_time=$(date +%s)
        
        # Try ML registration based on selected method
        case $ml_method in
            "voxelmorph")
                log "ML" "[${sub}] Using VoxelMorph for DWI registration"
                if run_voxelmorph_dwi_registration "$sub" "dwi_for_ml.nii.gz" "dwi_ml_registered.nii.gz" "${ML_QUICK_MODE:-true}"; then
                    # Convert back to MIF
                    mrconvert dwi_ml_registered.nii.gz dwi_preproc.mif \
                        -fslgrad dwi_ml.bvec dwi_ml.bval \
                        -quiet -force
                    registration_successful=true
                    
                    # Quality check if enabled
                    if [ "${REGISTRATION_QUALITY_CHECK:-true}" = true ]; then
                        log "ML" "[${sub}] Performing VoxelMorph quality assessment"
                        if ! validate_ml_registration_quality "$sub" "dwi_for_ml.nii.gz" "dwi_ml_registered.nii.gz" "VoxelMorph"; then
                            log "WARN" "[${sub}] VoxelMorph quality check failed, falling back to traditional method"
                            registration_successful=false
                        fi
                    fi
                fi
                ;;
                
            "synthmorph")
                log "ML" "[${sub}] Using SynthMorph for enhanced registration"
                # For SynthMorph, we still need to use traditional dwifslpreproc for DWI-specific processing
                # But we can use SynthMorph for T1w-DWI registration later
                registration_successful=false
                log "ML" "[${sub}] SynthMorph will be used for T1w-DWI registration in post-processing"
                ;;
                
            "ants")
                log "ML" "[${sub}] Using enhanced ANTs registration"
                # Enhanced ANTs with optimized parameters
                # For now, fall back to traditional method but with enhanced parameters
                registration_successful=false
                log "ML" "[${sub}] Enhanced ANTs parameters will be used"
                EDDY_OPTS=" --slm=${SLM_MODEL} --repol --ol_nstd=4 --ol_nvox=250 --ol_type=both"
                ;;
        esac
        
        local ml_end_time=$(date +%s)
        local ml_duration=$((ml_end_time - ml_start_time))
        
        if [ "$registration_successful" = true ]; then
            log "OK" "[${sub}] ML registration completed successfully in $((ml_duration / 60)) minutes"
            update_progress "$sub" "preprocessing" 80
        else
            log "WARN" "[${sub}] ML registration failed or unavailable, using traditional method"
        fi
    fi
    
    # Traditional FSL registration (fallback or primary method)
    if [ "$registration_successful" = false ]; then
        log "INFO" "[${sub}] Running traditional motion/distortion correction"
        
        # Check for synthetic b0
        local synb0_available=false
        if [ -f "${EXTERNAL_SYNB0}/${sub}/OUTPUTS/b0_u.nii.gz" ]; then
            log "INFO" "[${sub}] Using synthetic b0 for distortion correction"
            mrconvert "${EXTERNAL_SYNB0}/${sub}/OUTPUTS/b0_u.nii.gz" b0_syn.mif -quiet -force
            mrcat b0_orig.mif b0_syn.mif b0_pair.mif -axis 3 -quiet -force
            synb0_available=true
        fi
        
        local fsl_start_time=$(date +%s)
        
        if [ "$synb0_available" = true ]; then
            dwifslpreproc dwi_degibbs.mif dwi_preproc.mif \
                -pe_dir "$PE_DIR" -rpe_pair -se_epi b0_pair.mif -align_seepi \
                -eddy_options "${EDDY_OPTS}" -nthreads $(get_optimal_threads 4) -force || { 
                log "ERROR" "[${sub}] dwifslpreproc with synthetic b0 failed"; return 1; 
            }
        else
            log "INFO" "[${sub}] No synthetic b0 found, using motion correction only"
            dwifslpreproc dwi_degibbs.mif dwi_preproc.mif \
                -pe_dir "$PE_DIR" -rpe_none \
                -eddy_options "${EDDY_OPTS}" -nthreads $(get_optimal_threads 4) -force || { 
                log "ERROR" "[${sub}] dwifslpreproc failed"; return 1; 
            }
        fi
        
        local fsl_end_time=$(date +%s)
        local fsl_duration=$((fsl_end_time - fsl_start_time))
        
        log "OK" "[${sub}] Traditional registration completed in $((fsl_duration / 60)) minutes"
    fi
    
    update_progress "$sub" "preprocessing" 85
    
    # Generate brain mask
    log "INFO" "[${sub}] Generating brain mask"
    dwi2mask dwi_preproc.mif mask.mif -quiet -force || { 
        log "ERROR" "[${sub}] dwi2mask failed"; return 1; 
    }
    
    # Bias field correction
    log "INFO" "[${sub}] Performing bias field correction"
    dwibiascorrect ants dwi_preproc.mif dwi_biascorr.mif -mask mask.mif -quiet -force || { 
        log "ERROR" "[${sub}] dwibiascorrect failed"; return 1; 
    }
    
    update_progress "$sub" "preprocessing" 90
    
    # Compute DTI metrics
    log "INFO" "[${sub}] Computing DTI tensor and metrics"
    dwi2tensor dwi_biascorr.mif tensor.mif -mask mask.mif -quiet -force || { 
        log "ERROR" "[${sub}] dwi2tensor failed"; return 1; 
    }
    
    tensor2metric tensor.mif \
        -fa fa.mif -adc md.mif -ad ad.mif -rd rd.mif -vector ev.mif \
        -quiet -force || { 
        log "ERROR" "[${sub}] tensor2metric failed"; return 1; 
    }
    
    update_progress "$sub" "preprocessing" 95
    
    # Export to NIFTI - ensure output directory exists
    mkdir -p "$outdir"
    
    log "INFO" "[${sub}] Exporting results"
    mrconvert dwi_biascorr.mif "${outdir}/${sub}_dwi_preproc.nii.gz" \
        -export_grad_fsl "${outdir}/${sub}_dwi.bvec" "${outdir}/${sub}_dwi.bval" \
        -quiet -force || {
        log "ERROR" "[${sub}] Failed to export preprocessed DWI"
        return 1
    }
    
    mrconvert mask.mif "${outdir}/${sub}_mask.nii.gz" -datatype uint8 -quiet -force
    
    for metric in fa md ad rd ev; do 
        mrconvert "${metric}.mif" "${outdir}/${sub}_${metric}.nii.gz" -quiet -force
    done
    
    update_progress "$sub" "preprocessing" 98
    
    # Generate basic QC
    generate_basic_qc "$sub"
    
    # ML Registration Quality Report (if ML was used)
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "$registration_successful" = true ]; then
        generate_ml_registration_report "$sub"
    fi
    
    # Move to external storage with verification
    log "INFO" "[${sub}] Moving basic preprocessing results to external storage"
    mkdir -p "${EXTERNAL_MRTRIX}/${sub}"
    
    if retry_operation rsync -av "${outdir}/" "${EXTERNAL_MRTRIX}/${sub}/"; then
        # Verify critical files were transferred
        local critical_files=("${sub}_dwi_preproc.nii.gz" "${sub}_dwi.bvec" "${sub}_dwi.bval" "${sub}_mask.nii.gz")
        local all_transferred=true
        
        for file in "${critical_files[@]}"; do
            if [ ! -f "${EXTERNAL_MRTRIX}/${sub}/${file}" ]; then
                log "ERROR" "[${sub}] Critical file not transferred: $file"
                all_transferred=false
            fi
        done
        
        if [ "$all_transferred" = true ]; then
            rm -rf "${outdir}"
            log "OK" "[${sub}] Basic preprocessing moved to external storage"
            create_checkpoint "$sub" "preprocessing_complete"
            update_progress "$sub" "preprocessing" 100
        else
            log "ERROR" "[${sub}] Transfer verification failed, keeping local copy"
            return 1
        fi
    else
        log "ERROR" "[${sub}] Failed to move to external storage"
        return 1
    fi
    
    # Resource monitoring
    monitor_resources "$sub" "preprocessing_end"
    
    safe_cd_return
    log "OK" "[${sub}] Basic preprocessing completed"
    return 0
}

generate_basic_qc() {
    local sub=$1
    local outdir="${MRTRIX_DIR}/${sub}"
    local qcdir="${outdir}/qc"
    mkdir -p "$qcdir"
    
    log "INFO" "[${sub}] Generating basic QC report"
    
    { 
        echo "DTI Metrics Statistics for ${sub}"
        echo "================================="
        echo "Generated on: $(date)"
        echo ""
        echo "Processing parameters:"
        echo "- Phase encoding: ${PE_DIR}"
        echo "- Echo spacing: ${ECHO_SPACING}"
        echo "- SLM model: ${SLM_MODEL}"
        echo "- ML registration used: ${USE_ML_REGISTRATION:-false}"
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "- ML method: ${ML_REGISTRATION_METHOD:-auto}"
            echo "- ML mode: $([ "${ML_QUICK_MODE:-true}" = true ] && echo "Quick" || echo "Full")"
        fi
        echo ""
        
        # Get number of volumes
        local num_volumes=$(mrstats "${outdir}/${sub}_dwi_preproc.nii.gz" -quiet -output count 2>/dev/null | tail -1 || echo "N/A")
        echo "DWI volumes: $num_volumes"
        echo ""
        
        for metric in fa md ad rd; do 
            if [ -f "${outdir}/${sub}_${metric}.nii.gz" ]; then 
                echo -e "\n${metric^^} statistics:"
                mrstats "${outdir}/${sub}_${metric}.nii.gz" -mask "${outdir}/${sub}_mask.nii.gz" 2>/dev/null || \
                    echo "  Error computing stats"
                    
                # Add histogram information
                local mean_val=$(mrstats "${outdir}/${sub}_${metric}.nii.gz" -mask "${outdir}/${sub}_mask.nii.gz" -output mean 2>/dev/null || echo "N/A")
                local std_val=$(mrstats "${outdir}/${sub}_${metric}.nii.gz" -mask "${outdir}/${sub}_mask.nii.gz" -output std 2>/dev/null || echo "N/A")
                echo "  Summary: mean=$mean_val, std=$std_val"
            fi
        done
        
        echo ""
        echo "Processing completed: $(date)"
        
    } > "${qcdir}/${sub}_stats.txt"
    
    # Generate FA mosaic if slicer available
    if command -v slicer >/dev/null 2>&1 && [ -f "${outdir}/${sub}_fa.nii.gz" ]; then 
        slicer "${outdir}/${sub}_fa.nii.gz" -a "${qcdir}/${sub}_qc_fa_mosaic.png" 2>/dev/null || true
        if [ -f "${qcdir}/${sub}_qc_fa_mosaic.png" ]; then
            log "INFO" "[${sub}] FA mosaic created: ${qcdir}/${sub}_qc_fa_mosaic.png"
        fi
    fi
    
    # Create simple SNR estimate
    if [ -f "${outdir}/${sub}_dwi_preproc.nii.gz" ] && [ -f "${outdir}/${sub}_mask.nii.gz" ]; then
        $PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    dwi = nib.load('${outdir}/${sub}_dwi_preproc.nii.gz').get_fdata()
    mask = nib.load('${outdir}/${sub}_mask.nii.gz').get_fdata()
    
    # Simple SNR estimate using b0 volumes
    b0_vols = dwi[..., :5]  # Assume first 5 volumes are b0
    
    signal = np.mean(b0_vols[mask > 0])
    noise = np.std(b0_vols[mask > 0])
    snr = signal / noise if noise > 0 else 0
    
    with open('${qcdir}/${sub}_snr.txt', 'w') as f:
        f.write(f'Estimated SNR: {snr:.2f}\\n')
        f.write(f'Signal (mean): {signal:.2f}\\n')
        f.write(f'Noise (std): {noise:.2f}\\n')
    
    print(f'SNR estimate: {snr:.2f}')
except Exception as e:
    print(f'SNR calculation failed: {e}')
" 2>/dev/null || true
    fi
}

generate_ml_registration_report() {
    local sub=$1
    local outdir="${MRTRIX_DIR}/${sub}"
    local qcdir="${outdir}/qc"
    local report_file="${qcdir}/${sub}_ml_registration_report.txt"
    
    log "ML" "[${sub}] Generating ML registration report"
    
    {
        echo "ML Registration Report for ${sub}"
        echo "================================"
        echo "Generated on: $(date)"
        echo ""
        echo "ML Configuration:"
        echo "- Method used: ${ML_REGISTRATION_METHOD:-auto}"
        echo "- Mode: $([ "${ML_QUICK_MODE:-true}" = true ] && echo "Quick" || echo "Full")"
        echo "- GPU available: ${GPU_AVAILABLE:-false}"
        echo "- Quality check enabled: ${REGISTRATION_QUALITY_CHECK:-true}"
        echo ""
        
        # Check for ML-specific log files
        local workdir="${WORK_DIR}/${sub}/preproc"
        
        if [ -f "${workdir}/voxelmorph_registration.log" ]; then
            echo "VoxelMorph Registration Log:"
            echo "----------------------------"
            tail -n 20 "${workdir}/voxelmorph_registration.log" 2>/dev/null || echo "Log not available"
            echo ""
        fi
        
        # Performance metrics
        if [ -f "${LOG_DIR}/${sub}_progress.json" ]; then
            echo "Performance Metrics:"
            echo "-------------------"
            $PYTHON_EXECUTABLE -c "
import json, sys
try:
    with open('${LOG_DIR}/${sub}_progress.json', 'r') as f:
        data = json.load(f)
    print(f'Last update: {data.get(\"last_update\", \"N/A\")}')
    print(f'Preprocessing progress: {data.get(\"preprocessing\", \"N/A\")}%')
except:
    print('Progress data not available')
" 2>/dev/null || echo "Progress data not available"
            echo ""
        fi
        
        # Resource usage summary
        if [ -f "${LOG_DIR}/resource_monitor.log" ]; then
            echo "Resource Usage (last entry):"
            echo "----------------------------"
            grep -A 10 "preprocessing" "${LOG_DIR}/resource_monitor.log" | tail -10 2>/dev/null || echo "Resource data not available"
        fi
        
    } > "$report_file"
    
    # Copy to external QC directory
    mkdir -p "${EXTERNAL_QC}"
    cp "$report_file" "${EXTERNAL_QC}/${sub}_ml_registration_report.txt" 2>/dev/null || true
    
    log "OK" "[${sub}] ML registration report generated"
}

# Additional ML-specific helper for Stage 1
select_optimal_ml_method() {
    local sub=$1
    local dwi_file=$2
    
    log "ML" "[${sub}] Selecting optimal ML registration method"
    
    # Analyze DWI characteristics to choose best method
    _DWI_FILE="$dwi_file" $PYTHON_EXECUTABLE - << 'PYEOF'
import os, sys
import nibabel as nib
import numpy as np

try:
    dwi_file = os.environ['_DWI_FILE']
    dwi_img = nib.load(dwi_file)
    dwi_data = dwi_img.get_fdata()
    
    voxel_size = dwi_img.header.get_zooms()[:3]
    image_size = dwi_data.shape[:3]
    num_volumes = dwi_data.shape[3] if len(dwi_data.shape) > 3 else 1
    memory_gb = np.prod(image_size) * num_volumes * 4 / (1024**3)
    
    print(f"DWI characteristics:")
    print(f"  Image size: {image_size}")
    print(f"  Voxel size: {voxel_size}")
    print(f"  Volumes: {num_volumes}")
    print(f"  Estimated memory: {memory_gb:.1f}GB")
    
    if memory_gb > 8:
        print("Recommendation: Use quick mode or traditional registration for large images")
        sys.exit(1)
    elif memory_gb > 4:
        print("Recommendation: VoxelMorph quick mode")
        sys.exit(0)
    else:
        print("Recommendation: VoxelMorph full mode")
        sys.exit(0)
        
except Exception as e:
    print(f"Analysis failed: {e}")
    sys.exit(1)
PYEOF

    local analysis_exit=$?
    return $analysis_exit
}

# Enhanced preprocessing with adaptive ML selection
run_adaptive_ml_preprocessing() {
    local sub=$1
    
    if [ "${USE_ML_REGISTRATION:-false}" != true ] || [ "${ML_REGISTRATION_AVAILABLE:-false}" != true ]; then
        return 1  # Not using ML
    fi
    
    log "ML" "[${sub}] Running adaptive ML preprocessing"
    
    local workdir="${WORK_DIR}/${sub}/preproc"
    local dwi_file="${workdir}/dwi_for_ml.nii.gz"
    
    # Analyze DWI to select optimal method
    if select_optimal_ml_method "$sub" "$dwi_file"; then
        log "ML" "[${sub}] DWI suitable for ML registration"
        
        # Override quick mode based on analysis if auto-selected
        if [ "${ML_REGISTRATION_METHOD:-auto}" = "auto" ]; then
            local recommended_mode=true
            # Could parse the analysis output to set recommended_mode
        fi
        
        return 0  # Proceed with ML
    else
        log "ML" "[${sub}] DWI not optimal for ML registration, using traditional"
        return 1  # Use traditional
    fi
}

# End of Section 4

# --- Stage 2: Post-hoc Refinement (CORRECTED B0 FILENAME HANDLING) ---

run_posthoc_refinement() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/posthoc"
    local outdir="${POSTHOC_DIR}/${sub}"
    
    log "STAGE" "[${sub}] Starting post-hoc refinement (Stage 2)"
    monitor_resources "$sub" "posthoc_start"
    update_progress "$sub" "posthoc" 0
    
    # Check disk space before processing
    if ! check_disk_space "$(dirname "$BIDS_DIR")" 30; then
        log "ERROR" "[${sub}] Insufficient disk space for post-hoc refinement"
        return 1
    fi
    
    # Check if already done in external storage
    if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
        log "INFO" "[${sub}] Post-hoc refinement already completed"
        return 0
    fi
    
    # Copy data from external storage back for processing
    local input_dir="${EXTERNAL_MRTRIX}/${sub}"
    if [ ! -d "$input_dir" ]; then
        log "ERROR" "[${sub}] Basic preprocessing data not found in external storage"
        return 1
    fi
    
    # Create all necessary directories
    mkdir -p "$workdir" "$outdir" "${outdir}/qc"
    safe_cd "$workdir" || return 1
    
    # Copy necessary files
    log "INFO" "[${sub}] Retrieving data for refinement"
    for file in "${sub}_dwi_preproc.nii.gz" "${sub}_dwi.bvec" "${sub}_dwi.bval" "${sub}_mask.nii.gz"; do
        if [ ! -f "${input_dir}/${file}" ]; then
            log "ERROR" "[${sub}] Missing required file: ${file}"
            return 1
        fi
    done
    
    cp "${input_dir}/${sub}_dwi_preproc.nii.gz" dwi_input.nii.gz
    cp "${input_dir}/${sub}_dwi.bvec" dwi.bvec
    cp "${input_dir}/${sub}_dwi.bval" dwi.bval
    cp "${input_dir}/${sub}_mask.nii.gz" mask_basic.nii.gz
    
    update_progress "$sub" "posthoc" 10
    
    # Convert to MIF for processing
    mrconvert dwi_input.nii.gz dwi_input.mif \
        -fslgrad dwi.bvec dwi.bval \
        -datatype float32 -quiet -force || {
        log "ERROR" "[${sub}] Failed to convert DWI to MIF"
        return 1
    }
    
    mrconvert mask_basic.nii.gz mask.mif -datatype bit -quiet -force
    
    update_progress "$sub" "posthoc" 20
    
    # Check if b0_mean.nii.gz exists from preprocessing (accept alternate names)
    if [ -f "${input_dir}/b0_mean.nii.gz" ]; then
        cp "${input_dir}/b0_mean.nii.gz" b0_mean.nii.gz
    elif [ -f "${input_dir}/mean_bzero.nii.gz" ]; then
        cp "${input_dir}/mean_bzero.nii.gz" b0_mean.nii.gz
    elif [ -f "${input_dir}/b0_mean.nii" ]; then
        if command -v mrconvert &>/dev/null; then
            mrconvert "${input_dir}/b0_mean.nii" b0_mean.nii.gz -quiet -force
        else
            cp "${input_dir}/b0_mean.nii" b0_mean.nii.gz 2>/dev/null || true
        fi
    elif [ -f "${input_dir}/mean_bzero.nii" ]; then
        if command -v mrconvert &>/dev/null; then
            mrconvert "${input_dir}/mean_bzero.nii" b0_mean.nii.gz -quiet -force
        else
            cp "${input_dir}/mean_bzero.nii" b0_mean.nii.gz 2>/dev/null || true
        fi
    else
        log "WARN" "[${sub}] No b0 mean found in external preproc outputs (looked for b0_mean.* and mean_bzero.*)"
    fi

    # If still missing, extract mean b0 from the copied DWI input
    if [ ! -f "b0_mean.nii.gz" ]; then
        log "INFO" "[${sub}] Extracting b0_mean for bias correction"
        dwiextract dwi_input.mif - -bzero -quiet | \
            mrmath - mean b0_mean.mif -axis 3 -quiet -force

        # Convert to NIfTI for compatibility with registration tools
        if [ -f "b0_mean.mif" ]; then
            mrconvert b0_mean.mif b0_mean.nii.gz -quiet -force
            log "OK" "[${sub}] b0_mean extracted and converted to NIfTI"
        else
            log "ERROR" "[${sub}] Failed to create b0_mean.mif during extraction"
        fi
    fi

    # ✅ CORRECTED ALIAS BLOCK - Place BEFORE bias correction ✅
    # --- Ensure canonical b0 filenames exist for downstream tools ---
    # FIXED: Handle mean_bzero.nii.gz (NOT .mif) which dwibiascorrect actually creates

    # 1. Copy mean_bzero.nii.gz → b0_mean.nii.gz if needed
    if [ -f "mean_bzero.nii.gz" ] && [ ! -f "b0_mean.nii.gz" ]; then
        cp -f mean_bzero.nii.gz b0_mean.nii.gz 2>/dev/null || true
        log "INFO" "[${sub}] Created b0_mean.nii.gz alias from dwibiascorrect output"
    fi

    # 2. If we have b0_mean.nii.gz but need .mif for MRtrix tools
    if [ -f "b0_mean.nii.gz" ] && [ ! -f "b0_mean.mif" ]; then
        if command -v mrconvert &>/dev/null; then
            mrconvert b0_mean.nii.gz b0_mean.mif -quiet -force || true
        fi
    fi

    # 3. Create mean_bzero aliases for any tools that expect them (for future compatibility)
    if [ -f "b0_mean.nii.gz" ] && [ ! -f "mean_bzero.nii.gz" ]; then
        cp -f b0_mean.nii.gz mean_bzero.nii.gz 2>/dev/null || true
    fi

    if [ -f "b0_mean.mif" ] && [ ! -f "mean_bzero.mif" ]; then
        cp -f b0_mean.mif mean_bzero.mif 2>/dev/null || true
    fi
    # ✅ END CORRECTED ALIAS BLOCK ✅
    
    update_progress "$sub" "posthoc" 30
    
    # Advanced bias field correction
    log "INFO" "[${sub}] Applying refined bias field correction"   
    
    # Enhanced bias correction with multiple iterations
    local bias_success=false
    
    # Try N4 bias correction with optimized parameters
    log "INFO" "[${sub}] Attempting advanced N4 bias correction"
    if dwibiascorrect ants dwi_input.mif dwi_biascorr_refined.mif \
        -mask mask.mif \
        -bias biasfield.mif \
        -ants.b "[200]" \
        -ants.c "[50x50x30,1e-6]" \
        -ants.s "4" \
        -force; then
        bias_success=true
        log "OK" "[${sub}] Advanced bias correction successful"
    else
        log "WARN" "[${sub}] Advanced bias correction failed, trying standard approach"
        if dwibiascorrect ants dwi_input.mif dwi_biascorr_refined.mif \
            -mask mask.mif \
            -bias biasfield.mif \
            -quiet -force 2>/dev/null; then
            bias_success=true
            log "OK" "[${sub}] Standard bias correction successful"
        else
            log "WARN" "[${sub}] All bias correction methods failed, using original data"
            cp dwi_input.mif dwi_biascorr_refined.mif
        fi
    fi
    
    # ✅ CORRECTED - Handle dwibiascorrect output (creates mean_bzero.nii.gz) ✅
    # After bias correction, dwibiascorrect may create mean_bzero.nii.gz
    # Ensure b0_mean.nii.gz exists for downstream tools
    if [ -f "mean_bzero.nii.gz" ] && [ ! -f "b0_mean.nii.gz" ]; then
        cp -f mean_bzero.nii.gz b0_mean.nii.gz 2>/dev/null || true
        log "INFO" "[${sub}] Preserved b0_mean.nii.gz from dwibiascorrect output"
    fi
    # ✅ END POST-CORRECTION HANDLING ✅
    
    update_progress "$sub" "posthoc" 40
    
    # Intensity normalization
    log "INFO" "[${sub}] Applying intensity normalization"
    if dwinormalise individual dwi_biascorr_refined.mif mask.mif \
        dwi_normalized.mif -quiet -force 2>/dev/null; then
        log "OK" "[${sub}] Intensity normalization successful"
    else
        log "WARN" "[${sub}] Normalization failed, using bias-corrected data"
        cp dwi_biascorr_refined.mif dwi_normalized.mif
    fi
    
    update_progress "$sub" "posthoc" 60
    
    # Enhanced brain mask creation
    if ! enhance_brain_mask "$sub"; then
        log "WARN" "[${sub}] Enhanced mask creation failed, using basic mask"
        cp mask.mif mask_enhanced.mif
    fi
    
    update_progress "$sub" "posthoc" 70
    
    # ML-Enhanced Registration Refinement
    local ml_refinement_applied=false
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        log "ML" "[${sub}] Applying ML-based registration refinement"
        
        if apply_ml_registration_refinement "$sub"; then
            ml_refinement_applied=true
            log "OK" "[${sub}] ML registration refinement applied"
        else
            log "WARN" "[${sub}] ML registration refinement failed, continuing with standard processing"
        fi
    fi
    
    update_progress "$sub" "posthoc" 80
    
    # Export refined data
    log "INFO" "[${sub}] Exporting refined data"
    local export_source="dwi_normalized.mif"
    
    # Use ML-refined data if available
    if [ "$ml_refinement_applied" = true ] && [ -f "dwi_ml_refined.mif" ]; then
        export_source="dwi_ml_refined.mif"
        log "INFO" "[${sub}] Using ML-refined data for export"
    fi
    
    mrconvert "$export_source" "${outdir}/${sub}_dwi_refined.nii.gz" \
        -export_grad_fsl "${outdir}/${sub}_dwi.bvec" "${outdir}/${sub}_dwi.bval" \
        -datatype float32 -quiet -force
    
    if [ -f "mask_enhanced.mif" ]; then
        mrconvert mask_enhanced.mif "${outdir}/${sub}_mask_enhanced.nii.gz" \
            -datatype uint8 -quiet -force
    else
        cp mask_basic.nii.gz "${outdir}/${sub}_mask_enhanced.nii.gz"
    fi
    
    if [ -f "biasfield.mif" ]; then
        mrconvert biasfield.mif "${outdir}/${sub}_biasfield.nii.gz" -quiet -force
    fi
    
    update_progress "$sub" "posthoc" 85
    
    # Residual distortion check if T1w available
    if [ -f "${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz" ]; then
        check_residual_distortions "$sub"
    fi
    
    update_progress "$sub" "posthoc" 90
    
    # Generate refined QC
    generate_refined_qc "$sub" "$ml_refinement_applied"
    
    update_progress "$sub" "posthoc" 95
    
    # Move to external storage
    log "INFO" "[${sub}] Moving refined data to external storage"
    mkdir -p "${EXTERNAL_POSTHOC}/${sub}"
    if retry_operation rsync -av "${outdir}/" "${EXTERNAL_POSTHOC}/${sub}/"; then
        rm -rf "${outdir}"
        log "OK" "[${sub}] Refined data moved to external storage"
        create_checkpoint "$sub" "posthoc_complete"
        update_progress "$sub" "posthoc" 100
    else
        log "ERROR" "[${sub}] Failed to move refined data"
        return 1
    fi
    
    safe_cd_return
    cleanup_work_dir "$sub"
    monitor_resources "$sub" "posthoc_end"
    log "OK" "[${sub}] Post-hoc refinement completed"
    return 0
}

# End of Stage 2: Post-hoc Refinement with CORRECTED b0 filename handling

enhance_brain_mask() {
    local sub=$1
    
    log "INFO" "[${sub}] Creating enhanced brain mask"
    
    # Multi-approach mask creation with ML enhancement if available
    local mask_methods=()
    local mask_files=()
    
    # Method 1: DWI-based mask
    if dwi2mask dwi_normalized.mif mask_dwi.mif -quiet -force 2>/dev/null; then
        mask_methods+=("DWI-based")
        mask_files+=("mask_dwi.mif")
        log "INFO" "[${sub}] DWI-based mask created"
    else
        log "WARN" "[${sub}] DWI-based mask creation failed"
    fi
    
    # Method 2: FA-based mask
    if dwi2tensor dwi_normalized.mif tensor.mif -mask mask.mif -quiet -force 2>/dev/null; then
        tensor2metric tensor.mif -fa fa.mif -quiet -force
        
        if mrthreshold fa.mif - -abs 0.1 -quiet 2>/dev/null | \
           maskfilter - dilate mask_fa_temp.mif -npass 2 -quiet -force 2>/dev/null; then
            # Clean up FA mask
            maskfilter mask_fa_temp.mif erode mask_fa.mif -npass 1 -quiet -force 2>/dev/null
            mask_methods+=("FA-based")
            mask_files+=("mask_fa.mif")
            log "INFO" "[${sub}] FA-based mask created"
        else
            log "WARN" "[${sub}] FA-based mask creation failed"
        fi
    else
        log "WARN" "[${sub}] Tensor fitting for mask failed"
    fi
    
    # Method 3: BET-based mask from b0
    # Ensure a b0_mean NIfTI exists (accept several canonical names)
    if [ -f "b0_mean.nii.gz" ]; then
        : # already available
    elif [ -f "b0_mean.mif" ]; then
        mrconvert b0_mean.mif b0_mean.nii.gz -quiet -force
    elif [ -f "mean_bzero.nii.gz" ]; then
        cp -f mean_bzero.nii.gz b0_mean.nii.gz
    elif [ -f "mean_bzero.mif" ]; then
        mrconvert mean_bzero.mif b0_mean.nii.gz -quiet -force
    else
        # fallback: try to extract b0 mean from copied DWI input if present
        if [ -f "dwi_input.mif" ]; then
            dwiextract dwi_input.mif - -bzero -quiet | \
                mrmath - mean b0_mean_temp.mif -axis 3 -quiet -force && \
            mrconvert b0_mean_temp.mif b0_mean.nii.gz -quiet -force || true
            rm -f b0_mean_temp.mif 2>/dev/null || true
        fi
    fi

    if bet2 b0_mean.nii.gz b0_brain -m -f 0.3 2>/dev/null; then
        if [ -f "b0_brain_mask.nii.gz" ]; then
            mrconvert b0_brain_mask.nii.gz mask_bet.mif -datatype bit -quiet -force
            mask_methods+=("BET-based")
            mask_files+=("mask_bet.mif")
            log "INFO" "[${sub}] BET-based mask created"
        fi
    else
        log "WARN" "[${sub}] BET mask creation failed"
    fi
    
    # ML-Enhanced Mask Creation
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        if create_ml_enhanced_mask "$sub"; then
            mask_methods+=("ML-enhanced")
            mask_files+=("mask_ml.mif")
            log "OK" "[${sub}] ML-enhanced mask created"
        else
            log "WARN" "[${sub}] ML-enhanced mask creation failed"
        fi
    fi
    
    # Combine masks intelligently
    local num_masks=${#mask_files[@]}
    
    if [ $num_masks -eq 0 ]; then
        log "ERROR" "[${sub}] No masks were successfully created"
        return 1
    elif [ $num_masks -eq 1 ]; then
        log "INFO" "[${sub}] Using single mask: ${mask_methods[0]}"
        cp "${mask_files[0]}" mask_enhanced.mif
    else
        log "INFO" "[${sub}] Combining ${num_masks} masks: ${mask_methods[*]}"
        
        # Create consensus mask using majority voting (sum masks, threshold at >50%)
        local combine_cmd="mrcalc ${mask_files[0]}"
        for ((i=1; i<num_masks; i++)); do
            combine_cmd+=" ${mask_files[i]} -add"
        done
        local threshold=$(( (num_masks + 1) / 2 ))  # ceiling of N/2
        combine_cmd+=" ${threshold} -ge mask_consensus.mif -quiet -force"
        
        if eval "$combine_cmd" 2>/dev/null; then
            # Apply morphological operations to clean up
            if maskfilter mask_consensus.mif dilate - -npass 1 -quiet 2>/dev/null | \
               maskfilter - erode - -npass 1 -quiet 2>/dev/null | \
               maskfilter - dilate mask_enhanced.mif -npass 1 -quiet -force 2>/dev/null; then
                log "OK" "[${sub}] Enhanced mask created from ${num_masks} methods"
            else
                log "WARN" "[${sub}] Mask combination failed, using first available mask"
                cp "${mask_files[0]}" mask_enhanced.mif
            fi
        else
            log "WARN" "[${sub}] Mask consensus failed, using first available mask"
            cp "${mask_files[0]}" mask_enhanced.mif
        fi
    fi
    
    # Validate final mask
    if [ -f "mask_enhanced.mif" ]; then
        local mask_volume=$(safe_int "$(mrstats mask_enhanced.mif -output count 2>/dev/null || echo '0')")
        if [ "$mask_volume" -gt 1000 ]; then
            log "OK" "[${sub}] Enhanced mask validation passed (${mask_volume} voxels)"
        else
            log "WARN" "[${sub}] Enhanced mask seems too small (${mask_volume} voxels)"
        fi
    fi
    
    # Clean up temporary files
    # NOTE: b0_mean.nii.gz is preserved - it's needed for registration quality analysis later
    rm -f tensor.mif fa.mif mask_dwi.mif mask_fa.mif mask_fa_temp.mif mask_bet.mif mask_consensus.mif
    rm -f b0_brain*.nii.gz mask_ml.mif 2>/dev/null || true
    
    return 0
}

create_ml_enhanced_mask() {
    local sub=$1
    
    log "ML" "[${sub}] Creating ML-enhanced brain mask"

        # Create b0_for_ml.nii.gz from any available b0 mean filename
    if [ -f "b0_for_ml.nii.gz" ]; then
        : # already present
    elif [ -f "b0_mean.nii.gz" ]; then
        cp -f b0_mean.nii.gz b0_for_ml.nii.gz
    elif [ -f "b0_mean.mif" ]; then
        mrconvert b0_mean.mif b0_for_ml.nii.gz -quiet -force
    elif [ -f "mean_bzero.nii.gz" ]; then
        cp -f mean_bzero.nii.gz b0_for_ml.nii.gz
    elif [ -f "mean_bzero.mif" ]; then
        mrconvert mean_bzero.mif b0_for_ml.nii.gz -quiet -force
    else
        # fallback: extract from DWI if present
        if [ -f "dwi_input.mif" ]; then
            dwiextract dwi_input.mif - -bzero -quiet | \
                mrmath - mean b0_for_ml_temp.mif -axis 3 -quiet -force && \
                mrconvert b0_for_ml_temp.mif b0_for_ml.nii.gz -quiet -force || true
            rm -f b0_for_ml_temp.mif 2>/dev/null || true
        fi
    fi
    
    # Export data for ML processing (only if not already created by fallback logic above)
    if [ ! -f "b0_for_ml.nii.gz" ] && [ -f "b0_mean.mif" ]; then
        mrconvert b0_mean.mif b0_for_ml.nii.gz -quiet -force
    fi
    
    # Create ML mask using Python
    $PYTHON_EXECUTABLE << 'EOF'
import sys
import numpy as np
import nibabel as nib
from scipy import ndimage
from sklearn.cluster import KMeans

def create_ml_mask(b0_file, output_file):
    try:
        # Load b0 image
        b0_img = nib.load(b0_file)
        b0_data = b0_img.get_fdata()
        
        if b0_data.max() == 0:
            print("Empty b0 image")
            return False
        
                # Normalize intensity
        b0_norm = (b0_data - b0_data.min()) / (b0_data.max() - b0_data.min())
        
        # Initial threshold-based mask
        otsu_thresh = np.percentile(b0_norm[b0_norm > 0], 75)
        initial_mask = b0_norm > otsu_thresh
        
        # Remove small components
        labeled, num_labels = ndimage.label(initial_mask)
        if num_labels > 1:
            # Keep largest component
            sizes = ndimage.sum(initial_mask, labeled, range(num_labels + 1))
            max_label = np.argmax(sizes[1:]) + 1
            initial_mask = labeled == max_label
        
        # Morphological operations
        # Fill holes
        filled_mask = ndimage.binary_fill_holes(initial_mask)
        
        # Smooth with opening and closing
        from scipy.ndimage import binary_opening, binary_closing
        struct_elem = ndimage.generate_binary_structure(3, 1)
        
        smooth_mask = binary_opening(filled_mask, structure=struct_elem, iterations=2)
        smooth_mask = binary_closing(smooth_mask, structure=struct_elem, iterations=3)
        
        # Final cleanup - remove small objects
        labeled_final, num_final = ndimage.label(smooth_mask)
        if num_final > 1:
            sizes_final = ndimage.sum(smooth_mask, labeled_final, range(num_final + 1))
            max_label_final = np.argmax(sizes_final[1:]) + 1
            final_mask = labeled_final == max_label_final
        else:
            final_mask = smooth_mask
        
        # Save result
        mask_img = nib.Nifti1Image(
            final_mask.astype(np.uint8),
            b0_img.affine,
            b0_img.header
        )
        nib.save(mask_img, output_file)
        
        # Validation
        mask_volume = np.sum(final_mask)
        total_volume = np.prod(final_mask.shape)
        mask_ratio = mask_volume / total_volume
        
        print(f"ML mask created: {mask_volume} voxels ({mask_ratio:.1f} of image)")
        
        if 0.05 < mask_ratio < 0.8:  # Reasonable brain mask should be 5-80% of image
            return True
        else:
            print(f"ML mask failed validation: ratio {mask_ratio:.1f} outside expected range")
            return False
            
    except Exception as e:
        print(f"ML mask creation failed: {e}")
        return False

# Main execution
success = create_ml_mask('b0_for_ml.nii.gz', 'mask_ml_temp.nii.gz')
sys.exit(0 if success else 1)
EOF
    
    local ml_mask_exit=$?
    
    if [ $ml_mask_exit -eq 0 ] && [ -f "mask_ml_temp.nii.gz" ]; then
        mrconvert mask_ml_temp.nii.gz mask_ml.mif -datatype bit -quiet -force
        rm -f mask_ml_temp.nii.gz b0_for_ml.nii.gz
        log "OK" "[${sub}] ML-enhanced mask created successfully"
        return 0
    else
        log "WARN" "[${sub}] ML-enhanced mask creation failed"
        rm -f mask_ml_temp.nii.gz b0_for_ml.nii.gz
        return 1
    fi
}

apply_ml_registration_refinement() {
    local sub=$1
    
    log "ML" "[${sub}] Applying ML registration refinement"
    
    # This function applies additional ML-based registration refinement
    # for inter-volume registration within the DWI dataset
    
    local ml_method="${ML_REGISTRATION_METHOD:-auto}"
    local refinement_applied=false
    
    # Export current data for ML processing
    mrconvert dwi_normalized.mif dwi_for_refinement.nii.gz \
        -export_grad_fsl dwi_refine.bvec dwi_refine.bval \
        -quiet -force
    
    case $ml_method in
        "voxelmorph"|"auto")
            log "ML" "[${sub}] Applying VoxelMorph refinement"
            if run_voxelmorph_dwi_registration "$sub" \
                "dwi_for_refinement.nii.gz" \
                "dwi_voxelmorph_refined.nii.gz" \
                "${ML_QUICK_MODE:-true}"; then
                
                # Convert back to MIF
                mrconvert dwi_voxelmorph_refined.nii.gz dwi_ml_refined.mif \
                    -fslgrad dwi_refine.bvec dwi_refine.bval \
                    -quiet -force
                    
                refinement_applied=true
                log "OK" "[${sub}] VoxelMorph refinement successful"
                
                # Quality assessment
                if [ "${REGISTRATION_QUALITY_CHECK:-true}" = true ]; then
                    if validate_ml_registration_quality "$sub" \
                        "dwi_for_refinement.nii.gz" \
                        "dwi_voxelmorph_refined.nii.gz" \
                        "VoxelMorph-Refinement"; then
                        log "OK" "[${sub}] VoxelMorph refinement quality check passed"
                    else
                        log "WARN" "[${sub}] VoxelMorph refinement quality check failed"
                        refinement_applied=false
                    fi
                fi
            else
                log "WARN" "[${sub}] VoxelMorph refinement failed"
            fi
            ;;
            
        "ants")
            log "ML" "[${sub}] Applying enhanced ANTs refinement"
            # Could implement ANTs-based refinement here
            log "INFO" "[${sub}] ANTs refinement not yet implemented for post-hoc stage"
            ;;
            
        "synthmorph")
            log "ML" "[${sub}] SynthMorph refinement planned for T1w registration"
            # SynthMorph refinement will be applied in distortion checking
            ;;
    esac
    
    # Cleanup temporary files
    rm -f dwi_for_refinement.nii.gz dwi_refine.bvec dwi_refine.bval
    rm -f dwi_voxelmorph_refined.nii.gz
    
    return $([ "$refinement_applied" = true ] && echo 0 || echo 1)
}

# Additional helper function for ML integration
validate_refinement_quality() {
    local sub=$1
    local original_dwi=$2
    local refined_dwi=$3
    
    log "ML" "[${sub}] Validating refinement quality"
    
    $PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
from scipy import stats

try:
    orig_img = nib.load('$original_dwi')
    refined_img = nib.load('$refined_dwi')
    
    orig_data = orig_img.get_fdata()
    refined_data = refined_img.get_fdata()
    
    if orig_data.shape != refined_data.shape:
        print('Shape mismatch between original and refined data')
        exit(1)
    
    # Calculate improvement metrics
    # 1. SNR improvement
    orig_mean = np.mean(orig_data[orig_data > 0])
    orig_std = np.std(orig_data[orig_data > 0])
    orig_snr = orig_mean / orig_std if orig_std > 0 else 0
    
    refined_mean = np.mean(refined_data[refined_data > 0])
    refined_std = np.std(refined_data[refined_data > 0])
    refined_snr = refined_mean / refined_std if refined_std > 0 else 0
    
    snr_improvement = (refined_snr - orig_snr) / orig_snr * 100 if orig_snr > 0 else 0
    
    # 2. Intensity consistency
    consistency = np.corrcoef(orig_data.flatten(), refined_data.flatten())[0,1]
    
    print(f'SNR improvement: {snr_improvement:.1f}%')
    print(f'Data consistency: {consistency:.3f}')
    
    # Quality thresholds
    quality_good = snr_improvement > 5 and consistency > 0.95
    quality_acceptable = snr_improvement > 0 and consistency > 0.90
    
    if quality_good:
        print('Refinement quality: GOOD')
        exit(0)
    elif quality_acceptable:
        print('Refinement quality: ACCEPTABLE')
        exit(0)
    else:
        print('Refinement quality: POOR')
        exit(1)
        
except Exception as e:
    print(f'Quality validation failed: {e}')
    exit(1)
" 2>/dev/null

    return $?
}

# End of Section 5

# --- T1w-DWI Registration wrappers for check_residual_distortions ---
# These wrap the generic ML registration functions with T1w-DWI-specific logic.
# Both expect to be called from a workdir containing T1_n4.nii.gz, T1_brain.nii.gz,
# and b0_mean.nii.gz.  They must produce T1_in_b0_space.nii.gz on success.

run_synthmorph_t1_dwi_registration() {
    local sub=$1

    if [ "${SYNTHMORPH_AVAILABLE:-false}" != true ]; then
        log "WARN" "[${sub}] SynthMorph not available for T1w-DWI registration"
        return 1
    fi

    log "ML" "[${sub}] Running SynthMorph T1w-DWI registration"

    local fixed="b0_mean.nii.gz"
    local moving="T1_brain.nii.gz"
    [ ! -f "$moving" ] && moving="T1_n4.nii.gz"

    if [ ! -f "$fixed" ] || [ ! -f "$moving" ]; then
        log "ERROR" "[${sub}] Required images missing for SynthMorph (need b0_mean.nii.gz + T1)"
        return 1
    fi

    if run_synthmorph_registration "$sub" "$fixed" "$moving" "synthmorph_t1_dwi"; then
        # SynthMorph outputs synthmorph_t1_dwi_moved.mgz — convert to NIfTI
        if [ -f "synthmorph_t1_dwi_moved.mgz" ]; then
            mri_convert "synthmorph_t1_dwi_moved.mgz" "T1_in_b0_space.nii.gz" 2>/dev/null
        elif [ -f "synthmorph_t1_dwi_moved.nii.gz" ]; then
            cp -f "synthmorph_t1_dwi_moved.nii.gz" "T1_in_b0_space.nii.gz"
        fi

        if [ -f "T1_in_b0_space.nii.gz" ]; then
            log "OK" "[${sub}] SynthMorph T1w-DWI registration produced T1_in_b0_space.nii.gz"
            return 0
        fi
    fi

    log "WARN" "[${sub}] SynthMorph T1w-DWI registration did not produce output"
    return 1
}

run_enhanced_ants_t1_dwi_registration() {
    local sub=$1

    log "ML" "[${sub}] Running enhanced ANTs T1w-DWI registration"

    local fixed="b0_mean.nii.gz"
    local moving="T1_brain.nii.gz"
    [ ! -f "$moving" ] && moving="T1_n4.nii.gz"

    if [ ! -f "$fixed" ] || [ ! -f "$moving" ]; then
        log "ERROR" "[${sub}] Required images missing for enhanced ANTs (need b0_mean.nii.gz + T1)"
        return 1
    fi

    if run_ants_with_ml_features "$sub" "$fixed" "$moving" "ants_t1_dwi" "rigid+affine"; then
        if [ -f "ants_t1_dwi_Warped.nii.gz" ]; then
            cp -f "ants_t1_dwi_Warped.nii.gz" "T1_in_b0_space.nii.gz"
            log "OK" "[${sub}] Enhanced ANTs T1w-DWI registration produced T1_in_b0_space.nii.gz"
            return 0
        fi
    fi

    log "WARN" "[${sub}] Enhanced ANTs T1w-DWI registration did not produce output"
    return 1
}

# Replace the older failing check_residual_distortions with a robust, non-fatal implementation
check_residual_distortions() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/posthoc"
    local outdir="${POSTHOC_DIR}/${sub}"
    local n4_timeout=${N4_TIMEOUT:-300}   # seconds
    local n4_threads=${N4_THREADS:-1}     # conservative default

    log "INFO" "[${sub}] Checking for residual distortions using T1w"

    mkdir -p "$workdir" "$outdir"

    # Change to workdir for all subsequent file operations
    safe_cd "$workdir" || {
        log "ERROR" "[${sub}] Cannot change to workdir: $workdir"
        return 1
    }

    # --- N4 Bias Field Correction ---
    local t1_input="${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz"
    local b0_candidates=( "b0_mean.nii.gz" "b0_mean.nii" "mean_bzero.nii" )
    local n4_input=""

    if [ -f "$t1_input" ]; then
        n4_input="$t1_input"
    else
        for cand in "${b0_candidates[@]}"; do
            if [ -f "$cand" ]; then
                n4_input="$cand"
                break
            fi
        done
    fi

    if [ -z "$n4_input" ]; then
        log "WARN" "[${sub}] No suitable image found for N4BiasFieldCorrection (tried T1 and b0 candidates). Skipping N4."
        return 0
    fi

    if ! command -v N4BiasFieldCorrection &>/dev/null; then
        log "WARN" "[${sub}] N4BiasFieldCorrection not found in PATH; skipping N4"
        cp -f "$n4_input" "T1_n4.nii.gz" 2>/dev/null || true
    else
        local out_basename="T1_n4.nii.gz"
        local n4_log="n4_t1.log"
        local tmp_in="n4_input_temp.nii.gz"
        cp -f "$n4_input" "$tmp_in"

        log "INFO" "[${sub}] Running N4BiasFieldCorrection on: $(basename "$n4_input") (timeout ${n4_timeout}s)"
        local n4_rc=0
        (
            export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS="$n4_threads"
            export OMP_NUM_THREADS="$n4_threads"
            ulimit -c 0
            if command -v timeout &>/dev/null; then
                timeout "$n4_timeout" N4BiasFieldCorrection -d 3 -i "$tmp_in" -o "$out_basename" -c "[50x50x30,1e-6]" -s 4 -b "[200]"
            else
                N4BiasFieldCorrection -d 3 -i "$tmp_in" -o "$out_basename" -c "[50x50x30,1e-6]" -s 4 -b "[200]"
            fi
        ) &> "$n4_log" || n4_rc=$?

        if ! ([ $n4_rc -eq 0 ] && [ -f "$out_basename" ]); then
            log "WARN" "[${sub}] N4 failed (rc=$n4_rc). See ${n4_log}. Using original image."
            cp -f "$tmp_in" "$out_basename"
        else
            log "OK" "[${sub}] N4 bias correction succeeded."
        fi
        rm -f "$tmp_in"
    fi

    # --- Brain Extraction (BET) ---
    local bet_successful=false
    for f_val in 0.5 0.3 0.7; do
        if bet2 "T1_n4.nii.gz" "T1_brain" -m -f "$f_val" -g 0 &>/dev/null; then
            bet_successful=true
            log "INFO" "[${sub}] BET successful with f=${f_val}"
            break
        fi
    done

    if [ "$bet_successful" = false ]; then
        log "WARN" "[${sub}] T1 brain extraction failed with all parameter sets. Registration will likely fail."
        # Do not hard-fail; continue so other steps can proceed
    fi

    # --- T1w-DWI Registration ---
    local registration_successful=false
    local registration_method=""

    # Try ML-Enhanced approaches first (SynthMorph then Enhanced-ANTs), if available
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        if [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
            log "ML" "[${sub}] Attempting SynthMorph T1w-DWI registration (preferred ML method)"
            if run_synthmorph_t1_dwi_registration "$sub"; then
                registration_successful=true
                registration_method="SynthMorph"
                log "OK" "[${sub}] SynthMorph T1w-DWI registration succeeded"
            else
                log "WARN" "[${sub}] SynthMorph registration attempt failed - falling back"
            fi
        fi

        if [ "$registration_successful" = false ]; then
            log "ML" "[${sub}] Attempting enhanced ANTs T1w-DWI registration (ML fallback)"
            if run_enhanced_ants_t1_dwi_registration "$sub"; then
                registration_successful=true
                registration_method="Enhanced-ANTs"
                log "OK" "[${sub}] Enhanced ANTs T1w-DWI registration succeeded"
            else
                log "WARN" "[${sub}] Enhanced ANTs registration attempt failed - falling back to traditional methods"
            fi
        fi
    fi

    # Traditional BBR registration (fallback or primary method)
    if [ "$registration_successful" = false ]; then
        log "INFO" "[${sub}] Using traditional BBR registration as fallback"
        registration_method="BBR"
        if run_traditional_bbr_registration "$sub"; then
            registration_successful=true
            log "OK" "[${sub}] Traditional BBR registration successful"
        else
            log "WARN" "[${sub}] Traditional BBR registration failed"
        fi
    fi

    # --- Analyze Final Results ---
    if [ "$registration_successful" = true ] && [ -f "T1_in_b0_space.nii.gz" ]; then
        analyze_registration_results "$sub" "$registration_method"
        log "OK" "[${sub}] Post-hoc distortion check and registration completed."
        return 0 # Final success
    else
        # Do not abort the entire pipeline for registration failure.
        # Create a QC entry and continue; downstream steps will detect missing registration and handle accordingly.
        log "WARN" "[${sub}] All registration methods failed or produced no usable output. Continuing pipeline without T1w-DWI registration."
        # Save a small indicator file for QC
        echo "registration_failed" > "${outdir}/${sub}_registration_status.txt" 2>/dev/null || true
        return 0
    fi
}

analyze_registration_results() {
    local sub=$1
    local registration_method=$2
    local outdir="${POSTHOC_DIR}/${sub}"
    
    log "INFO" "[${sub}] Analyzing registration results using $registration_method"
    
    # Ensure b0_mean.nii.gz is available - try to restore from output dir if needed
    if [ ! -f "b0_mean.nii.gz" ] && [ -f "${outdir}/${sub}_b0_mean.nii.gz" ]; then
        cp "${outdir}/${sub}_b0_mean.nii.gz" b0_mean.nii.gz
        log "INFO" "[${sub}] Restored b0_mean.nii.gz from output directory"
    elif [ ! -f "b0_mean.nii.gz" ] && [ -f "${outdir}/b0_mean.nii.gz" ]; then
        cp "${outdir}/b0_mean.nii.gz" b0_mean.nii.gz
        log "INFO" "[${sub}] Restored b0_mean.nii.gz from output directory"
    fi
    
    # Verify required files exist
    if [ ! -f "b0_mean.nii.gz" ]; then
        log "WARN" "[${sub}] b0_mean.nii.gz not found even after attempting recovery, skipping registration analysis"
        return 0
    fi
    
    if [ ! -f "T1_in_b0_space.nii.gz" ]; then
        log "WARN" "[${sub}] T1_in_b0_space.nii.gz not found, skipping registration analysis"
        return 0
    fi
    
    # Calculate residual distortions
    fslmaths b0_mean.nii.gz -sub T1_in_b0_space.nii.gz \
             "${outdir}/${sub}_residual_distortion.nii.gz"
    
    # Comprehensive statistical analysis
    {
        echo "T1w-DWI Registration Analysis for ${sub}"
        echo "========================================"
        echo "Method used: $registration_method"
        echo "Generated on: $(date)"
        echo ""
        
        # Basic statistics
        echo "RESIDUAL DISTORTION STATISTICS:"
        echo "==============================="
        local range_stats=$(fslstats "${outdir}/${sub}_residual_distortion.nii.gz" -R)
        local mean_std=$(fslstats "${outdir}/${sub}_residual_distortion.nii.gz" -M -S)
        local percentiles=$(fslstats "${outdir}/${sub}_residual_distortion.nii.gz" -P 5 -P 95)
        
        echo "Range: $range_stats"
        echo "Mean ± SD: $mean_std"
        echo "5th-95th percentiles: $percentiles"
        
        # Volume overlap analysis
        echo ""
        echo "VOLUME OVERLAP ANALYSIS:"
        echo "======================="
        
        # Create binary masks for overlap analysis
        fslmaths b0_mean.nii.gz -bin b0_mask.nii.gz
        fslmaths T1_in_b0_space.nii.gz -bin T1_mask.nii.gz
        
        # Calculate Dice coefficient
        local dice_coeff=$($PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    b0_mask = nib.load('b0_mask.nii.gz').get_fdata()
    t1_mask = nib.load('T1_mask.nii.gz').get_fdata()
    
    intersection = np.sum(b0_mask * t1_mask)
    union = np.sum(b0_mask) + np.sum(t1_mask)
    
    dice = 2.0 * intersection / union if union > 0 else 0
    jaccard = intersection / (union - intersection) if (union - intersection) > 0 else 0
    
    print(f'Dice coefficient: {dice:.4f}')
    print(f'Jaccard index: {jaccard:.4f}')
    
    # Volume ratio
    t1_vol = np.sum(t1_mask)
    b0_vol = np.sum(b0_mask)
    vol_ratio = t1_vol / b0_vol if b0_vol > 0 else 0
    print(f'Volume ratio (T1/b0): {vol_ratio:.4f}')
    
except Exception as e:
    print(f'Overlap analysis failed: {e}')
" 2>/dev/null || echo "Overlap analysis failed")
        
        echo "$dice_coeff"
        
        # Registration quality assessment
        echo ""
        echo "REGISTRATION QUALITY ASSESSMENT:"
        echo "==============================="
        
        # Mutual information
        local mi_score=$($PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    # Check if files exist
    import os
    if not os.path.exists('b0_mean.nii.gz'):
        print('b0_mean.nii.gz not found')
        exit(1)
    if not os.path.exists('T1_in_b0_space.nii.gz'):
        print('T1_in_b0_space.nii.gz not found')
        exit(1)
    
    b0 = nib.load('b0_mean.nii.gz').get_fdata()
    t1 = nib.load('T1_in_b0_space.nii.gz').get_fdata()
    
    # Normalize intensities
    b0_norm = (b0 - b0.min()) / (b0.max() - b0.min()) if b0.max() > b0.min() else b0
    t1_norm = (t1 - t1.min()) / (t1.max() - t1.min()) if t1.max() > t1.min() else t1
    
    # Create masks
    mask = (b0_norm > 0.1) & (t1_norm > 0.1)
    
    if np.sum(mask) > 1000:
        # Simple MI calculation
        hist_2d, _, _ = np.histogram2d(
            b0_norm[mask].flatten(), 
            t1_norm[mask].flatten(), 
            bins=50
        )
        hist_2d = hist_2d + 1e-10
        
        pxy = hist_2d / np.sum(hist_2d)
        px = np.sum(pxy, axis=1)
        py = np.sum(pxy, axis=0)
        
        mi = 0
        for i in range(len(px)):
            for j in range(len(py)):
                if pxy[i, j] > 0:
                    mi += pxy[i, j] * np.log(pxy[i, j] / (px[i] * py[j]))
        
        # Correlation coefficient
        corr = np.corrcoef(b0_norm[mask], t1_norm[mask])[0, 1]
        
        print(f'Mutual information: {mi:.4f}')
        print(f'Correlation coefficient: {corr:.4f}')
        
        # Quality rating
        if mi > 0.5 and corr > 0.3:
            print('Registration quality: GOOD')
        elif mi > 0.3 and corr > 0.2:
            print('Registration quality: ACCEPTABLE')
        else:
            print('Registration quality: POOR')
    else:
        print('Insufficient overlap for quality assessment')
        
except Exception as e:
    print(f'Quality assessment failed: {e}')
" 2>/dev/null || echo "Quality assessment failed")
        
        echo "$mi_score"
        
        # Method-specific analysis
        echo ""
        echo "METHOD-SPECIFIC ANALYSIS:"
        echo "========================"
        case $registration_method in
            "SynthMorph")
                echo "SynthMorph benefits:"
                echo "- Robust to contrast differences"
                echo "- No need for tissue segmentation" 
                echo "- Fast execution time"
                echo "- Pre-trained on diverse datasets"
                ;;
            "Enhanced-ANTs")
                echo "Enhanced ANTs benefits:"
                echo "- High-precision registration"
                echo "- Multi-metric optimization"
                echo "- Robust convergence criteria"
                echo "- Suitable for challenging cases"
                ;;
            "BBR")
                echo "BBR registration characteristics:"
                echo "- Boundary-based optimization"
                echo "- Requires good WM segmentation"
                echo "- Sensitive to image quality"
                echo "- Standard FSL approach"
                ;;
        esac
        
        # Processing time if available
        if [ -f "${LOG_DIR}/resource_monitor.log" ]; then
            echo ""
            echo "PROCESSING EFFICIENCY:"
            echo "===================="
            local reg_entries=$(grep -c "$registration_method\|posthoc" "${LOG_DIR}/resource_monitor.log" 2>/dev/null || echo "0")
            echo "Resource monitor entries: $reg_entries"
            
            # Extract timing information if possible
            if [ $reg_entries -gt 0 ]; then
                echo "Registration method: $registration_method"
                echo "See resource_monitor.log for detailed timing"
            fi
        fi
        
    } > "${outdir}/${sub}_registration_report.txt"
    
    # Save detailed statistics
    fslstats "${outdir}/${sub}_residual_distortion.nii.gz" -R > \
        "${outdir}/${sub}_residual_distortion_range.txt"
    fslstats "${outdir}/${sub}_residual_distortion.nii.gz" -M -S > \
        "${outdir}/${sub}_residual_distortion_stats.txt"
    
    # Create visualization if possible
    if command -v slicer >/dev/null 2>&1; then
        log "INFO" "[${sub}] Creating registration visualization"
        
        # Create overlay visualization
        slicer b0_mean.nii.gz T1_in_b0_space.nii.gz \
            -a "${outdir}/${sub}_registration_overlay.png" 2>/dev/null || true
        
        # Create residual map visualization
        slicer "${outdir}/${sub}_residual_distortion.nii.gz" \
            -a "${outdir}/${sub}_residual_distortion_map.png" 2>/dev/null || true
    fi
    
    # Copy registration report to QC directory
    cp "${outdir}/${sub}_registration_report.txt" "${outdir}/qc/" 2>/dev/null || true
    
    # Clean up temporary files
    rm -f b0_mask.nii.gz T1_mask.nii.gz 2>/dev/null || true
    
    log "OK" "[${sub}] Registration analysis completed using $registration_method"
}

generate_refined_qc() {
    local sub=$1
    local ml_applied=${2:-false}
    local outdir="${POSTHOC_DIR}/${sub}"
    local qcdir="${outdir}/qc"
    mkdir -p "$qcdir"
    
    log "INFO" "[${sub}] Generating comprehensive refined QC report"
    
    # Re-compute DTI metrics from refined data
    mrconvert "${outdir}/${sub}_dwi_refined.nii.gz" dwi_refined.mif \
        -fslgrad "${outdir}/${sub}_dwi.bvec" "${outdir}/${sub}_dwi.bval" \
        -quiet -force
    
    mrconvert "${outdir}/${sub}_mask_enhanced.nii.gz" mask_refined.mif \
        -datatype bit -quiet -force
    
    dwi2tensor dwi_refined.mif tensor_refined.mif -mask mask_refined.mif -quiet -force
    tensor2metric tensor_refined.mif \
        -fa fa_refined.mif -adc md_refined.mif \
        -ad ad_refined.mif -rd rd_refined.mif -quiet -force
    
    { 
        echo "Comprehensive Refined DTI Analysis for ${sub}"
        echo "============================================="
        echo "Generated on: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        echo ""
        
        echo "PROCESSING SUMMARY:"
        echo "=================="
        echo "- Enhanced bias correction: Yes"
        echo "- Intensity normalization: Yes"
        echo "- Enhanced brain mask: Yes"
        echo "- ML registration refinement: $ml_applied"
        
        if [ "$ml_applied" = true ]; then
            echo "- ML method: ${ML_REGISTRATION_METHOD:-auto}"
            echo "- ML mode: $([ "${ML_QUICK_MODE:-true}" = true ] && echo "Quick" || echo "Full")"
            echo "- GPU acceleration: ${GPU_AVAILABLE:-false}"
        fi
        
        # Registration method used for T1w-DWI
        if [ -f "${outdir}/${sub}_registration_report.txt" ]; then
            local reg_method=$(grep "Method used:" "${outdir}/${sub}_registration_report.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "Unknown")
            echo "- T1w-DWI registration: $reg_method"
            
            # Include registration quality
            local reg_quality=$(grep "Registration quality:" "${outdir}/${sub}_registration_report.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "Unknown")
            echo "- Registration quality: $reg_quality"
        else
            echo "- T1w-DWI registration: Not performed (T1w not available)"
        fi
        
        echo ""
        
        # Detailed DTI metrics comparison
        echo "DTI METRICS COMPARISON:"
        echo "======================"
        local basic_dir="${EXTERNAL_MRTRIX}/${sub}"
        
        if [ -f "${basic_dir}/${sub}_fa.nii.gz" ]; then
            echo ""
            echo "Comparing basic vs refined preprocessing:"
            echo "----------------------------------------"
            
            for metric in fa md ad rd; do
                echo ""
                echo "${metric^^} Comparison:"
                
                # Basic stats
                echo "  Basic preprocessing:"
                local basic_stats=$(mrstats "${basic_dir}/${sub}_${metric}.nii.gz" -mask "${basic_dir}/${sub}_mask.nii.gz" -output mean,std 2>/dev/null || echo "N/A N/A")
                echo "    Mean ± SD: $basic_stats"
                
                # Refined stats  
                echo "  Refined preprocessing:"
                local refined_stats=$(mrstats "${metric}_refined.mif" -mask mask_refined.mif -output mean,std 2>/dev/null || echo "N/A N/A")
                echo "    Mean ± SD: $refined_stats"
                
                # Calculate improvement
                if [ "$basic_stats" != "N/A N/A" ] && [ "$refined_stats" != "N/A N/A" ]; then
                    $PYTHON_EXECUTABLE -c "
basic = '$basic_stats'.split()
refined = '$refined_stats'.split()
try:
    basic_mean, basic_std = float(basic[0]), float(basic[1])
    refined_mean, refined_std = float(refined[0]), float(refined[1])
    
    mean_change = ((refined_mean - basic_mean) / basic_mean * 100) if basic_mean != 0 else 0
    std_change = ((refined_std - basic_std) / basic_std * 100) if basic_std != 0 else 0
    
    print(f'    Improvement: Mean {mean_change:+.1f}%, SD {std_change:+.1f}%')
except:
    print('    Improvement: Calculation failed')
" 2>/dev/null || echo "    Improvement: Calculation failed"
                fi
            done
            
            # Mask comparison
            echo ""
            echo "MASK COMPARISON:"
            echo "==============="
            local basic_mask_vol=$(mrstats "${basic_dir}/${sub}_mask.nii.gz" -output count 2>/dev/null || echo "N/A")
            local refined_mask_vol=$(mrstats mask_refined.mif -output count 2>/dev/null || echo "N/A")
            echo "Basic mask volume: $basic_mask_vol voxels"
            echo "Enhanced mask volume: $refined_mask_vol voxels"
            
            if [ "$basic_mask_vol" != "N/A" ] && [ "$refined_mask_vol" != "N/A" ]; then
                local volume_change=$($PYTHON_EXECUTABLE -c "
try:
    change = ($refined_mask_vol - $basic_mask_vol) / $basic_mask_vol * 100
    print(f'{change:+.1f}%')
except:
    print('N/A')
" 2>/dev/null || echo "N/A")
                echo "Volume change: $volume_change"
            fi
        fi
        
        # Detailed refined metrics
        echo ""
        echo "REFINED METRICS DETAILED STATISTICS:"
        echo "===================================="
        
        for metric in fa md ad rd; do
            echo ""
            echo "${metric^^} (refined):"
            echo "----------"
            mrstats "${metric}_refined.mif" -mask mask_refined.mif 2>/dev/null || echo "  Error computing detailed stats"
            
            # Additional percentile analysis
            # Convert .mif to .nii.gz for nibabel (nibabel does not support .mif)
            mrconvert "${metric}_refined.mif" "${metric}_refined_tmp.nii.gz" -quiet -force 2>/dev/null || true
            mrconvert "mask_refined.mif" "mask_refined_tmp.nii.gz" -quiet -force 2>/dev/null || true
            local percentiles=$($PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    img = nib.load('${metric}_refined_tmp.nii.gz').get_fdata()
    mask = nib.load('mask_refined_tmp.nii.gz').get_fdata()
    
    masked_data = img[mask > 0]
    if len(masked_data) > 0:
        p25 = np.percentile(masked_data, 25)
        p50 = np.percentile(masked_data, 50)
        p75 = np.percentile(masked_data, 75)
        print(f'  Percentiles - 25th: {p25:.4f}, 50th: {p50:.4f}, 75th: {p75:.4f}')
    else:
        print('  Percentiles: No valid data in mask')
except Exception as e:
    print(f'  Percentiles: Calculation failed ({e})')
" 2>/dev/null || echo "  Percentiles: Calculation failed")
            rm -f "${metric}_refined_tmp.nii.gz" "mask_refined_tmp.nii.gz" 2>/dev/null
            echo "$percentiles"
        done
        
        # Signal quality assessment
        echo ""
        echo "SIGNAL QUALITY ASSESSMENT:"
        echo "========================="
        
        # SNR estimation for refined data
        $PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    dwi = nib.load('${outdir}/${sub}_dwi_refined.nii.gz').get_fdata()
    mask = nib.load('${outdir}/${sub}_mask_enhanced.nii.gz').get_fdata()
    
    # Estimate SNR from first few volumes (assumed b0)
    b0_vols = dwi[..., :min(5, dwi.shape[-1])]
    
    if np.sum(mask > 0) > 100:
        signal = np.mean(b0_vols[mask > 0])
        noise = np.std(b0_vols[mask > 0])
        snr = signal / noise if noise > 0 else 0
        
        print(f'Refined SNR estimate: {snr:.2f}')
        print(f'Signal intensity (mean b0): {signal:.2f}')
        print(f'Noise level (std b0): {noise:.2f}')
        
        # Compare with basic if available
        try:
            basic_dwi = nib.load('${basic_dir}/${sub}_dwi_preproc.nii.gz').get_fdata()
            basic_mask = nib.load('${basic_dir}/${sub}_mask.nii.gz').get_fdata()
            basic_b0 = basic_dwi[..., :min(5, basic_dwi.shape[-1])]
            
            if np.sum(basic_mask > 0) > 100:
                basic_signal = np.mean(basic_b0[basic_mask > 0])
                basic_noise = np.std(basic_b0[basic_mask > 0])
                basic_snr = basic_signal / basic_noise if basic_noise > 0 else 0
                
                snr_improvement = ((snr - basic_snr) / basic_snr * 100) if basic_snr > 0 else 0
                print(f'Basic SNR estimate: {basic_snr:.2f}')
                print(f'SNR improvement: {snr_improvement:+.1f}%')
            
        except:
            pass
            
    else:
        print('SNR calculation failed: insufficient mask coverage')
        
except Exception as e:
    print(f'Signal quality assessment failed: {e}')
" 2>/dev/null || echo "Signal quality assessment failed"
        
        # ML-specific quality metrics
        if [ "$ml_applied" = true ]; then
            echo ""
            echo "ML REGISTRATION QUALITY METRICS:"
            echo "==============================="
            
            # Include ML-specific reports if available
            for ml_report in "${qcdir}/${sub}_ml_registration_report.txt" \
                            "${qcdir}/${sub}_synthmorph_quality.txt" \
                            "${qcdir}/${sub}_ants_quality.txt"; do
                if [ -f "$ml_report" ]; then
                    echo "Including: $(basename "$ml_report")"
                    echo "See separate ML quality report for details"
                    break
                fi
            done
            
            # Performance comparison
            echo ""
            echo "ML Performance Summary:"
            echo "- Method: ${ML_REGISTRATION_METHOD:-auto}"
            echo "- GPU used: ${GPU_AVAILABLE:-false}"
            echo "- Quick mode: ${ML_QUICK_MODE:-true}"
        fi
        
        # Overall quality rating
        echo ""
        echo "OVERALL QUALITY ASSESSMENT:"
        echo "=========================="
        
        # Calculate overall quality score
        # Convert .mif to .nii.gz for nibabel
        mrconvert fa_refined.mif fa_refined_qc.nii.gz -quiet -force 2>/dev/null || true
        mrconvert mask_refined.mif mask_refined_qc.nii.gz -quiet -force 2>/dev/null || true
        local overall_quality=$($PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np

score = 0
max_score = 0

try:
    # FA quality (0-25 points)
    fa = nib.load('fa_refined_qc.nii.gz').get_fdata()
    mask = nib.load('mask_refined_qc.nii.gz').get_fdata()
    
    fa_masked = fa[mask > 0]
    if len(fa_masked) > 100:
        fa_mean = np.mean(fa_masked)
        fa_std = np.std(fa_masked)
        
        # Good FA range and variability
        if 0.3 <= fa_mean <= 0.5 and 0.1 <= fa_std <= 0.25:
            score += 25
        elif 0.25 <= fa_mean <= 0.6 and 0.05 <= fa_std <= 0.3:
            score += 15
        else:
            score += 5
    max_score += 25
    
    # Mask quality (0-25 points)
    mask_vol = np.sum(mask > 0)
    total_vol = np.prod(mask.shape)
    mask_ratio = mask_vol / total_vol
    
    if 0.15 <= mask_ratio <= 0.4:  # Reasonable brain coverage
        score += 25
    elif 0.1 <= mask_ratio <= 0.5:
        score += 15
    else:
        score += 5
    max_score += 25
    
    # Registration quality (0-25 points) - if available
    try:
        with open('${outdir}/${sub}_registration_report.txt', 'r') as f:
            content = f.read()
            if 'GOOD' in content:
                score += 25
            elif 'ACCEPTABLE' in content:
                score += 15
            else:
                score += 5
    except:
        score += 10  # Neutral score if no registration
    max_score += 25
    
    # Processing completeness (0-25 points)
    files_exist = [
        '${outdir}/${sub}_dwi_refined.nii.gz',
        '${outdir}/${sub}_mask_enhanced.nii.gz'
    ]
    
    existing_files = sum(1 for f in files_exist if __import__('os').path.exists(f))
    score += int(25 * existing_files / len(files_exist))
    max_score += 25
    
    final_score = (score / max_score * 100) if max_score > 0 else 0
    
    print(f'Quality score: {final_score:.1f}/100')
    
    if final_score >= 80:
        print('Overall quality: EXCELLENT')
    elif final_score >= 65:
        print('Overall quality: GOOD')  
    elif final_score >= 50:
        print('Overall quality: ACCEPTABLE')
    else:
        print('Overall quality: NEEDS REVIEW')
        
    print(f'Component scores: FA={score-75+25}/25, Mask=25/25, Registration=varies, Files={existing_files}/{len(files_exist)}')
    
except Exception as e:
    print(f'Quality assessment failed: {e}')
    print('Overall quality: UNKNOWN')
" 2>/dev/null || echo "Quality assessment failed")
        
        echo "$overall_quality"
        
        # Processing efficiency summary
        if [ -f "${LOG_DIR}/${sub}_progress.json" ]; then
            echo ""
            echo "PROCESSING EFFICIENCY:"
            echo "===================="
            $PYTHON_EXECUTABLE -c "
import json
try:
    with open('${LOG_DIR}/${sub}_progress.json', 'r') as f:
        data = json.load(f)
    
    print(f'Last update: {data.get(\"last_update\", \"N/A\")}')
    
    stages = ['preprocessing', 'posthoc']
    for stage in stages:
        if stage in data:
            print(f'{stage.title()}: {data[stage]}% complete')
            
    # Calculate approximate processing time if timestamps available
    if 'last_update' in data:
        print('Processing completed successfully')
    else:
        print('Processing status: In progress or incomplete')
        
except Exception as e:
    print(f'Efficiency data unavailable: {e}')
" 2>/dev/null || echo "Processing efficiency data not available"
        fi
        
        echo ""
        echo "RECOMMENDATIONS:"
        echo "==============="
        
        # Generate recommendations based on quality metrics
        # Reuse the .nii.gz files converted earlier; reconvert if missing
        [ ! -f "fa_refined_qc.nii.gz" ] && mrconvert fa_refined.mif fa_refined_qc.nii.gz -quiet -force 2>/dev/null || true
        [ ! -f "mask_refined_qc.nii.gz" ] && mrconvert mask_refined.mif mask_refined_qc.nii.gz -quiet -force 2>/dev/null || true
        $PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np

try:
    # Load FA for recommendations
    fa = nib.load('fa_refined_qc.nii.gz').get_fdata()
    mask = nib.load('mask_refined_qc.nii.gz').get_fdata()
    
    fa_masked = fa[mask > 0]
    
    if len(fa_masked) > 100:
        fa_mean = np.mean(fa_masked)
        fa_std = np.std(fa_masked)
        
        print('Based on refined data quality:')
        
        if fa_mean < 0.25:
            print('- Consider reviewing data acquisition parameters')
            print('- Low FA values may indicate processing issues')
        elif fa_mean > 0.6:
            print('- Unusually high FA values detected')
            print('- Verify mask quality and processing parameters')
        else:
            print('- FA values within expected range')
        
        if fa_std < 0.05:
            print('- Low FA variability may indicate over-smoothing')
        elif fa_std > 0.3:
            print('- High FA variability may indicate noise or artifacts')
        else:
            print('- FA variability within acceptable range')
            
        # ML-specific recommendations
        if '$ml_applied' == 'true':
            print('- ML registration applied successfully')
            print('- Consider using refined data for further analysis')
        else:
            print('- Consider enabling ML registration for improved quality')
            print('- Use --use-ml-registration flag in future processing')
            
    else:
        print('- Insufficient mask coverage for reliable recommendations')
        print('- Review brain extraction and masking procedures')
        
except Exception as e:
    print(f'- Recommendation generation failed: {e}')
    print('- Manual quality review recommended')
" 2>/dev/null || echo "- Manual quality review recommended"
        
        echo ""
        echo "Report generated: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        
    } > "${qcdir}/${sub}_refined_comprehensive_report.txt"
    
    # Generate enhanced visualizations
    create_enhanced_visualizations "$sub" "$ml_applied"
    
    # Copy all relevant reports to QC directory
    for report in "${outdir}/${sub}_registration_report.txt" \
                  "${outdir}/${sub}_residual_distortion_range.txt" \
                  "${outdir}/${sub}_residual_distortion_stats.txt"; do
        [ -f "$report" ] && cp "$report" "$qcdir/" 2>/dev/null || true
    done
    
    # Clean up temporary MIF files and QC temp files
    rm -f dwi_refined.mif mask_refined.mif tensor_refined.mif \
          fa_refined.mif md_refined.mif ad_refined.mif rd_refined.mif \
          fa_refined_qc.nii.gz mask_refined_qc.nii.gz
    
    log "OK" "[${sub}] Comprehensive refined QC report generated"
}

create_enhanced_visualizations() {
    local sub=$1
    local ml_applied=$2
    local qcdir="${POSTHOC_DIR}/${sub}/qc"
    
    if ! command -v slicer >/dev/null 2>&1; then
        log "INFO" "[${sub}] Slicer not available, skipping enhanced visualizations"
        return 0
    fi
    
    log "INFO" "[${sub}] Creating enhanced visualizations"
    
    # FA comparison if basic data available
    local basic_dir="${EXTERNAL_MRTRIX}/${sub}"
    if [ -f "${basic_dir}/${sub}_fa.nii.gz" ]; then
        # Create side-by-side FA comparison
        slicer "${basic_dir}/${sub}_fa.nii.gz" \
            -a "${qcdir}/${sub}_fa_basic_mosaic.png" 2>/dev/null || true
            
        # Create refined FA mosaic (need to export from MIF first)
        if [ -f fa_refined.mif ]; then
            mrconvert fa_refined.mif "${qcdir}/${sub}_fa_refined_temp.nii.gz" -quiet -force 2>/dev/null
            slicer "${qcdir}/${sub}_fa_refined_temp.nii.gz" \
                -a "${qcdir}/${sub}_fa_refined_mosaic.png" 2>/dev/null || true
            rm -f "${qcdir}/${sub}_fa_refined_temp.nii.gz"
        fi
    fi
    
    # Registration overlay if available
    if [ -f "T1_in_b0_space.nii.gz" ] && [ -f "b0_mean.nii.gz" ]; then
        slicer b0_mean.nii.gz T1_in_b0_space.nii.gz \
            -a "${qcdir}/${sub}_registration_overlay.png" 2>/dev/null || true
    fi
    
    # Enhanced mask visualization
    if [ -f mask_refined.mif ]; then
        mrconvert mask_refined.mif "${qcdir}/${sub}_mask_refined_temp.nii.gz" -quiet -force 2>/dev/null
        slicer "${qcdir}/${sub}_mask_refined_temp.nii.gz" \
            -a "${qcdir}/${sub}_mask_refined_mosaic.png" 2>/dev/null || true
        rm -f "${qcdir}/${sub}_mask_refined_temp.nii.gz"
    fi
    
    log "OK" "[${sub}] Enhanced visualizations created"
}


# --- Stage 2: Post-hoc Refinement (Final) and FreeSurfer Output Validation ---

# Continuation of residual distortion analysis with enhanced reporting
generate_distortion_correction_report() {
    local sub=$1
    local registration_method=$2
    local outdir="${POSTHOC_DIR}/${sub}"
    
    log "INFO" "[${sub}] Generating distortion correction summary"
    
    {
        echo "Distortion Correction Summary for ${sub}"
        echo "========================================"
        echo "Generated on: $(date)"
        echo "Registration method: $registration_method"
        echo ""
        
        # Synb0 correction status
        echo "DISTORTION CORRECTION PIPELINE:"
        echo "==============================="
        if [ -f "${EXTERNAL_SYNB0}/${sub}/OUTPUTS/b0_u.nii.gz" ]; then
            echo "1. Synb0-DisCo: APPLIED"
            echo "   - Synthetic b0 created for distortion correction"
            echo "   - Used in FSL eddy correction"
        else
            echo "1. Synb0-DisCo: NOT APPLIED"
            echo "   - Motion correction only (no distortion correction)"
        fi
        
        # ML registration status
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "2. ML Registration: ENABLED"
            echo "   - Method: ${ML_REGISTRATION_METHOD:-auto}"
            echo "   - Applied in: $([ "$registration_method" != "BBR" ] && echo "T1w-DWI registration" || echo "Fallback to traditional")"
        else
            echo "2. ML Registration: DISABLED"
        fi
        
        # Post-hoc registration assessment
        echo "3. T1w-DWI Registration: $registration_method"
        
        if [ -f "${outdir}/${sub}_residual_distortion_stats.txt" ]; then
            local mean_residual=$(awk '{print $1}' "${outdir}/${sub}_residual_distortion_stats.txt" 2>/dev/null || echo "N/A")
            local std_residual=$(awk '{print $2}' "${outdir}/${sub}_residual_distortion_stats.txt" 2>/dev/null || echo "N/A")
            
            echo "   - Mean residual: $mean_residual"
            echo "   - Std residual: $std_residual"
            
            # Interpret residuals
            if [ "$mean_residual" != "N/A" ]; then
                $PYTHON_EXECUTABLE -c "
try:
    mean_abs = abs(float('$mean_residual'))
    if mean_abs < 1.0:
        print('   - Assessment: EXCELLENT correction')
    elif mean_abs < 2.0:
        print('   - Assessment: GOOD correction')
    elif mean_abs < 5.0:
        print('   - Assessment: ACCEPTABLE correction')
    else:
        print('   - Assessment: POOR correction - review needed')
except:
    print('   - Assessment: Unable to evaluate')
" 2>/dev/null
            fi
        fi
        
        echo ""
        echo "CORRECTION EFFECTIVENESS:"
        echo "========================"
        
        # Compare pre and post correction if data available
        local effectiveness="Unknown"
        case $registration_method in
            "SynthMorph")
                effectiveness="ML-based registration typically provides robust correction"
                ;;
            "Enhanced-ANTs")  
                effectiveness="Multi-metric optimization provides high-precision correction"
                ;;
            "BBR")
                effectiveness="Boundary-based registration effective for good quality data"
                ;;
        esac
        
        echo "Method characteristics: $effectiveness"
        
        # Recommendations
        echo ""
        echo "RECOMMENDATIONS:"
        echo "==============="
        
        if [ -f "${EXTERNAL_SYNB0}/${sub}/OUTPUTS/b0_u.nii.gz" ]; then
            echo "✓ Synb0-DisCo correction applied - good foundation for analysis"
        else
            echo "⚠ Consider enabling Synb0-DisCo for better distortion correction"
        fi
        
        if [ "$registration_method" != "BBR" ]; then
            echo "✓ Advanced registration method used - enhanced accuracy expected"
        else
            echo "ℹ Traditional registration used - consider ML methods for challenging cases"
        fi
        
    } > "${outdir}/${sub}_distortion_correction_summary.txt"
    
    # Copy to QC directory
    cp "${outdir}/${sub}_distortion_correction_summary.txt" "${outdir}/qc/" 2>/dev/null || true
}

# Enhanced FreeSurfer output validation with ML integration
check_freesurfer_outputs() {
    local sub=$1
    local fs_dir=$2
    
    log "INFO" "[${sub}] Validating FreeSurfer outputs with enhanced checks"
    
    # Check if FreeSurfer already completed in external storage
    if [ -d "${EXTERNAL_FS}/${sub}" ]; then
        log "INFO" "[${sub}] Checking FreeSurfer outputs in external storage"
        
        local required_files=(
            "mri/aparc+aseg.mgz"
            "mri/brain.mgz"
            "mri/T1.mgz"
            "surf/lh.pial"
            "surf/rh.pial"
            "surf/lh.white"
            "surf/rh.white"
            "surf/lh.inflated"
            "surf/rh.inflated"
            "scripts/recon-all.done"
        )
        
        local critical_files=(
            "mri/aparc+aseg.mgz"
            "surf/lh.pial"
            "surf/rh.pial"
            "scripts/recon-all.done"
        )
        
        local all_critical_exist=true
        local all_files_exist=true
        local missing_files=()
        local missing_critical=()
        
        # Check all required files
        for file in "${required_files[@]}"; do
            if [ ! -f "${EXTERNAL_FS}/${sub}/${file}" ]; then
                missing_files+=("$file")
                all_files_exist=false
                
                # Check if it's a critical file
                for critical in "${critical_files[@]}"; do
                    if [ "$file" = "$critical" ]; then
                        missing_critical+=("$file")
                        all_critical_exist=false
                        break
                    fi
                done
            fi
        done
        
        if [ "$all_critical_exist" = true ]; then
            log "OK" "[${sub}] FreeSurfer critical outputs found in external storage"
            
            # Validate FreeSurfer output quality
            validate_freesurfer_quality "$sub" "${EXTERNAL_FS}/${sub}"
            
            if [ "$all_files_exist" = false ]; then
                log "WARN" "[${sub}] Some non-critical FreeSurfer files missing: ${missing_files[*]}"
            else
                log "OK" "[${sub}] All FreeSurfer outputs complete"
            fi
            
            return 0
        else
            log "ERROR" "[${sub}] Critical FreeSurfer files missing: ${missing_critical[*]}"
            return 1
        fi
    fi
    
    # Check local FreeSurfer directory
    if [ -n "$fs_dir" ] && [ -d "$fs_dir" ]; then
        if [ -f "${fs_dir}/scripts/recon-all.done" ]; then
            log "INFO" "[${sub}] FreeSurfer processing completed locally"
            validate_freesurfer_quality "$sub" "$fs_dir"
            return 0
        else
            log "INFO" "[${sub}] FreeSurfer processing not completed locally"
            return 1
        fi
    fi
    
    log "INFO" "[${sub}] No FreeSurfer outputs found"
    return 1
}

validate_freesurfer_quality() {
    local sub=$1
    local fs_dir=$2
    
    log "INFO" "[${sub}] Validating FreeSurfer reconstruction quality"
    
    # Create quality assessment report
    local qc_file="${EXTERNAL_QC}/${sub}_freesurfer_quality.txt"
    mkdir -p "$(dirname "$qc_file")"
    
    {
        echo "FreeSurfer Quality Assessment for ${sub}"
        echo "========================================"
        echo "Generated on: $(date)"
        echo "FreeSurfer directory: $fs_dir"
        echo ""
        
        # Check log for errors and warnings
        echo "LOG ANALYSIS:"
        echo "============"
        if [ -f "${fs_dir}/scripts/recon-all.log" ]; then
            local error_count=$(grep -c "ERROR\|FAILED\|Error\|Failed" "${fs_dir}/scripts/recon-all.log" 2>/dev/null || echo "0")
            local warning_count=$(grep -c "WARNING\|Warning" "${fs_dir}/scripts/recon-all.log" 2>/dev/null || echo "0")
            
            echo "Errors found: $error_count"
            echo "Warnings found: $warning_count"
            
            if [ "$error_count" -gt 0 ]; then
                echo ""
                echo "Recent errors:"
                grep "ERROR\|FAILED\|Error\|Failed" "${fs_dir}/scripts/recon-all.log" | tail -5 || echo "None found"
            fi
            
            # Check processing time
            if [ -f "${fs_dir}/scripts/recon-all.done" ]; then
                local start_time=$(stat -c %Y "${fs_dir}/mri/orig/001.mgz" 2>/dev/null || echo "")
                local end_time=$(stat -c %Y "${fs_dir}/scripts/recon-all.done" 2>/dev/null || echo "")
                
                if [ -n "$start_time" ] && [ -n "$end_time" ]; then
                    local duration=$((end_time - start_time))
                    local hours=$((duration / 3600))
                    local minutes=$(((duration % 3600) / 60))
                    echo "Processing time: ${hours}h ${minutes}m"
                fi
            fi
        else
            echo "Log file not found"
        fi
        
        echo ""
        echo "SURFACE ANALYSIS:"
        echo "================"
        
        # Validate surface files
        for hemi in lh rh; do
            echo ""
            echo "${hemi^^} hemisphere:"
            
            for surf in white pial inflated; do
                local surf_file="${fs_dir}/surf/${hemi}.${surf}"
                if [ -f "$surf_file" ]; then
                    # Get surface statistics using mris_info if available
                    if command -v mris_info &>/dev/null; then
                        local surf_info=$(mris_info "$surf_file" 2>/dev/null | grep -E "vertices|faces" || echo "Info not available")
                        echo "  ${surf}: $(echo $surf_info | tr '\n' ' ')"
                    else
                        echo "  ${surf}: Present"
                    fi
                else
                    echo "  ${surf}: MISSING"
                fi
            done
        done
        
        echo ""
        echo "SEGMENTATION ANALYSIS:"
        echo "===================="
        
        if [ -f "${fs_dir}/mri/aparc+aseg.mgz" ]; then
            echo "Parcellation: Present"
            
            # Analyze segmentation if mri_segstats is available
            if command -v mri_segstats &>/dev/null; then
                echo ""
                echo "Volume statistics (selected regions):"
                
                # Get key brain region volumes
                local seg_stats=$(mri_segstats --seg "${fs_dir}/mri/aparc+aseg.mgz" \
                                              --ctab $FREESURFER_HOME/FreeSurferColorLUT.txt \
                                              --sum /tmp/${sub}_segstats.$$.txt 2>/dev/null && \
                                  grep -E "Left-Cerebral-White-Matter|Right-Cerebral-White-Matter|Left-Cerebral-Cortex|Right-Cerebral-Cortex" \
                                       /tmp/${sub}_segstats.$$.txt 2>/dev/null | \
                                  awk '{print "  " $5 ": " $4 " mm³"}' && \
                                  rm -f /tmp/${sub}_segstats.$$.txt 2>/dev/null)
                
                if [ -n "$seg_stats" ]; then
                    echo "$seg_stats"
                else
                    echo "  Volume analysis not available"
                fi
            fi
        else
            echo "Parcellation: MISSING"
        fi
        
        echo ""
        echo "QUALITY METRICS:"
        echo "==============="
        
        # Check for common quality indicators
        local quality_score=0
        local max_score=100
        
        # Surface completeness (40 points)
        local surface_score=0
        for hemi in lh rh; do
            for surf in white pial; do
                [ -f "${fs_dir}/surf/${hemi}.${surf}" ] && surface_score=$((surface_score + 5))
            done
        done
        echo "Surface completeness: ${surface_score}/20 points"
        quality_score=$((quality_score + surface_score * 2))
        
        # Segmentation (30 points)
        local seg_score=0
        [ -f "${fs_dir}/mri/aparc+aseg.mgz" ] && seg_score=30
        echo "Segmentation: ${seg_score}/30 points"
        quality_score=$((quality_score + seg_score))
        
        # Processing completion (30 points)
        local completion_score=0
        [ -f "${fs_dir}/scripts/recon-all.done" ] && completion_score=30
        echo "Processing completion: ${completion_score}/30 points"
        quality_score=$((quality_score + completion_score))
        
        echo ""
        echo "OVERALL QUALITY SCORE: ${quality_score}/${max_score}"
        
        if [ $quality_score -ge 90 ]; then
            echo "Quality assessment: EXCELLENT"
        elif [ $quality_score -ge 75 ]; then
            echo "Quality assessment: GOOD"
        elif [ $quality_score -ge 60 ]; then
            echo "Quality assessment: ACCEPTABLE"
        else
            echo "Quality assessment: POOR - Manual review recommended"
        fi
        
        # ML Integration recommendations
        echo ""
        echo "ML INTEGRATION NOTES:"
        echo "===================="
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "✓ ML registration enabled in pipeline"
            
            if [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
                                echo "✓ SynthMorph available for enhanced T1w-DWI registration"
                echo "  → FreeSurfer surfaces can be used with ML-registered DWI data"
            else
                echo "⚠ SynthMorph not available - using traditional registration methods"
            fi
            
            # Check if ML registration quality reports exist
            if [ -f "${EXTERNAL_QC}/${sub}_synthmorph_quality.txt" ]; then
                echo "✓ SynthMorph quality report available"
            elif [ -f "${EXTERNAL_QC}/${sub}_ants_quality.txt" ]; then
                echo "✓ Enhanced ANTs quality report available"
            fi
        else
            echo "ℹ ML registration disabled"
            echo "  → Consider enabling for improved T1w-DWI alignment"
        fi
        
        echo ""
        echo "CONNECTIVITY ANALYSIS READINESS:"
        echo "==============================="
        
        # Check readiness for connectivity analysis
        local connectivity_ready=true
        local missing_for_connectivity=()
        
        # Required files for 5ttgen and labelconvert
        local connectivity_files=(
            "mri/aparc+aseg.mgz"
            "surf/lh.pial"
            "surf/rh.pial"
            "surf/lh.white"
            "surf/rh.white"
        )
        
        for file in "${connectivity_files[@]}"; do
            if [ ! -f "${fs_dir}/${file}" ]; then
                connectivity_ready=false
                missing_for_connectivity+=("$file")
            fi
        done
        
        if [ "$connectivity_ready" = true ]; then
            echo "✓ All required files present for connectivity analysis"
            echo "✓ Ready for 5ttgen (5-tissue-type segmentation)"
            echo "✓ Ready for labelconvert (parcellation conversion)"
        else
            echo "✗ Missing files for connectivity analysis:"
            for file in "${missing_for_connectivity[@]}"; do
                echo "  - $file"
            done
        fi
        
        # Additional quality checks for connectivity
        if [ "$connectivity_ready" = true ]; then
            echo ""
            echo "Connectivity-specific validation:"
            
            # Check if surfaces are reasonable
            if command -v mris_info &>/dev/null; then
                for hemi in lh rh; do
                    local vertex_count=$(mris_info "${fs_dir}/surf/${hemi}.pial" 2>/dev/null | grep "vertices" | awk '{print $3}' || echo "0")
                    if [ "$vertex_count" -gt 100000 ] && [ "$vertex_count" -lt 200000 ]; then
                        echo "  ✓ ${hemi} surface vertex count reasonable: $vertex_count"
                    else
                        echo "  ⚠ ${hemi} surface vertex count unusual: $vertex_count"
                    fi
                done
            fi
        fi
        
    } > "$qc_file"
    
    log "OK" "[${sub}] FreeSurfer quality assessment completed"
    
    # Create summary for main log
    local quality_line=$(grep "Quality assessment:" "$qc_file" | cut -d: -f2 | xargs)
    log "INFO" "[${sub}] FreeSurfer quality: $quality_line"
}

# Enhanced connectivity readiness check with ML integration
check_connectivity_readiness() {
    local sub=$1
    
    log "INFO" "[${sub}] Checking connectivity analysis readiness"
    
    local readiness_report="${EXTERNAL_QC}/${sub}_connectivity_readiness.txt"
    
    {
        echo "Connectivity Analysis Readiness for ${sub}"
        echo "=========================================="
        echo "Generated on: $(date)"
        echo ""
        
        echo "REQUIRED COMPONENTS CHECK:"
        echo "========================="
        
        # 1. FreeSurfer outputs
        echo "1. FreeSurfer Reconstruction:"
        if check_freesurfer_outputs "$sub" "${EXTERNAL_FS}/${sub}"; then
            echo "   ✓ AVAILABLE - FreeSurfer reconstruction complete"
            local fs_ready=true
        else
            echo "   ✗ MISSING - FreeSurfer reconstruction required"
            local fs_ready=false
        fi
        
        # 2. DWI preprocessing
        echo ""
        echo "2. DWI Preprocessing:"
        local dwi_file=""
        local dwi_source=""
        
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            dwi_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz"
            dwi_source="Refined (Post-hoc)"
            echo "   ✓ AVAILABLE - Refined DWI data found"
        elif [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz" ]; then
            dwi_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz"
            dwi_source="Basic preprocessing"
            echo "   ✓ AVAILABLE - Basic preprocessed DWI data found"
        else
            echo "   ✗ MISSING - No preprocessed DWI data found"
            local dwi_ready=false
        fi
        
        if [ -n "$dwi_file" ]; then
            local dwi_ready=true
            echo "   Source: $dwi_source"
            
            # Check gradients
# ✅ FIXED - Handle both basic and refined gradient file naming:
local bvec_file=""
local bval_file=""

# Try refined naming first, then fall back to basic naming
if [[ "$dwi_file" == *"_refined"* ]] || [[ "$dwi_file" == *"_preproc"* ]]; then
    # For refined/preproc DWI, gradient files use base naming (without suffix)
    local base_path=$(dirname "$dwi_file")
    local subject=$(basename "$dwi_file" | cut -d'_' -f1)
    bvec_file="${base_path}/${subject}_dwi.bvec"
    bval_file="${base_path}/${subject}_dwi.bval"
else
    # For basic DWI, try subject-based naming first, then strip extension
    local base_path=$(dirname "$dwi_file")
    local subject=$(basename "$dwi_file" | cut -d'_' -f1)
    if [ -f "${base_path}/${subject}_dwi.bvec" ]; then
        bvec_file="${base_path}/${subject}_dwi.bvec"
        bval_file="${base_path}/${subject}_dwi.bval"
    else
        bvec_file="${dwi_file%.*.*}.bvec"
        bval_file="${dwi_file%.*.*}.bval"
    fi
fi

if [ -f "$bvec_file" ] && [ -f "$bval_file" ]; then
                echo "   ✓ Gradient files available"
                
                # Analyze gradient scheme
                local num_dirs=$(head -1 "$bvec_file" 2>/dev/null | wc -w || echo "0")
                local unique_bvals=$(awk '{for(i=1;i<=NF;i++) if($i>50) print int($i/100)*100}' "$bval_file" | sort -nu | wc -l 2>/dev/null || echo "0")
                
                echo "   Gradient directions: $num_dirs"
                echo "   Unique b-values: $unique_bvals"
                
                if [ "$num_dirs" -ge 30 ] && [ "$unique_bvals" -ge 1 ]; then
                    echo "   ✓ Adequate for tractography"
                else
                    echo "   ⚠ May be suboptimal for tractography"
                fi
            else
                echo "   ✗ Gradient files missing"
                dwi_ready=false
            fi
        fi
        
        # 3. Brain mask
        echo ""
        echo "3. Brain Mask:"
        local mask_file=""
        
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_mask_enhanced.nii.gz" ]; then
            mask_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_mask_enhanced.nii.gz"
            echo "   ✓ AVAILABLE - Enhanced brain mask found"
        elif [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_mask.nii.gz" ]; then
            mask_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_mask.nii.gz"
            echo "   ✓ AVAILABLE - Basic brain mask found"
        else
            echo "   ✗ MISSING - Brain mask required"
            local mask_ready=false
        fi
        
        if [ -n "$mask_file" ]; then
            local mask_ready=true
        fi
        
        # 4. Registration quality
        echo ""
        echo "4. T1w-DWI Registration:"
        local reg_quality="Unknown"
        
        # Check for registration reports
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_registration_report.txt" ]; then
            reg_quality=$(grep "Registration quality:" "${EXTERNAL_POSTHOC}/${sub}/${sub}_registration_report.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "Unknown")
            local reg_method=$(grep "Method used:" "${EXTERNAL_POSTHOC}/${sub}/${sub}_registration_report.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "Unknown")
            
            echo "   ✓ AVAILABLE - Registration performed"
            echo "   Method: $reg_method"
            echo "   Quality: $reg_quality"
            
            case $reg_quality in
                "GOOD"|"EXCELLENT")
                    echo "   ✓ Registration quality sufficient for connectivity"
                    local reg_ready=true
                    ;;
                "ACCEPTABLE")
                    echo "   ⚠ Registration quality acceptable but may affect results"
                    local reg_ready=true
                    ;;
                *)
                    echo "   ⚠ Registration quality may be suboptimal"
                    local reg_ready=true  # Still allow processing
                    ;;
            esac
        else
            echo "   ⚠ No registration assessment available"
            echo "   → Registration will be performed during connectivity analysis"
            local reg_ready=true
        fi
        
        # ML Integration Assessment
        echo ""
        echo "ML INTEGRATION STATUS:"
        echo "====================="
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "✓ ML registration enabled in pipeline"
            
            if [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
                echo "✓ SynthMorph available for T1w-DWI registration"
                echo "  → Enhanced accuracy expected for connectivity analysis"
            fi
            
            if [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
                echo "✓ VoxelMorph available for DWI refinement"
                echo "  → Improved motion correction may benefit tractography"
            fi
            
            # Check for ML-specific quality reports
            local ml_reports=0
            for report in "${EXTERNAL_QC}/${sub}_synthmorph_quality.txt" \
                         "${EXTERNAL_QC}/${sub}_ants_quality.txt" \
                         "${EXTERNAL_QC}/${sub}_ml_registration_report.txt"; do
                [ -f "$report" ] && ml_reports=$((ml_reports + 1))
            done
            
            echo "ML quality reports available: $ml_reports"
        else
            echo "ℹ ML registration disabled"
            echo "  → Traditional registration methods will be used"
        fi
        
        # Overall readiness assessment
        echo ""
        echo "OVERALL READINESS ASSESSMENT:"
        echo "============================"
        
        local components_ready=0
        local total_components=4
        
        [ "${fs_ready:-false}" = true ] && components_ready=$((components_ready + 1))
        [ "${dwi_ready:-false}" = true ] && components_ready=$((components_ready + 1))
        [ "${mask_ready:-false}" = true ] && components_ready=$((components_ready + 1))
        [ "${reg_ready:-false}" = true ] && components_ready=$((components_ready + 1))
        
        echo "Components ready: ${components_ready}/${total_components}"
        
        if [ $components_ready -eq $total_components ]; then
            echo "✓ READY FOR CONNECTIVITY ANALYSIS"
            echo ""
            echo "Recommended processing order:"
            echo "1. 5ttgen (5-tissue-type segmentation)"
            echo "2. labelconvert (parcellation conversion)"
            echo "3. dwi2response (response function estimation)"
            echo "4. dwi2fod (fiber orientation distribution)"
            echo "5. tckgen (tractography)"
            echo "6. tcksift2 (track filtering)"
            echo "7. tck2connectome (connectome construction)"
            
            if [ "$dwi_source" = "Refined (Post-hoc)" ]; then
                echo ""
                echo "✓ Using refined DWI data for optimal results"
            fi
            
        elif [ $components_ready -ge 2 ]; then
            echo "⚠ PARTIALLY READY - Some components missing"
            echo ""
            echo "Missing components:"
            [ "${fs_ready:-false}" != true ] && echo "- FreeSurfer reconstruction"
            [ "${dwi_ready:-false}" != true ] && echo "- DWI preprocessing"
            [ "${mask_ready:-false}" != true ] && echo "- Brain mask"
            [ "${reg_ready:-false}" != true ] && echo "- Registration assessment"
            
        else
            echo "✗ NOT READY - Major components missing"
            echo ""
            echo "Critical missing components prevent connectivity analysis"
        fi
        
        # Processing time estimates
        echo ""
        echo "ESTIMATED PROCESSING TIMES:"
        echo "=========================="
        
        if [ $components_ready -eq $total_components ]; then
            echo "5ttgen:          5-15 minutes"
            echo "labelconvert:    1-2 minutes"
            echo "dwi2response:    10-20 minutes"
            echo "dwi2fod:         20-40 minutes"
            echo "tckgen:          30-90 minutes"
            echo "tcksift2:        20-60 minutes"
            echo "tck2connectome:  5-15 minutes"
            echo ""
            echo "Total estimated time: 1.5-4 hours"
            
            if [ "${GPU_AVAILABLE:-false}" = true ]; then
                echo "Note: GPU acceleration may reduce some processing times"
            fi
        else
            echo "Cannot estimate - missing required components"
        fi
        
    } > "$readiness_report"
    
    # Determine return value based on readiness
    if [ "${fs_ready:-false}" = true ] && [ "${dwi_ready:-false}" = true ] && [ "${mask_ready:-false}" = true ]; then
        log "OK" "[${sub}] Ready for connectivity analysis (${components_ready}/${total_components} components)"
        return 0
    else
        log "WARN" "[${sub}] Not ready for connectivity analysis (${components_ready}/${total_components} components)"
        return 1
    fi
}

# Generate comprehensive post-hoc summary
generate_posthoc_summary() {
    local sub=$1
    
    log "INFO" "[${sub}] Generating post-hoc processing summary"
    
    local summary_file="${EXTERNAL_QC}/${sub}_posthoc_summary.txt"
    
    {
        echo "Post-hoc Refinement Summary for ${sub}"
        echo "======================================"
        echo "Generated on: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        echo ""
        
        echo "PROCESSING STAGES COMPLETED:"
        echo "==========================="
        
        # Stage 1 status
        if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz" ]; then
            echo "✓ Stage 1: Basic preprocessing complete"
        else
            echo "✗ Stage 1: Basic preprocessing incomplete"
        fi
        
        # Stage 2 status
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            echo "✓ Stage 2: Post-hoc refinement complete"
        else
            echo "✗ Stage 2: Post-hoc refinement incomplete"
        fi
        
        # FreeSurfer status
        if check_freesurfer_outputs "$sub" "${EXTERNAL_FS}/${sub}" &>/dev/null; then
            echo "✓ FreeSurfer: Reconstruction complete"
        else
            echo "⏳ FreeSurfer: Not started or incomplete"
        fi
        
        # Connectivity readiness
        if check_connectivity_readiness "$sub" &>/dev/null; then
            echo "✓ Connectivity: Ready for analysis"
        else
            echo "⚠ Connectivity: Not ready (missing components)"
        fi
        
        echo ""
        echo "ML INTEGRATION SUMMARY:"
        echo "======================"
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
                    echo "ML Registration: ENABLED"
        echo "  Method: ${ML_REGISTRATION_METHOD:-auto}"
        echo "  GPU Available: ${GPU_AVAILABLE:-false}"
        echo "  Quick Mode: ${ML_QUICK_MODE:-true}"
        
        # Check which ML methods were actually used
        local ml_methods_used=()
        
        if [ -f "${EXTERNAL_QC}/${sub}_synthmorph_quality.txt" ]; then
            ml_methods_used+=("SynthMorph")
        fi
        
        if [ -f "${EXTERNAL_QC}/${sub}_ants_quality.txt" ]; then
            ml_methods_used+=("Enhanced-ANTs")
        fi
        
        if [ -f "${EXTERNAL_QC}/${sub}_ml_registration_report.txt" ]; then
            ml_methods_used+=("VoxelMorph")
        fi
        
        if [ ${#ml_methods_used[@]} -gt 0 ]; then
            echo "  Methods Applied: ${ml_methods_used[*]}"
        else
            echo "  Methods Applied: None (fell back to traditional)"
        fi
        
        # Registration quality summary
        local reg_qualities=()
        
        for quality_file in "${EXTERNAL_POSTHOC}/${sub}/${sub}_registration_report.txt" \
                           "${EXTERNAL_QC}/${sub}_synthmorph_quality.txt" \
                           "${EXTERNAL_QC}/${sub}_ants_quality.txt"; do
            if [ -f "$quality_file" ]; then
                local quality=$(grep -i "quality.*:" "$quality_file" | head -1 | cut -d: -f2 | xargs 2>/dev/null || echo "")
                [ -n "$quality" ] && reg_qualities+=("$quality")
            fi
        done
        
        if [ ${#reg_qualities[@]} -gt 0 ]; then
            echo "  Registration Quality: ${reg_qualities[0]}"  # Use first/best quality found
        else
            echo "  Registration Quality: Not assessed"
        fi
        
        else
            echo "ML Registration: DISABLED"
            echo "  → Traditional registration methods used"
        fi
        
        echo ""
        echo "DATA QUALITY IMPROVEMENTS:"
        echo "========================="
        
        # Compare basic vs refined if both available
        if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_fa.nii.gz" ] && [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            echo "Data refinement applied:"
            echo "  ✓ Enhanced bias correction"
            echo "  ✓ Intensity normalization"
            echo "  ✓ Enhanced brain masking"
            
            if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
                echo "  ✓ ML-based registration refinement"
            fi
            
            # SNR improvement if calculated
            local snr_improvement=$(grep "SNR improvement:" "${EXTERNAL_POSTHOC}/${sub}/qc/${sub}_refined_comprehensive_report.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "Not calculated")
            echo "  SNR improvement: $snr_improvement"
            
        else
            echo "Data refinement: Not applied or incomplete"
        fi
        
        echo ""
        echo "OUTPUT FILES SUMMARY:"
        echo "===================="
        
        # List key output files and their locations
        echo "Basic preprocessing outputs (fast storage):"
        local basic_outputs=("${sub}_dwi_preproc.nii.gz" "${sub}_mask.nii.gz" "${sub}_fa.nii.gz" "${sub}_md.nii.gz")
        for output in "${basic_outputs[@]}"; do
            if [ -f "${EXTERNAL_MRTRIX}/${sub}/$output" ]; then
                echo "  ✓ $output"
            else
                echo "  ✗ $output"
            fi
        done
        
        echo ""
        echo "Refined outputs (fast storage):"
        local refined_outputs=("${sub}_dwi_refined.nii.gz" "${sub}_mask_enhanced.nii.gz" "${sub}_biasfield.nii.gz")
        for output in "${refined_outputs[@]}"; do
            if [ -f "${EXTERNAL_POSTHOC}/${sub}/$output" ]; then
                echo "  ✓ $output"
            else
                echo "  ✗ $output"
            fi
        done
        
        echo ""
        echo "FreeSurfer outputs (large storage):"
        local fs_outputs=("mri/aparc+aseg.mgz" "surf/lh.pial" "surf/rh.pial" "scripts/recon-all.done")
        for output in "${fs_outputs[@]}"; do
            if [ -f "${EXTERNAL_FS}/${sub}/$output" ]; then
                echo "  ✓ $output"
            else
                echo "  ✗ $output"
            fi
        done
        
        echo ""
        echo "Quality control reports (fast storage):"
        local qc_outputs=("${sub}_refined_comprehensive_report.txt" "${sub}_freesurfer_quality.txt" "${sub}_connectivity_readiness.txt")
        for output in "${qc_outputs[@]}"; do
            if [ -f "${EXTERNAL_QC}/$output" ]; then
                echo "  ✓ $output"
            else
                echo "  ✗ $output"
            fi
        done
        
        echo ""
        echo "STORAGE UTILIZATION:"
        echo "=================="
        
        # Calculate storage usage per subject
        local storage_c_used="N/A"
        local storage_e_used="N/A"
        local storage_f_used="N/A"
        
        if command -v du &>/dev/null; then
            # Check BIDS drive usage (should be minimal after cleanup)
            if [ -d "${DERIV_DIR}" ]; then
                storage_c_used=$(du -sh "${DERIV_DIR}" 2>/dev/null | cut -f1 || echo "N/A")
            fi
            
            # Check fast storage usage
            if [ -d "${EXTERNAL_MRTRIX}/${sub}" ]; then
                local e_mrtrix=$(du -sh "${EXTERNAL_MRTRIX}/${sub}" 2>/dev/null | cut -f1 || echo "0")
                local e_posthoc=$(du -sh "${EXTERNAL_POSTHOC}/${sub}" 2>/dev/null | cut -f1 || echo "0")
                storage_e_used="MRtrix: $e_mrtrix, Post-hoc: $e_posthoc"
            fi
            
            # Check large storage usage
            if [ -d "${EXTERNAL_FS}/${sub}" ]; then
                storage_f_used=$(du -sh "${EXTERNAL_FS}/${sub}" 2>/dev/null | cut -f1 || echo "N/A")
            fi
        fi
        
        echo "BIDS drive (processing): $storage_c_used"
        echo "fast storage (outputs): $storage_e_used"
        echo "large storage (FreeSurfer): $storage_f_used"
        
        echo ""
        echo "PROCESSING PERFORMANCE:"
        echo "======================"
        
        # Processing time summary if available
        if [ -f "${LOG_DIR}/${sub}_progress.json" ]; then
            echo "Progress tracking:"
            $PYTHON_EXECUTABLE -c "
import json
try:
    with open('${LOG_DIR}/${sub}_progress.json', 'r') as f:
        data = json.load(f)
    
    print(f'  Last update: {data.get(\"last_update\", \"N/A\")}')
    
    stages = ['preprocessing', 'posthoc']
    for stage in stages:
        if stage in data:
            print(f'  {stage.title()}: {data[stage]}% complete')
            
except Exception as e:
    print(f'  Progress data unavailable: {e}')
" 2>/dev/null || echo "  Progress data not available"
        fi
        
        # Resource usage summary
        if [ -f "${LOG_DIR}/resource_monitor.log" ]; then
            local resource_entries=$(grep -c "$sub" "${LOG_DIR}/resource_monitor.log" 2>/dev/null || echo "0")
            echo "Resource monitoring entries: $resource_entries"
        fi
        
        echo ""
        echo "NEXT STEPS:"
        echo "=========="
        
        # Provide recommendations for next steps
        if check_connectivity_readiness "$sub" &>/dev/null; then
            echo "✓ Ready for Stage 3: Connectivity Analysis"
            echo "  → Run with --subject $sub to continue"
            echo "  → Estimated time: 1.5-4 hours"
        else
            echo "⚠ Stage 3: Connectivity Analysis blocked"
            echo "  → Review connectivity readiness report"
            echo "  → Complete missing prerequisites"
        fi
        
        echo ""
        echo "⏳ Stage 4: NODDI Estimation"
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            echo "  → Will use refined DWI data"
        else
            echo "  → Will use basic preprocessed data"
        fi
        echo "  → Estimated time: 15-45 minutes"
        
        echo ""
        echo "QUALITY RECOMMENDATIONS:"
        echo "======================="
        
        # Generate specific recommendations based on processing results
        local recommendations=()
        
        # Check if ML registration improved results
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            if [ ${#ml_methods_used[@]} -gt 0 ]; then
                recommendations+=("✓ ML registration successfully applied - enhanced accuracy expected")
            else
                recommendations+=("⚠ ML registration enabled but not used - check dependency installation")
            fi
        else
            recommendations+=("ℹ Consider enabling ML registration (--use-ml-registration) for improved accuracy")
        fi
        
        # Check data quality
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/qc/${sub}_refined_comprehensive_report.txt" ]; then
            local quality_score=$(grep "Quality score:" "${EXTERNAL_POSTHOC}/${sub}/qc/${sub}_refined_comprehensive_report.txt" 2>/dev/null | cut -d: -f2 | cut -d/ -f1 | xargs || echo "")
            
            if [ -n "$quality_score" ]; then
                if (( $(echo "$quality_score >= 80" | bc -l 2>/dev/null || echo 0) )); then
                    recommendations+=("✓ Excellent data quality - proceed with confidence")
                elif (( $(echo "$quality_score >= 65" | bc -l 2>/dev/null || echo 0) )); then
                    recommendations+=("✓ Good data quality - suitable for most analyses")
                elif (( $(echo "$quality_score >= 50" | bc -l 2>/dev/null || echo 0) )); then
                    recommendations+=("⚠ Acceptable data quality - review QC reports carefully")
                else
                    recommendations+=("⚠ Poor data quality - manual review strongly recommended")
                fi
            fi
        fi
        
        # FreeSurfer recommendations
        if check_freesurfer_outputs "$sub" "${EXTERNAL_FS}/${sub}" &>/dev/null; then
            local fs_quality=$(grep "Quality assessment:" "${EXTERNAL_QC}/${sub}_freesurfer_quality.txt" 2>/dev/null | cut -d: -f2 | xargs || echo "")
            
            case "$fs_quality" in
                "EXCELLENT"|"GOOD")
                    recommendations+=("✓ FreeSurfer reconstruction quality sufficient for connectivity")
                    ;;
                "ACCEPTABLE")
                    recommendations+=("⚠ FreeSurfer quality acceptable - verify critical structures")
                    ;;
                "POOR")
                    recommendations+=("⚠ FreeSurfer quality poor - consider manual review or reprocessing")
                    ;;
            esac
        fi
        
        # Print recommendations
        for rec in "${recommendations[@]}"; do
            echo "$rec"
        done
        
        if [ ${#recommendations[@]} -eq 0 ]; then
            echo "ℹ No specific recommendations at this time"
        fi
        
        echo ""
        echo "CONTACT INFORMATION:"
        echo "=================="
        echo "For questions about this pipeline or results:"
        echo "  → Check pipeline documentation"
        echo "  → Review QC reports in: ${EXTERNAL_QC}/"
        echo "  → Logs available in: ${LOG_DIR}/"
        
        echo ""
        echo "Report generated: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        
    } > "$summary_file"
    
    log "OK" "[${sub}] Post-hoc processing summary generated: $summary_file"
    
    # Create a brief summary for the main log
    local summary_line="Post-hoc complete"
    [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ] && summary_line+=", refined data available"
    [ "${USE_ML_REGISTRATION:-false}" = true ] && summary_line+=", ML registration applied"
    
    log "INFO" "[${sub}] $summary_line"
}

# End of Section 7
                
# --- Stage 3: Connectivity Analysis ---
run_connectivity_analysis() {
    local sub=$1
    local fs_sub_dir="${SUBJECTS_DIR}/${sub}"
    local workdir="${WORK_DIR}/${sub}/connectivity"
    
    log "STAGE" "[${sub}] Starting connectivity analysis (Stage 3)"
    monitor_resources "$sub" "connectivity_start"
    update_progress "$sub" "connectivity" 0
    
    if [ "$RUN_CONNECTOME" = false ]; then
        log "INFO" "[${sub}] Skipping connectivity analysis (--skip-connectome)"
        return 0
    fi
    
    # Check connectivity readiness
    if ! check_connectivity_readiness "$sub" &>/dev/null; then
        log "ERROR" "[${sub}] Not ready for connectivity analysis - check readiness report"
        return 1
    fi
    
    # Check if connectome already exists in external storage
    if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" ]; then
        log "INFO" "[${sub}] Connectivity analysis already completed"
        return 0
    fi
    
    # Check for T1w
    if [ ! -f "${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz" ]; then
        log "ERROR" "[${sub}] T1w anatomical image required for connectivity"
        return 1
    fi
    
    # Check disk space before starting
    if ! check_disk_space "$(dirname "$BIDS_DIR")" 50; then
        log "ERROR" "[${sub}] Insufficient disk space for connectivity analysis"
        return 1
    fi
    
    mkdir -p "$workdir"
    safe_cd "$workdir" || return 1
    
    update_progress "$sub" "connectivity" 10
    
    # Run FreeSurfer if needed
    if ! run_freesurfer "$sub"; then
        log "ERROR" "[${sub}] FreeSurfer processing failed"
        safe_cd_return
        return 1
    fi
    
    update_progress "$sub" "connectivity" 30
    
    # Prepare for tractography with ML-enhanced registration
    if ! prepare_tractography_with_ml "$sub"; then
        log "ERROR" "[${sub}] Tractography preparation failed"
        safe_cd_return
        return 1
    fi
    
    update_progress "$sub" "connectivity" 50
    
    # Run tractography and generate connectome
    if ! run_tractography_and_connectome "$sub"; then
        log "ERROR" "[${sub}] Tractography failed"
        safe_cd_return
        return 1
    fi
    
    update_progress "$sub" "connectivity" 100
    
    safe_cd_return
    cleanup_work_dir "$sub"
    monitor_resources "$sub" "connectivity_end"
    log "OK" "[${sub}] Connectivity analysis completed"
    return 0
}

run_freesurfer() {
    local sub=$1
    local fs_sub_dir="${SUBJECTS_DIR}/${sub}"
    
    # Check if already done
    if check_freesurfer_outputs "$sub" "$fs_sub_dir"; then
        # If in external storage, we'll use it from there
        if [ -d "${EXTERNAL_FS}/${sub}" ]; then
            export FS_OUTPUTS_DIR="${EXTERNAL_FS}/${sub}"
        else
            export FS_OUTPUTS_DIR="$fs_sub_dir"
        fi
        log "INFO" "[${sub}] Using existing FreeSurfer outputs from: $FS_OUTPUTS_DIR"
        return 0
    fi
    
    # Check if FreeSurfer environment is properly set
    if [ -z "$FREESURFER_HOME" ]; then
        log "ERROR" "[${sub}] FREESURFER_HOME not set"
        return 1
    fi
    
    if [ -z "$FREESURFER_LICENSE" ] || [ ! -f "$FREESURFER_LICENSE" ]; then
        log "ERROR" "[${sub}] FreeSurfer license not found"
        return 1
    fi
    
    # Check available memory and adjust processing if needed
    local available_gb=$(free -g | awk '/^Mem:/{print $7}')
    local fs_threads=$OMP_NUM_THREADS
    
    if [ "$available_gb" -lt 8 ]; then
        log "WARN" "[${sub}] Low memory: ${available_gb}GB available, 8GB recommended"
        # Reduce parallelization for low memory systems
        fs_threads=$((OMP_NUM_THREADS / 2))
        [ $fs_threads -lt 1 ] && fs_threads=1
    elif [ "$available_gb" -lt 16 ]; then
        log "INFO" "[${sub}] Moderate memory: ${available_gb}GB available"
        fs_threads=$((OMP_NUM_THREADS * 3 / 4))
    fi
    
    # Adjust for ML registration if active (leave resources for GPU)
    if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${GPU_AVAILABLE:-false}" = true ]; then
        fs_threads=$((fs_threads - 2))
        [ $fs_threads -lt 1 ] && fs_threads=1
        log "INFO" "[${sub}] Reduced FreeSurfer threads to $fs_threads (ML registration active)"
    fi
    
    log "INFO" "[${sub}] Running FreeSurfer recon-all (6-12 hours)"
    log "INFO" "[${sub}] Using ${fs_threads} threads (${available_gb}GB memory available)"
    log "INFO" "[${sub}] Progress will be logged to: ${LOG_DIR}/${sub}_recon-all.log"
    
    # Enhanced T1w preprocessing for better FreeSurfer results
    local t1_input="${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz"
    local t1_processed="$t1_input"
    
    # Apply ML-enhanced T1w preprocessing if available
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        if enhance_t1w_for_freesurfer "$sub"; then
            t1_processed="${workdir}/T1w_enhanced.nii.gz"
            log "ML" "[${sub}] Using ML-enhanced T1w for FreeSurfer"
        else
            log "INFO" "[${sub}] ML T1w enhancement failed, using original"
        fi
    fi
    
    # Record start time
    local start_time=$(date +%s)
    
    # Run recon-all with enhanced error handling
    local recon_success=false
    local recon_attempts=0
    local max_attempts=2
    
    while [ $recon_attempts -lt $max_attempts ] && [ "$recon_success" = false ]; do
        recon_attempts=$((recon_attempts + 1))
        
        log "INFO" "[${sub}] FreeSurfer attempt ${recon_attempts}/${max_attempts}"
        
        # Run recon-all
        recon-all -i "$t1_processed" \
                  -s "$sub" \
                  -all \
                  -parallel \
                  -openmp ${fs_threads} \
                  &> "${LOG_DIR}/${sub}_recon-all_attempt${recon_attempts}.log"
        
        local exit_status=$?
        
        if [ $exit_status -eq 0 ]; then
            recon_success=true
            log "OK" "[${sub}] FreeSurfer completed successfully on attempt ${recon_attempts}"
        else
            log "WARN" "[${sub}] FreeSurfer attempt ${recon_attempts} failed with exit code ${exit_status}"
            
            # Analyze failure and potentially retry with different parameters
            if [ $recon_attempts -lt $max_attempts ]; then
                analyze_freesurfer_failure "$sub" $recon_attempts
                
                # Clean up for retry
                [ -d "$fs_sub_dir" ] && rm -rf "$fs_sub_dir"
                
                # Adjust parameters for retry
                fs_threads=$((fs_threads / 2))
                [ $fs_threads -lt 1 ] && fs_threads=1
                
                log "INFO" "[${sub}] Retrying with reduced threads: $fs_threads"
                sleep 10  # Brief pause before retry
            fi
        fi
    done
    
    if [ "$recon_success" = false ]; then
        log "ERROR" "[${sub}] FreeSurfer failed after $max_attempts attempts"
        
        # Provide detailed failure analysis
        if [ -f "${LOG_DIR}/${sub}_recon-all_attempt${recon_attempts}.log" ]; then
            log "ERROR" "[${sub}] Last 20 lines of FreeSurfer log:"
            tail -n 20 "${LOG_DIR}/${sub}_recon-all_attempt${recon_attempts}.log" >&2
            
            # Check for common errors
            analyze_freesurfer_failure "$sub" $recon_attempts "final"
        fi
        
        return 1
    fi
    
    # Verify outputs
    local required_files=(
        "mri/aparc+aseg.mgz"
        "surf/lh.pial"
        "surf/rh.pial"
        "scripts/recon-all.done"
    )
    
    local missing_files=0
    for file in "${required_files[@]}"; do
        if [ ! -f "${fs_sub_dir}/${file}" ]; then
            log "ERROR" "[${sub}] Missing required FreeSurfer output: ${file}"
            ((missing_files++))
        fi
    done
    
    if [ $missing_files -gt 0 ]; then
        log "ERROR" "[${sub}] FreeSurfer outputs incomplete ($missing_files missing files)"
        return 1
    fi
    
    # Calculate processing time
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    local hours=$((duration / 3600))
    local minutes=$(( (duration % 3600) / 60 ))
    
    log "OK" "[${sub}] FreeSurfer completed in ${hours}h ${minutes}m"
    
    # Post-processing quality assessment
    validate_freesurfer_quality "$sub" "$fs_sub_dir"
    
    # Move to external storage immediately
    log "INFO" "[${sub}] Moving FreeSurfer outputs to external storage"
    mkdir -p "${EXTERNAL_FS}"
    
    # Check space on target drive
    local required_space_gb=5
    if ! check_disk_space "$STORAGE_LARGE" $required_space_gb; then
        log "ERROR" "[${sub}] Insufficient space on large storage for FreeSurfer outputs"
        return 1
    fi
    
    # Use rsync to move large FreeSurfer directory with progress monitoring
    log "INFO" "[${sub}] Transferring FreeSurfer data (~5GB) to external storage..."
    
    if retry_operation rsync -av --remove-source-files "$fs_sub_dir/" "${EXTERNAL_FS}/${sub}/"; then
        # Remove empty directories
        find "$fs_sub_dir" -type d -empty -delete 2>/dev/null || true
        rmdir "$fs_sub_dir" 2>/dev/null || true
        export FS_OUTPUTS_DIR="${EXTERNAL_FS}/${sub}"
        log "OK" "[${sub}] FreeSurfer outputs moved to external storage"
        create_checkpoint "$sub" "freesurfer_complete"
    else
        log "ERROR" "[${sub}] Failed to move FreeSurfer outputs"
        return 1
    fi
    
    return 0
}

enhance_t1w_for_freesurfer() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/connectivity"

    log "ML" "[${sub}] Applying ML-based T1w enhancement for FreeSurfer"

    # Ensure working directory exists
    mkdir -p "$workdir" || { log "WARN" "[${sub}] Could not create workdir: $workdir"; return 1; }
    safe_cd "$workdir" || { log "ERROR" "[${sub}] Cannot cd to workdir: $workdir"; return 1; }

    # Copy original T1w (use a local copy to avoid modifying source)
    if [ ! -f "${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz" ]; then
        log "ERROR" "[${sub}] T1w not found: ${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz"
        return 1
    fi
    cp "${BIDS_DIR}/${sub}/anat/${sub}_T1w.nii.gz" T1w_original.nii.gz

    # Run enhanced N4 bias correction (robust, but non-fatal)
    if command -v N4BiasFieldCorrection &>/dev/null; then
        N4BiasFieldCorrection -d 3 \
            -i T1w_original.nii.gz \
            -o T1w_n4_enhanced.nii.gz \
            -c '[100x100x100x100,0.0]' \
            -s 2 \
            -b '[200,3]' \
            &> n4_enhanced.log || {
            log "WARN" "[${sub}] Enhanced N4 failed; falling back to the original T1"
            cp -f T1w_original.nii.gz T1w_n4_enhanced.nii.gz ;
        }
    else
        log "WARN" "[${sub}] N4BiasFieldCorrection not available; skipping enhanced N4"
        cp -f T1w_original.nii.gz T1w_n4_enhanced.nii.gz
    fi

# ML-based intensity normalization + optional contrast enhancement in Python
{
    $PYTHON_EXECUTABLE <<'PY'
import nibabel as nib
import numpy as np
from scipy import ndimage
import sys

def ml_intensity_normalization(img_data):
    """ML-inspired intensity normalization for better FreeSurfer performance"""
    try:
        data_pos = img_data[img_data > 0]
        if data_pos.size == 0:
            return img_data.astype(np.float32)
        p1, p99 = np.percentile(data_pos, [1, 99])
        img_clipped = np.clip(img_data, p1, p99)
        hist, bins = np.histogram(img_clipped[img_clipped > 0], bins=100)
        # pick a brain tissue peak ignoring background peak
        peak_idx = np.argmax(hist[10:]) + 10 if hist.size > 10 else np.argmax(hist)
        brain_peak_intensity = (bins[min(peak_idx, len(bins)-2)] + bins[min(peak_idx+1, len(bins)-1)]) / 2.0
        target_intensity = 110.0
        if brain_peak_intensity <= 0:
            scale_factor = 1.0
        else:
            scale_factor = target_intensity / brain_peak_intensity
        img_normalized = img_clipped * scale_factor
        img_normalized = np.clip(img_normalized, 0, 255)
        return img_normalized.astype(np.float32)
    except Exception as e:
        print(f"ML normalization failed: {e}", file=sys.stderr)
        return img_data.astype(np.float32)

def enhance_contrast(img_data):
    """Enhance contrast for better tissue differentiation (optional, conservative)"""
    try:
        from scipy.ndimage import uniform_filter
        # Compute local mean and std in a moderate-sized neighborhood
        footprint = 9
        local_mean = uniform_filter(img_data.astype(np.float32), size=footprint)
        local_mean_sq = uniform_filter((img_data.astype(np.float32))**2, size=footprint)
        local_var = np.maximum(local_mean_sq - local_mean**2, 1e-6)
        local_std = np.sqrt(local_var)
        enhanced = (img_data - local_mean) / (local_std + 1e-6)
        # Scale back to 0-255
        enhanced = enhanced - enhanced.min()
        denom = enhanced.max() if enhanced.max() != 0 else 1.0
        enhanced = enhanced / denom * 255.0
        return enhanced.astype(np.float32)
    except Exception as e:
        print(f"Contrast enhancement failed: {e}", file=sys.stderr)
        return img_data.astype(np.float32)

try:
    img = nib.load('T1w_n4_enhanced.nii.gz')
    img_data = img.get_fdata()
    normalized = ml_intensity_normalization(img_data)
    # Optional: keep contrast step conservative (commented by default)
    # enhanced = enhance_contrast(normalized)
    enhanced_img = nib.Nifti1Image(normalized.astype(np.float32), img.affine, img.header)
    nib.save(enhanced_img, 'T1w_enhanced.nii.gz')
    print('ML T1w enhancement completed')
except Exception as e:
    print(f"ML T1w enhancement failed: {e}", file=sys.stderr)
    sys.exit(1)
PY
} > enhance_t1_ml.log 2>&1 || {
    log "WARN" "[${sub}] ML T1 enhancement script failed; see enhance_t1_ml.log"
    # Keep fallback file
}

    # If Python saved enhanced image, keep it; otherwise use N4 output
    if [ -f "T1w_enhanced.nii.gz" ]; then
        log "OK" "[${sub}] ML T1w enhancement completed (T1w_enhanced.nii.gz)"
        safe_cd_return
        return 0
    else
        log "WARN" "[${sub}] ML enhancement failed, using N4 result"
        safe_cd_return
        return 1
    fi
}

# Consolidated FLIRT/BBR matrix validator used by registration flows
validate_registration_matrix() {
    local matrix_file=$1

    if [ ! -f "$matrix_file" ]; then
        log "WARN" "Registration matrix not found: $matrix_file"
        return 1
    fi

    $PYTHON_EXECUTABLE - <<PY 2>/dev/null || return 1
import numpy as np, sys
try:
    matrix = np.loadtxt('${matrix_file}')
    if matrix.shape != (4,4) and matrix.shape != (3,4):
        print('Matrix shape unexpected:', matrix.shape)
        sys.exit(1)
    # If 3x4, build 4x4
    if matrix.shape == (3,4):
        mat4 = np.vstack([matrix, [0,0,0,1]])
    else:
        mat4 = matrix
    rot = mat4[:3,:3]
    trans = mat4[:3,3]
    det = np.linalg.det(rot)
    trans_mag = float(np.linalg.norm(trans))
    if not (0.8 <= abs(det) <= 1.2):
        print(f"det_fail {det:.3f}")
        sys.exit(1)
    if trans_mag > 1000:  # extremely large translation is suspect (mm)
        print(f"trans_fail {trans_mag:.1f}")
        sys.exit(1)
    print(f"Matrix OK: det={det:.3f}, trans={trans_mag:.1f}")
    sys.exit(0)
except Exception as e:
    print('Matrix validation error:', e)
    sys.exit(1)
PY

    return $?
}

# Consolidated BBR/traditional registration implementation (single canonical copy)
run_traditional_bbr_registration() {
    local sub=$1
    log "INFO" "[${sub}] Running traditional BBR registration"

    # Work in current directory (caller should cd into workdir)
    # Expect: b0_mean.nii.gz and T1_n4.nii.gz present

    if [ ! -f "b0_mean.nii.gz" ]; then
        if [ -f "${WORK_DIR}/${sub}/preproc/b0_mean.nii.gz" ]; then
            cp "${WORK_DIR}/${sub}/preproc/b0_mean.nii.gz" .
        else
            log "ERROR" "[${sub}] b0_mean.nii.gz not found for BBR"
            return 1
        fi
    fi

    if [ ! -f "T1_n4.nii.gz" ]; then
        log "ERROR" "[${sub}] T1_n4.nii.gz not found for BBR"
        return 1
    fi

    # Ensure brain and wm segmentation present
    if [ ! -f "T1_brain.nii.gz" ]; then
        if command -v bet2 &>/dev/null; then
            bet2 T1_n4.nii.gz T1_brain -m -f 0.5 -g 0 &>/dev/null || {
                log "WARN" "[${sub}] BET failed on T1_n4; attempting more conservative BET"
                bet2 T1_n4.nii.gz T1_brain -m -f 0.3 -g 0 &>/dev/null || true
            }
        fi
    fi

    # Run FAST to obtain WM segmentation
    if command -v fast &>/dev/null; then
        fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 -o T1_brain T1_brain.nii.gz &> fast_log.txt || true
    else
        log "WARN" "[${sub}] FSL fast not available; continuing with simple linear registration fallback"
    fi

    # Prefer BBR (epi_reg) if wm segmentation edge exists
    if [ -f "T1_brain_pve_2.nii.gz" ] && command -v epi_reg &>/dev/null; then
        # Create a WM edge
        fslmaths T1_brain_pve_2.nii.gz -thr 0.5 -bin T1_wm_bin 2>/dev/null || true
        fslmaths T1_wm_bin -edge T1_wm_edge_raw 2>/dev/null || true
        if [ -f "T1_brain_mask.nii.gz" ]; then
            fslmaths T1_wm_edge_raw -mas T1_brain_mask.nii.gz T1_wm_edge 2>/dev/null || true
        else
            cp -f T1_wm_edge_raw T1_wm_edge 2>/dev/null || true
        fi

        # Attempt epi_reg (BBR)
        if epi_reg --epi=b0_mean.nii.gz --t1=T1_n4.nii.gz --t1brain=T1_brain.nii.gz --wmseg=T1_wm_edge.nii.gz --out=b0_to_T1_bbr &> bbr_log.txt; then
            if [ -f "b0_to_T1_bbr.mat" ]; then
                if validate_registration_matrix "b0_to_T1_bbr.mat"; then
                    convert_xfm -omat T1_to_b0_bbr.mat -inverse b0_to_T1_bbr.mat 2>/dev/null || true
                    flirt -in T1_brain.nii.gz -ref b0_mean.nii.gz -applyxfm -init T1_to_b0_bbr.mat -out T1_in_b0_space.nii.gz -interp spline 2>/dev/null || true
                    if [ -f "T1_in_b0_space.nii.gz" ]; then
                        log "OK" "[${sub}] BBR registration completed"
                        return 0
                    fi
                else
                    log "WARN" "[${sub}] BBR produced an implausible matrix; falling back"
                fi
            else
                log "WARN" "[${sub}] epi_reg did not produce a matrix file"
            fi
        else
            log "WARN" "[${sub}] epi_reg (BBR) failed - see bbr_log.txt"
        fi
    fi

    # Fallback: linear FLIRT
    if command -v flirt &>/dev/null; then
        if flirt -in T1_brain.nii.gz -ref b0_mean.nii.gz -out T1_in_b0_space.nii.gz -omat T1_to_b0_linear.mat -dof 6 -searchrx -180 180 -searchry -180 180 -searchrz -180 180 &> linear_reg_log.txt; then
            log "OK" "[${sub}] Linear registration completed as fallback"
            return 0
        else
            log "ERROR" "[${sub}] Linear registration failed - see linear_reg_log.txt"
            return 1
        fi
    else
        log "ERROR" "[${sub}] flirt not available for fallback registration"
        return 1
    fi
}

analyze_freesurfer_failure() {
    local sub=$1
    local attempt=$2
    local analysis_type=${3:-"retry"}
    
    local log_file="${LOG_DIR}/${sub}_recon-all_attempt${attempt}.log"
    
    if [ ! -f "$log_file" ]; then
        return 1
    fi
    
    log "INFO" "[${sub}] Analyzing FreeSurfer failure (attempt $attempt)"
    
    # Common FreeSurfer error patterns
    local error_patterns=(
        "cannot allocate memory|out of memory"
        "No such file or directory"
        "Talairach registration failed"
        "skull strip failed"
        "Surface tessellation failed"
        "white matter segmentation failed"
    )
    
    local error_messages=(
        "Memory exhaustion"
        "Missing files"
        "Talairach registration error"
        "Skull stripping error"
        "Surface generation error"
        "Segmentation error"
    )
    
    # Analyze log for specific errors
    for i in "${!error_patterns[@]}"; do
        if grep -qi "${error_patterns[$i]}" "$log_file"; then
            log "WARN" "[${sub}] Detected: ${error_messages[$i]}"
            
            case $i in
                0) # Memory error
                    log "INFO" "[${sub}] Suggestion: Reduce parallel processing or increase swap"
                    ;;
                1) # Missing files
                    log "INFO" "[${sub}] Suggestion: Check input data integrity"
                    ;;
                2) # Talairach error
                    log "INFO" "[${sub}] Suggestion: T1w quality issue, consider manual review"
                    ;;
                3) # Skull strip error
                    log "INFO" "[${sub}] Suggestion: T1w contrast issue, try enhanced preprocessing"
                    ;;
                4|5) # Surface/segmentation errors
                    log "INFO" "[${sub}] Suggestion: T1w quality or resolution issue"
                    ;;
            esac
        fi
    done
    
    # Generate failure report for final analysis
    if [ "$analysis_type" = "final" ]; then
        local failure_report="${EXTERNAL_QC}/${sub}_freesurfer_failure_analysis.txt"
        
        {
            echo "FreeSurfer Failure Analysis for ${sub}"
            echo "======================================"
            echo "Generated on: $(date)"
            echo "Total attempts: $attempt"
            echo ""
            echo "Error Analysis:"
            
            for i in "${!error_patterns[@]}"; do
                if grep -qi "${error_patterns[$i]}" "$log_file"; then
                    echo "- ${error_messages[$i]}: DETECTED"
                fi
            done
            
            echo ""
            echo "Last 50 lines of log:"
            tail -n 50 "$log_file"
            
        } > "$failure_report"
        
        log "INFO" "[${sub}] FreeSurfer failure analysis saved: $failure_report"
    fi
}

prepare_tractography_with_ml() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/connectivity"
    
    log "INFO" "[${sub}] Preparing anatomical data for tractography with ML enhancements"
    monitor_resources "$sub" "tractography_prep_start"
    
    # Verify FS_OUTPUTS_DIR is set
    if [ -z "$FS_OUTPUTS_DIR" ]; then
        log "ERROR" "[${sub}] FS_OUTPUTS_DIR not set"
        return 1
    fi
    
    # Get DWI data (prefer refined over basic)
    local dwi_file=""
    local bvec=""
    local bval=""
    local mask=""
    local dwi_source=""
    
    if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
        log "INFO" "[${sub}] Using refined DWI data for tractography"
        dwi_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz"
        bvec="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bvec"
        bval="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bval"
        mask="${EXTERNAL_POSTHOC}/${sub}/${sub}_mask_enhanced.nii.gz"
        dwi_source="refined"
    else
        log "INFO" "[${sub}] Using standard preprocessed data for tractography"
        dwi_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz"
        bvec="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi.bvec"
        bval="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi.bval"
        mask="${EXTERNAL_MRTRIX}/${sub}/${sub}_mask.nii.gz"
        dwi_source="basic"
    fi
    
    # Check files exist
    for f in "$dwi_file" "$bvec" "$bval" "$mask"; do
        if [ ! -f "$f" ]; then
            log "ERROR" "[${sub}] Required file not found: $f"
            return 1
        fi
    done
    
    # Check FreeSurfer parcellation exists
    if [ ! -f "${FS_OUTPUTS_DIR}/mri/aparc+aseg.mgz" ]; then
        log "ERROR" "[${sub}] FreeSurfer parcellation not found: ${FS_OUTPUTS_DIR}/mri/aparc+aseg.mgz"
        return 1
    fi
    
    # Copy DWI data locally for processing
    log "INFO" "[${sub}] Copying DWI data for tractography preparation"
    cp "$dwi_file" dwi_preproc.nii.gz || return 1
    cp "$bvec" dwi.bvec || return 1
    cp "$bval" dwi.bval || return 1
    cp "$mask" mask.nii.gz || return 1
    
    # ML-Enhanced T1w-DWI Registration for Tractography
    local registration_transform=""
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "ML" "[${sub}] Applying ML-enhanced T1w-DWI registration for tractography"
        
        if perform_ml_t1w_dwi_registration "$sub"; then
            registration_transform="ml_transform"
            log "OK" "[${sub}] ML registration successful for tractography"
        else
            log "WARN" "[${sub}] ML registration failed, using identity transform"
        fi
    fi
    
    # Generate 5TT image from FreeSurfer with enhanced processing
    log "INFO" "[${sub}] Generating 5-tissue-type image"
    
    # Use enhanced 5ttgen if ML registration provided good alignment
    if [ -n "$registration_transform" ]; then
        # Apply registration to improve 5TT generation
        5ttgen freesurfer "${FS_OUTPUTS_DIR}/mri/aparc+aseg.mgz" 5tt_temp.mif -nocrop -quiet
        
        # Transform 5TT to DWI space using ML registration
        if apply_ml_transform_to_5tt "$sub" "$registration_transform"; then
            log "ML" "[${sub}] 5TT transformed to DWI space using ML registration"
        else
            log "WARN" "[${sub}] ML transform failed, using standard 5TT"
            mv 5tt_temp.mif 5tt.mif
        fi
    else
        5ttgen freesurfer "${FS_OUTPUTS_DIR}/mri/aparc+aseg.mgz" 5tt.mif -nocrop -quiet
    fi
    
    if [ ! -f "5tt.mif" ]; then
        log "ERROR" "[${sub}] 5ttgen failed"
        return 1
    fi
    
    # Verify MRtrix3 home for labelconvert
    if [ -z "$MRTRIX_HOME" ] || [ ! -f "${MRTRIX_HOME}/share/mrtrix3/labelconvert/fs_default.txt" ]; then
        log "ERROR" "[${sub}] Cannot find MRtrix3 labelconvert files"
        return 1
    fi
    
    # Create parcellation with enhanced processing
    log "INFO" "[${sub}] Converting FreeSurfer parcellation"
    
    # Enhanced labelconvert with better error handling
    if ! labelconvert "${FS_OUTPUTS_DIR}/mri/aparc+aseg.mgz" \
        "${FREESURFER_HOME}/FreeSurferColorLUT.txt" \
        "${MRTRIX_HOME}/share/mrtrix3/labelconvert/fs_default.txt" \
        nodes_temp.mif -quiet; then
        log "ERROR" "[${sub}] labelconvert failed"
        return 1
    fi
    
    # Apply ML registration to parcellation if available
    if [ -n "$registration_transform" ]; then
        if apply_ml_transform_to_parcellation "$sub" "$registration_transform"; then
            log "ML" "[${sub}] Parcellation transformed to DWI space using ML registration"
        else
            log "WARN" "[${sub}] ML transform of parcellation failed, using standard"
            mv nodes_temp.mif nodes.mif
        fi
    else
        mv nodes_temp.mif nodes.mif
    fi
    
    # Create GM-WM interface for seeding with enhanced parameters
    log "INFO" "[${sub}] Creating GM-WM interface mask"
    if ! 5tt2gmwmi 5tt.mif gmwm_seed.mif -quiet; then
        log "ERROR" "[${sub}] 5tt2gmwmi failed"
        return 1
    fi
    
    # Enhanced seed validation and optimization
    validate_and_optimize_seeds "$sub" "$dwi_source"
    
    log "OK" "[${sub}] Tractography preparation completed with ML enhancements"
    monitor_resources "$sub" "tractography_prep_end"
    return 0
}

perform_ml_t1w_dwi_registration() {
    local sub=$1
    
    log "ML" "[${sub}] Performing ML-enhanced T1w-DWI registration for tractography"
    
    # Extract b0 from DWI for registration
    mrconvert dwi_preproc.nii.gz -fslgrad dwi.bvec dwi.bval dwi_temp.mif -quiet -force
    dwiextract dwi_temp.mif - -bzero -quiet | mrmath - mean b0_for_reg.nii.gz -axis 3 -quiet -force
    
    # Get T1w brain from FreeSurfer
    if [ -f "${FS_OUTPUTS_DIR}/mri/brain.mgz" ]; then
        mri_convert "${FS_OUTPUTS_DIR}/mri/brain.mgz" T1_brain_for_reg.nii.gz
    else
        log "ERROR" "[${sub}] FreeSurfer brain not found"
        return 1
    fi
    
    local ml_success=false
    
    # Try ML registration methods in order of preference
    if [ "${SYNTHMORPH_AVAILABLE:-false}" = true ]; then
        log "ML" "[${sub}] Attempting SynthMorph registration"
        
        if run_synthmorph_registration "$sub" \
            "b0_for_reg.nii.gz" \
            "T1_brain_for_reg.nii.gz" \
            "synthmorph_t1_to_dwi"; then
            
            # Extract transformation matrix
            if extract_synthmorph_transform "$sub"; then
                ml_success=true
                echo "synthmorph" > ml_transform_method.txt
                log "OK" "[${sub}] SynthMorph registration successful"
            fi
        fi
    fi
    
    if [ "$ml_success" = false ] && [ "${ML_REGISTRATION_AVAILABLE:-false}" = true ]; then
        log "ML" "[${sub}] Attempting enhanced ANTs registration"
        
        if run_ants_with_ml_features "$sub" \
            "b0_for_reg.nii.gz" \
            "T1_brain_for_reg.nii.gz" \
            "ants_t1_to_dwi" \
            "rigid+affine"; then
            
            ml_success=true
            echo "ants" > ml_transform_method.txt
            log "OK" "[${sub}] Enhanced ANTs registration successful"
        fi
    fi
    
    # Clean up temporary files
    rm -f dwi_temp.mif b0_for_reg.nii.gz T1_brain_for_reg.nii.gz
    
    return $([ "$ml_success" = true ] && echo 0 || echo 1)
}

extract_synthmorph_transform() {
    local sub=$1
    
    # SynthMorph produces displacement fields, convert to usable transform
    if [ -f "synthmorph_t1_to_dwi_transform.mgz" ]; then
        # Convert displacement field to transformation matrix (simplified)
        # This is a placeholder - real implementation would need proper conversion
        mri_convert synthmorph_t1_to_dwi_transform.mgz synthmorph_transform.nii.gz
        echo "synthmorph_transform.nii.gz" > ml_transform_file.txt
        return 0
    else
        return 1
    fi
}

apply_ml_transform_to_5tt() {
    local sub=$1
    local transform_type=$2
    
    log "ML" "[${sub}] Applying ML transform to 5TT image"
    
    # Get transform method
    local method=$(cat ml_transform_method.txt 2>/dev/null || echo "unknown")
    
    case $method in
        "synthmorph")
            if [ -f "synthmorph_transform.nii.gz" ]; then
                # Apply SynthMorph transform to 5TT
                # Note: This is simplified - real implementation needs proper warp application
                log "ML" "[${sub}] Applying SynthMorph transform to 5TT"
                
                # Convert 5TT to NIfTI for transformation
                mrconvert 5tt_temp.mif 5tt_temp.nii.gz -quiet -force
                
                # Apply transformation (placeholder - needs actual warp application)
                cp 5tt_temp.nii.gz 5tt_transformed.nii.gz
                
                # Convert back to MIF
                mrconvert 5tt_transformed.nii.gz 5tt.mif -quiet -force
                
                rm -f 5tt_temp.nii.gz 5tt_transformed.nii.gz
                return 0
            fi
            ;;
            
        "ants")
            if [ -f "ants_t1_to_dwi_1Warp.nii.gz" ] && [ -f "ants_t1_to_dwi_0GenericAffine.mat" ]; then
                log "ML" "[${sub}] Applying ANTs transform to 5TT"
                
                # Convert 5TT to NIfTI
                mrconvert 5tt_temp.mif 5tt_temp.nii.gz -quiet -force
                
                # Apply ANTs transformation
                antsApplyTransforms -d 3 \
                    -i 5tt_temp.nii.gz \
                    -r dwi_preproc.nii.gz \
                    -t ants_t1_to_dwi_1Warp.nii.gz \
                    -t ants_t1_to_dwi_0GenericAffine.mat \
                    -o 5tt_transformed.nii.gz \
                    -n Linear
                
                if [ -f "5tt_transformed.nii.gz" ]; then
                    # Convert back to MIF
                    mrconvert 5tt_transformed.nii.gz 5tt.mif -quiet -force
                    rm -f 5tt_temp.nii.gz 5tt_transformed.nii.gz
                    return 0
                fi
            fi
            ;;
    esac
    
    log "WARN" "[${sub}] ML transform application failed"
    return 1
}

apply_ml_transform_to_parcellation() {
    local sub=$1
    local transform_type=$2
    
    log "ML" "[${sub}] Applying ML transform to parcellation"
    
    local method=$(cat ml_transform_method.txt 2>/dev/null || echo "unknown")
    
    case $method in
        "ants")
            if [ -f "ants_t1_to_dwi_1Warp.nii.gz" ] && [ -f "ants_t1_to_dwi_0GenericAffine.mat" ]; then
                # Convert nodes to NIfTI
                mrconvert nodes_temp.mif nodes_temp.nii.gz -quiet -force
                
                # Apply ANTs transformation with nearest neighbor interpolation
                antsApplyTransforms -d 3 \
                    -i nodes_temp.nii.gz \
                    -r dwi_preproc.nii.gz \
                    -t ants_t1_to_dwi_1Warp.nii.gz \
                    -t ants_t1_to_dwi_0GenericAffine.mat \
                    -o nodes_transformed.nii.gz \
                    -n NearestNeighbor
                
                if [ -f "nodes_transformed.nii.gz" ]; then
                    # Convert back to MIF
                    mrconvert nodes_transformed.nii.gz nodes.mif -quiet -force
                    rm -f nodes_temp.nii.gz nodes_transformed.nii.gz
                    return 0
                fi
            fi
            ;;
            
        "synthmorph")
            # Similar process for SynthMorph (simplified)
            log "ML" "[${sub}] SynthMorph parcellation transform (simplified)"
            mv nodes_temp.mif nodes.mif
            return 0
            ;;
    esac
    
    return 1
}

validate_and_optimize_seeds() {
    local sub=$1
    local dwi_source=$2
    
    log "INFO" "[${sub}] Validating and optimizing seeding regions"
    
    # Validate GM-WM interface
    local gmwm_volume=$(mrstats gmwm_seed.mif -output count 2>/dev/null || echo "0")
    
    if [ "$gmwm_volume" -lt 1000 ]; then
        log "WARN" "[${sub}] Small GM-WM interface ($gmwm_volume voxels) - may affect tractography"
        
        # Try to improve seeding by dilating the interface
        maskfilter gmwm_seed.mif dilate gmwm_seed_dilated.mif -npass 1 -quiet -force
        
        local dilated_volume=$(mrstats gmwm_seed_dilated.mif -output count 2>/dev/null || echo "0")
        
        if [ "$dilated_volume" -gt "$gmwm_volume" ]; then
            mv gmwm_seed_dilated.mif gmwm_seed.mif
            log "INFO" "[${sub}] Improved seeding interface: $dilated_volume voxels"
        else
            rm -f gmwm_seed_dilated.mif
        fi
    else
        log "OK" "[${sub}] GM-WM interface adequate: $gmwm_volume voxels"
    fi
    
    # Create additional seeding strategy for refined data
    if [ "$dwi_source" = "refined" ]; then
        log "INFO" "[${sub}] Creating enhanced seeding strategy for refined data"
        
        # Create white matter mask for additional seeding
        mrconvert 5tt.mif - -coord 3 2 -quiet | mrmath - mean wm_mask.mif -axis 3 -quiet -force
        mrthreshold wm_mask.mif wm_seed.mif -abs 0.5 -quiet -force
        
        # Combine GM-WM interface with WM mask
        mrcalc gmwm_seed.mif wm_seed.mif -add -quiet | mrthreshold - gmwm_enhanced.mif -abs 0.5 -quiet -force
        
        local enhanced_volume=$(mrstats gmwm_enhanced.mif -output count 2>/dev/null || echo "0")
        
        if [ "$enhanced_volume" -gt "$gmwm_volume" ]; then
            mv gmwm_enhanced.mif gmwm_seed.mif
            log "OK" "[${sub}] Enhanced seeding for refined data: $enhanced_volume voxels"
        fi
        
        rm -f wm_mask.mif wm_seed.mif gmwm_enhanced.mif
    fi
}

# Continuation of tractography analysis
run_tractography_and_connectome() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/connectivity"
    
    log "INFO" "[${sub}] Running CSD and tractography with ML optimizations"
    monitor_resources "$sub" "tractography_start"
    
    # Convert DWI to MIF
    mrconvert dwi_preproc.nii.gz dwi.mif \
        -fslgrad dwi.bvec dwi.bval \
        -datatype float32 -quiet -force
    
    mrconvert mask.nii.gz mask.mif -datatype bit -quiet -force
    
    # Enhanced response function estimation with ML insights
    log "INFO" "[${sub}] Estimating tissue response functions with enhanced methods"
    
    # Detect if data is single-shell or multi-shell
    local unique_bvals=$(awk '{for(i=1;i<=NF;i++) if($i>50) print int($i/100)*100}' dwi.bval | \
                        sort -nu | wc -l)
    
    local num_volumes=$(mrstats dwi.mif -quiet -output count | tail -1)
    
    log "INFO" "[${sub}] DWI characteristics: $num_volumes volumes, $unique_bvals unique b-values"
    
    # ML-informed response function estimation
    local response_success=false
    
    if [ "$unique_bvals" -eq 1 ]; then
        # Single-shell data - use optimized single-shell approach
        log "INFO" "[${sub}] Single-shell data detected - using optimized approach"
        
        # Try different response estimation methods in order of preference
        local methods=("tournier" "fa" "manual")
        
        for method in "${methods[@]}"; do
            log "INFO" "[${sub}] Trying $method response function estimation"
            
            case $method in
                "tournier")
                    if dwi2response tournier dwi.mif wm.txt -mask mask.mif -quiet -force 2>/dev/null; then
                        response_success=true
                        break
                    fi
                    ;;
                "fa")
                    if dwi2response fa dwi.mif wm.txt -mask mask.mif -quiet -force 2>/dev/null; then
                        response_success=true
                        break
                    fi
                    ;;
                "manual")
                    # Create manual response function based on typical values
                    echo "0.300 0.000 0.000" > wm.txt
                    response_success=true
                    log "WARN" "[${sub}] Using manual response function"
                    break
                    ;;
            esac
        done
        
        if [ "$response_success" = true ]; then
            # Validate response function
            local wm_lines=$(wc -l < wm.txt 2>/dev/null || echo "0")
            if [ "$wm_lines" -lt 1 ]; then
                log "ERROR" "[${sub}] Invalid response function file"
                return 1
            fi
            
            # FOD computation for single-shell with adaptive lmax
            log "INFO" "[${sub}] Computing FODs using optimized single-shell CSD"
            
            # Determine optimal lmax based on number of directions
            local lmax=8
            if [ "$num_volumes" -lt 45 ]; then
                lmax=6
                log "INFO" "[${sub}] Using lmax=6 for limited directions ($num_volumes)"
            elif [ "$num_volumes" -ge 64 ]; then
                lmax=8
                log "INFO" "[${sub}] Using lmax=8 for adequate directions ($num_volumes)"
            fi
            
            if dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -lmax $lmax -quiet -force 2>/dev/null; then
                log "OK" "[${sub}] Single-shell FOD computation successful"
            else
                log "WARN" "[${sub}] FOD computation failed with lmax=$lmax, trying lmax=6"
                if ! dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -lmax 6 -quiet -force; then
                    log "ERROR" "[${sub}] FOD computation failed"
                    return 1
                fi
            fi
        fi
        
    else
        # Multi-shell data - use multi-tissue approach
        log "INFO" "[${sub}] Multi-shell data detected - using multi-tissue approach"
        
        # Try dhollander for multi-shell
        if dwi2response dhollander dwi.mif wm.txt gm.txt csf.txt \
            -mask mask.mif -quiet -force 2>/dev/null; then
            
            # Validate all response functions
            local all_valid=true
            for rf in wm.txt gm.txt csf.txt; do
                if [ ! -f "$rf" ] || [ $(wc -l < "$rf") -lt 1 ]; then
                    all_valid=false
                    break
                fi
            done
            
            if [ "$all_valid" = true ]; then
                # Multi-tissue CSD
                log "INFO" "[${sub}] Using multi-tissue CSD"
                if dwi2fod msmt_csd dwi.mif -mask mask.mif \
                    wm.txt wmfod.mif \
                    gm.txt gmfod.mif \
                    csf.txt csffod.mif \
                    -quiet -force 2>/dev/null; then
                    
                    log "OK" "[${sub}] Multi-tissue CSD successful"
                    response_success=true
                else
                    log "WARN" "[${sub}] MSMT-CSD failed, falling back to single-tissue"
                fi
            fi
        fi
        
        # Fallback to single-tissue for multi-shell
        if [ "$response_success" = false ]; then
            log "INFO" "[${sub}] Using single-tissue approach for multi-shell data"
            if dwi2response tournier dwi.mif wm.txt -mask mask.mif -quiet -force; then
                if dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -quiet -force; then
                    response_success=true
                    log "OK" "[${sub}] Single-tissue FOD computation successful"
                fi
            fi
        fi
    fi
    
    if [ "$response_success" = false ]; then
        log "ERROR" "[${sub}] All response function methods failed"
        return 1
    fi
    
    # Verify FOD output
    if [ ! -f "wmfod.mif" ]; then
        log "ERROR" "[${sub}] FOD file not created"
        return 1
    fi
    
    # Validate FOD quality
    local fod_stats=$(mrstats wmfod.mif -mask mask.mif -quiet 2>/dev/null || echo "")
    if [ -z "$fod_stats" ]; then
        log "ERROR" "[${sub}] Invalid FOD file generated"
        return 1
    fi
    
    log "OK" "[${sub}] FOD computation completed successfully"
    update_progress "$sub" "connectivity" 60
    
    # Enhanced tractography with adaptive parameters
    log "INFO" "[${sub}] Generating streamlines with ML-optimized parameters"
    local start_time=$(date +%s)
    
    # Determine tractography parameters based on data quality and ML insights
    local target_tracks=10000000
    local min_length=10
    local max_length=250
    local angle_threshold=45
    local step_size="auto"
    
    # Adjust parameters based on DWI source and quality
    if [ -f "${EXTERNAL_POSTHOC}/${sub}/qc/${sub}_refined_comprehensive_report.txt" ]; then
        local quality_score=$(grep "Quality score:" "${EXTERNAL_POSTHOC}/${sub}/qc/${sub}_refined_comprehensive_report.txt" 2>/dev/null | cut -d: -f2 | cut -d/ -f1 | xargs || echo "50")
        
        if (( $(echo "$quality_score >= 80" | bc -l 2>/dev/null || echo 0) )); then
            log "INFO" "[${sub}] High quality data - using optimal tractography parameters"
            angle_threshold=35
            min_length=15
        elif (( $(echo "$quality_score < 60" | bc -l 2>/dev/null || echo 1) )); then
            log "INFO" "[${sub}] Lower quality data - using conservative tractography parameters"
            target_tracks=5000000
            angle_threshold=50
        fi
    fi
    
    # Adaptive tractography with multiple attempts
    local track_success=false
    local track_attempts=0
    local max_track_attempts=3
    
    while [ $track_attempts -lt $max_track_attempts ] && [ "$track_success" = false ]; do
        track_attempts=$((track_attempts + 1))
        
        local current_target=$((target_tracks / track_attempts))
        log "INFO" "[${sub}] Tractography attempt $track_attempts: targeting $current_target streamlines"
        
        if tckgen wmfod.mif tracks_${current_target}.tck \
            -act 5tt.mif \
            -backtrack \
            -seed_gmwmi gmwm_seed.mif \
            -select $current_target \
            -maxlength $max_length \
            -minlength $min_length \
                        -angle $angle_threshold \
            -nthreads $(get_optimal_threads 4) \
            -quiet -force 2>/dev/null; then
            
            # Verify track file
            local track_count=$(tckinfo tracks_${current_target}.tck | grep "count:" | awk '{print $2}' 2>/dev/null || echo "0")
            
            if [ "$track_count" -gt 1000 ]; then
                mv tracks_${current_target}.tck tracks_10M.tck
                track_success=true
                log "OK" "[${sub}] Tractography successful: $track_count streamlines generated"
            else
                log "WARN" "[${sub}] Tractography attempt $track_attempts produced insufficient tracks: $track_count"
                rm -f tracks_${current_target}.tck
            fi
        else
            log "WARN" "[${sub}] Tractography attempt $track_attempts failed"
        fi
        
        # Adjust parameters for retry
        if [ "$track_success" = false ] && [ $track_attempts -lt $max_track_attempts ]; then
            angle_threshold=$((angle_threshold + 10))
            min_length=$((min_length - 5))
            [ $min_length -lt 5 ] && min_length=5
            log "INFO" "[${sub}] Adjusting parameters for retry: angle=$angle_threshold, min_length=$min_length"
        fi
    done
    
    if [ "$track_success" = false ]; then
        log "ERROR" "[${sub}] All tractography attempts failed"
        return 1
    fi
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    log "INFO" "[${sub}] Tractography completed in $((duration / 60)) minutes"
    
    update_progress "$sub" "connectivity" 75
    
    # Enhanced SIFT2 filtering with quality assessment
    log "INFO" "[${sub}] Running SIFT2 filtering with quality assessment"
    
    local sift_success=false
    
    # Try SIFT2 with different configurations
    local sift_configs=("standard" "relaxed")
    
    for config in "${sift_configs[@]}"; do
        log "INFO" "[${sub}] Attempting SIFT2 with $config parameters"
        
        case $config in
            "standard")
                sift_opts="-act 5tt.mif -nthreads $(get_optimal_threads 4)"
                ;;
            "relaxed")
                sift_opts="-act 5tt.mif -nthreads $(get_optimal_threads 4) -fd_scale_gm"
                ;;
        esac
        
        if tcksift2 tracks_10M.tck wmfod.mif tracks_sift2.txt \
            $sift_opts -quiet 2>/dev/null; then
            
            # Validate SIFT2 output
            local sift_lines=$(wc -l < tracks_sift2.txt 2>/dev/null || echo "0")
            local track_lines=$(tckinfo tracks_10M.tck | grep "count:" | awk '{print $2}' 2>/dev/null || echo "0")
            
            if [ "$sift_lines" -eq "$track_lines" ] && [ "$sift_lines" -gt 0 ]; then
                sift_success=true
                log "OK" "[${sub}] SIFT2 successful with $config parameters"
                break
            else
                log "WARN" "[${sub}] SIFT2 $config produced invalid output"
                rm -f tracks_sift2.txt
            fi
        else
            log "WARN" "[${sub}] SIFT2 $config failed"
        fi
    done
    
    if [ "$sift_success" = false ]; then
        log "WARN" "[${sub}] SIFT2 failed, creating uniform weights"
        tckstats tracks_10M.tck -dump - 2>/dev/null | awk '{print 1}' > tracks_sift2.txt || {
            # Fallback: create weights file with track count
            local track_count=$(tckinfo tracks_10M.tck | grep "count:" | awk '{print $2}' 2>/dev/null || echo "1000")
            seq 1 $track_count | awk '{print 1}' > tracks_sift2.txt
        }
        log "INFO" "[${sub}] Using uniform track weights"
    fi
    
    update_progress "$sub" "connectivity" 85
    
    # Generate enhanced connectomes with multiple metrics
    log "INFO" "[${sub}] Building comprehensive structural connectomes"
    
    # Primary connectome (streamline count)
    local connectome_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv"
    mkdir -p "$(dirname "$connectome_file")"
    
    if ! tck2connectome tracks_10M.tck nodes.mif "$connectome_file" \
        -tck_weights_in tracks_sift2.txt \
        -symmetric \
        -zero_diagonal \
        -nthreads $(get_optimal_threads 4) \
        -quiet; then
        log "ERROR" "[${sub}] Primary connectome generation failed"
        return 1
    fi
    
    # Validate connectome
    local connectome_size=$(wc -l < "$connectome_file" 2>/dev/null || echo "0")
    local connectome_edges=$(awk -F',' '{for(i=1;i<=NF;i++) if($i>0) count++} END{print count/2}' "$connectome_file" 2>/dev/null || echo "0")
    
    log "INFO" "[${sub}] Primary connectome: ${connectome_size}x${connectome_size} nodes, $connectome_edges edges"
    
    if [ "$connectome_size" -lt 50 ] || [ "$connectome_edges" -lt 100 ]; then
        log "WARN" "[${sub}] Connectome seems sparse - check parcellation and tractography quality"
    fi
    
    # Generate additional connectome metrics if DTI metrics available
    generate_enhanced_connectomes "$sub"
    
    update_progress "$sub" "connectivity" 95
    
    # Generate comprehensive connectivity QC
    generate_enhanced_connectivity_qc "$sub"
    
    update_progress "$sub" "connectivity" 100
    
    monitor_resources "$sub" "tractography_end"
    log "OK" "[${sub}] Structural connectivity analysis completed"
    return 0
}

generate_enhanced_connectomes() {
    local sub=$1
    
    log "INFO" "[${sub}] Generating enhanced connectome metrics"
    
    # Mean FA connectome
    local fa_source=""
    if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
        # Calculate FA from refined data
        log "INFO" "[${sub}] Computing FA from refined DWI data"
        mrconvert "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" dwi_for_fa.mif \
            -fslgrad "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bvec" "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bval" \
            -quiet -force
        
        dwi2tensor dwi_for_fa.mif tensor_for_fa.mif -mask mask.mif -quiet -force
        tensor2metric tensor_for_fa.mif -fa fa_refined.mif -quiet -force
        fa_source="fa_refined.mif"
        
        rm -f dwi_for_fa.mif tensor_for_fa.mif
    elif [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_fa.nii.gz" ]; then
        mrconvert "${EXTERNAL_MRTRIX}/${sub}/${sub}_fa.nii.gz" fa_basic.mif -quiet -force
        fa_source="fa_basic.mif"
    fi
    
    if [ -n "$fa_source" ] && [ -f "$fa_source" ]; then
        log "INFO" "[${sub}] Creating FA-weighted connectome"
        
        if tcksample tracks_10M.tck "$fa_source" mean_fa_per_streamline.txt \
            -stat_tck mean -quiet 2>/dev/null; then
            
            tck2connectome tracks_10M.tck nodes.mif \
                "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_fa.csv" \
                -scale_file mean_fa_per_streamline.txt \
                -stat_edge mean \
                -symmetric \
                -zero_diagonal \
                -quiet && \
            log "OK" "[${sub}] FA connectome generated"
        else
            log "WARN" "[${sub}] FA connectome generation failed"
        fi
        
        rm -f "$fa_source" mean_fa_per_streamline.txt
    fi
    
    # Length-weighted connectome
    log "INFO" "[${sub}] Creating length-weighted connectome"
    
    if tck2connectome tracks_10M.tck nodes.mif \
        "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_length.csv" \
        -scale_length \
        -stat_edge mean \
        -symmetric \
        -zero_diagonal \
        -quiet 2>/dev/null; then
        log "OK" "[${sub}] Length-weighted connectome generated"
    else
        log "WARN" "[${sub}] Length-weighted connectome generation failed"
    fi
    
    # SIFT2-weighted connectome (different from primary)
    if [ -f "tracks_sift2.txt" ]; then
        log "INFO" "[${sub}] Creating SIFT2-weighted connectome"
        
        if tck2connectome tracks_10M.tck nodes.mif \
            "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_sift2.csv" \
            -tck_weights_in tracks_sift2.txt \
            -symmetric \
            -zero_diagonal \
            -stat_edge sum \
            -quiet 2>/dev/null; then
            log "OK" "[${sub}] SIFT2-weighted connectome generated"
        else
            log "WARN" "[${sub}] SIFT2-weighted connectome generation failed"
        fi
    fi
}

generate_enhanced_connectivity_qc() {
    local sub=$1
    local qc_file="${EXTERNAL_QC}/${sub}_connectivity_comprehensive.txt"
    
    mkdir -p "${EXTERNAL_QC}"
    
    {
        echo "Comprehensive Connectivity Analysis Report for ${sub}"
        echo "===================================================="
        echo "Generated on: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        echo ""
        
        echo "PROCESSING SUMMARY:"
        echo "=================="
        echo "FreeSurfer parcellation: Desikan-Killiany"
        echo "Tractography algorithm: iFOD2 (probabilistic)"
        
        # Track information
        local track_count=$(tckinfo tracks_10M.tck 2>/dev/null | grep 'count:' | awk '{print $2}' || echo 'N/A')
        local track_mean_length=$(tckstats tracks_10M.tck -dump - 2>/dev/null | awk '{sum+=$4; count++} END {print sum/count}' || echo 'N/A')
        
        echo "Number of streamlines: $track_count"
        echo "Mean streamline length: $track_mean_length mm"
        echo "SIFT2 filtering: $([ -f tracks_sift2.txt ] && echo 'Applied' || echo 'Not applied')"
        
        # ML integration summary
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "ML registration: ENABLED"
            local ml_method=$(cat ml_transform_method.txt 2>/dev/null || echo "Not applied")
            echo "ML method used: $ml_method"
            
            if [ "$ml_method" != "Not applied" ]; then
                echo "T1w-DWI registration: ML-enhanced ($ml_method)"
            else
                echo "T1w-DWI registration: Traditional (ML failed)"
            fi
        else
            echo "ML registration: DISABLED"
            echo "T1w-DWI registration: Traditional methods"
        fi
        
        echo ""
        echo "DATA SOURCES:"
        echo "============"
        
        # DWI source
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            echo "DWI data: Refined (post-hoc processing)"
            echo "  ✓ Enhanced bias correction applied"
            echo "  ✓ Intensity normalization applied"
            echo "  ✓ Enhanced brain masking applied"
        else
            echo "DWI data: Basic preprocessing"
            echo "  → Consider using refined data for improved results"
        fi
        
        # FreeSurfer quality
        if [ -f "${EXTERNAL_QC}/${sub}_freesurfer_quality.txt" ]; then
            local fs_quality=$(grep "Quality assessment:" "${EXTERNAL_QC}/${sub}_freesurfer_quality.txt" | cut -d: -f2 | xargs)
            echo "FreeSurfer quality: $fs_quality"
        else
            echo "FreeSurfer quality: Not assessed"
        fi
        
        echo ""
        echo "CONNECTOME STATISTICS:"
        echo "====================="
        
        if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" ]; then
            local num_nodes=$(wc -l < "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv")
            local num_edges=$(awk -F',' '{for(i=1;i<=NF;i++) if($i>0) count++} END{print count/2}' \
                            "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" 2>/dev/null || echo 0)
            local network_density=$($PYTHON_EXECUTABLE -c "print(f'{2*$num_edges/($num_nodes*($num_nodes-1)):.4f}')" 2>/dev/null || echo "N/A")
            
            echo "Primary connectome (streamline count):"
            echo "  Nodes: $num_nodes"
            echo "  Edges: $num_edges"
            echo "  Density: $network_density"
            
            # Connection strength statistics
            $PYTHON_EXECUTABLE -c "
import numpy as np
try:
    data = np.loadtxt('${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv', delimiter=',')
    
    # Get upper triangle (undirected network)
    mask = np.triu(np.ones_like(data, dtype=bool), k=1)
    connections = data[mask]
    nonzero_connections = connections[connections > 0]
    
    if len(nonzero_connections) > 0:
        print(f'  Connection strength - Mean: {np.mean(nonzero_connections):.2f}')
        print(f'  Connection strength - Std: {np.std(nonzero_connections):.2f}')
        print(f'  Connection strength - Range: {np.min(nonzero_connections):.2f} - {np.max(nonzero_connections):.2f}')
    else:
        print('  No connections found')
        
except Exception as e:
    print(f'  Statistics calculation failed: {e}')
" 2>/dev/null || echo "  Connection statistics: Calculation failed"
            
            # Additional connectomes
            echo ""
            echo "Additional connectomes generated:"
            local additional_connectomes=()
            
            [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_fa.csv" ] && additional_connectomes+=("FA-weighted")
            [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_length.csv" ] && additional_connectomes+=("Length-weighted")
            [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_sift2.csv" ] && additional_connectomes+=("SIFT2-weighted")
            
            if [ ${#additional_connectomes[@]} -gt 0 ]; then
                for connectome in "${additional_connectomes[@]}"; do
                    echo "  ✓ $connectome"
                done
            else
                echo "  None generated"
            fi
        else
            echo "Primary connectome: NOT FOUND"
            echo "ERROR: Connectivity analysis appears to have failed"
        fi
        
        echo ""
        echo "QUALITY ASSESSMENT:"
        echo "=================="
        
        # Tractography quality assessment
        if [ "$track_count" != "N/A" ] && [ "$track_count" -gt 0 ]; then
            if [ "$track_count" -gt 1000000 ]; then
                echo "Tractography quality: EXCELLENT (>1M streamlines)"
            elif [ "$track_count" -gt 500000 ]; then
                echo "Tractography quality: GOOD (>500K streamlines)"
            elif [ "$track_count" -gt 100000 ]; then
                echo "Tractography quality: ACCEPTABLE (>100K streamlines)"
            else
                echo "Tractography quality: POOR (<100K streamlines)"
            fi
        else
            echo "Tractography quality: FAILED (no valid streamlines)"
        fi
        
        # Connectome quality assessment
        if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" ]; then
            local edge_ratio=$($PYTHON_EXECUTABLE -c "print($num_edges / ($num_nodes * ($num_nodes - 1) / 2))" 2>/dev/null || echo "0")
            
            if (( $(echo "$edge_ratio > 0.3" | bc -l 2>/dev/null || echo 0) )); then
                echo "Connectome density: HIGH (>30% possible connections)"
            elif (( $(echo "$edge_ratio > 0.1" | bc -l 2>/dev/null || echo 0) )); then
                echo "Connectome density: MODERATE (10-30% connections)"
            elif (( $(echo "$edge_ratio > 0.05" | bc -l 2>/dev/null || echo 0) )); then
                echo "Connectome density: LOW (5-10% connections)"
            else
                echo "Connectome density: VERY LOW (<5% connections)"
            fi
        fi
        
        # Overall connectivity quality score
        local quality_score=0
        local max_score=100
        
        # Tractography contribution (40 points)
        if [ "$track_count" != "N/A" ] && [ "$track_count" -gt 0 ]; then
            if [ "$track_count" -gt 1000000 ]; then
                quality_score=$((quality_score + 40))
            elif [ "$track_count" -gt 500000 ]; then
                quality_score=$((quality_score + 30))
            elif [ "$track_count" -gt 100000 ]; then
                quality_score=$((quality_score + 20))
            else
                quality_score=$((quality_score + 10))
            fi
        fi
        
        # Connectome contribution (30 points)
        if [ "$num_edges" -gt 1000 ]; then
            quality_score=$((quality_score + 30))
        elif [ "$num_edges" -gt 500 ]; then
            quality_score=$((quality_score + 20))
        elif [ "$num_edges" -gt 100 ]; then
            quality_score=$((quality_score + 10))
        fi
        
        # Processing completeness (30 points)
        local completeness=0
        [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" ] && completeness=$((completeness + 10))
        [ -f "tracks_10M.tck" ] && completeness=$((completeness + 10))
        [ -f "tracks_sift2.txt" ] && completeness=$((completeness + 5))
        [ ${#additional_connectomes[@]} -gt 0 ] && completeness=$((completeness + 5))
        
        quality_score=$((quality_score + completeness))
        
        echo ""
        echo "OVERALL CONNECTIVITY QUALITY:"
        echo "============================"
        echo "Quality score: ${quality_score}/${max_score}"
        
        if [ $quality_score -ge 80 ]; then
            echo "Assessment: EXCELLENT - High-quality connectivity data"
        elif [ $quality_score -ge 65 ]; then
            echo "Assessment: GOOD - Suitable for most connectivity analyses"
        elif [ $quality_score -ge 50 ]; then
            echo "Assessment: ACCEPTABLE - Usable with some limitations"
        else
            echo "Assessment: POOR - Significant limitations, manual review recommended"
        fi
        
        echo ""
        echo "PROCESSING PERFORMANCE:"
        echo "======================"
        
        # Processing times if available
        if [ -f "${LOG_DIR}/resource_monitor.log" ]; then
            local connectivity_entries=$(grep -c "connectivity\|tractography" "${LOG_DIR}/resource_monitor.log" 2>/dev/null || echo "0")
            echo "Resource monitoring entries: $connectivity_entries"
        fi
        
        # ML performance summary
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            echo "ML registration performance:"
            if [ -f "ml_transform_method.txt" ]; then
                local ml_method=$(cat ml_transform_method.txt)
                echo "  Method used: $ml_method"
                echo "  Status: Successfully applied to tractography preparation"
            else
                echo "  Status: Attempted but failed, used traditional methods"
            fi
        fi
        
        echo ""
        echo "RECOMMENDATIONS:"
        echo "==============="
        
        # Generate specific recommendations
        local recommendations=()
        
        # Data quality recommendations
        if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
            recommendations+=("✓ Using refined DWI data - optimal for connectivity analysis")
        else
            recommendations+=("→ Consider using refined DWI data (--posthoc-refinement) for improved results")
        fi
        
        # ML recommendations
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            if [ -f "ml_transform_method.txt" ]; then
                recommendations+=("✓ ML registration successfully applied - enhanced T1w-DWI alignment")
            else
                recommendations+=("⚠ ML registration enabled but failed - check dependencies")
            fi
        else
            recommendations+=("→ Consider ML registration (--use-ml-registration) for improved alignment")
        fi
        
        # Tractography recommendations
        if [ "$track_count" != "N/A" ] && [ "$track_count" -lt 500000 ]; then
            recommendations+=("⚠ Low streamline count - consider adjusting tractography parameters")
        fi
        
        if [ "$num_edges" -lt 500 ]; then
            recommendations+=("⚠ Sparse connectome - verify parcellation and tractography quality")
        fi
        
        # SIFT2 recommendations
        if [ ! -f "tracks_sift2.txt" ]; then
            recommendations+=("⚠ SIFT2 filtering failed - connectome may be less accurate")
        fi
        
        # Quality-based recommendations
        if [ $quality_score -ge 80 ]; then
            recommendations+=("✓ Excellent quality - proceed with confidence to network analysis")
        elif [ $quality_score -ge 50 ]; then
            recommendations+=("→ Good quality - suitable for most network analyses")
        else
            recommendations+=("⚠ Quality concerns - consider manual review before analysis")
        fi
        
        # Print recommendations
        for rec in "${recommendations[@]}"; do
            echo "$rec"
        done
        
        echo ""
        echo "NEXT STEPS:"
        echo "=========="
        echo "1. Quality review: Examine connectivity matrices and QC reports"
        echo "2. Network analysis: Use connectomes for graph theory analysis"
        echo "3. Visualization: Create network visualizations using connectome data"
        echo "4. Statistical analysis: Compare networks across subjects/groups"
        
        if [ $quality_score -lt 50 ]; then
            echo ""
            echo "TROUBLESHOOTING STEPS:"
            echo "====================="
            echo "1. Review FreeSurfer reconstruction quality"
            echo "2. Check DWI data quality and preprocessing"
            echo "3. Verify T1w-DWI registration accuracy"
            echo "4. Consider adjusting tractography parameters"
            echo "5. Manual inspection of 5TT and parcellation alignment"
        fi
        
        echo ""
        echo "OUTPUT FILES:"
        echo "============"
        echo "Primary outputs:"
        echo "  - ${sub}_connectome_dk.csv (structural connectivity matrix)"
        
        for connectome in "${additional_connectomes[@]}"; do
            case $connectome in
                "FA-weighted") echo "  - ${sub}_connectome_fa.csv (FA-weighted connectivity)" ;;
                "Length-weighted") echo "  - ${sub}_connectome_length.csv (length-weighted connectivity)" ;;
                "SIFT2-weighted") echo "  - ${sub}_connectome_sift2.csv (SIFT2-weighted connectivity)" ;;
            esac
        done
        
        echo ""
        echo "Intermediate files (in work directory):"
        echo "  - tracks_10M.tck (streamlines)"
        echo "  - tracks_sift2.txt (SIFT2 weights)"
        echo "  - wmfod.mif (fiber orientation distributions)"
        echo "  - 5tt.mif (5-tissue-type image)"
        echo "  - nodes.mif (parcellation in DWI space)"
        
        echo ""
        echo "Report generated: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        
    } > "$qc_file"
    
    # Create brief summary for main log
    local summary_parts=()
    [ "$track_count" != "N/A" ] && summary_parts+=("${track_count} streamlines")
    [ "$num_edges" != "0" ] && summary_parts+=("$num_edges connections")
    
    local summary_text="Connectivity complete"
    [ ${#summary_parts[@]} -gt 0 ] && summary_text+=": ${summary_parts[*]}"
    
    log "OK" "[${sub}] $summary_text"
    log "INFO" "[${sub}] Connectivity QC report: $qc_file"
}

# Create checkpoint for connectivity completion
create_connectivity_checkpoint() {
    local sub=$1
    
    if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv" ]; then
        create_checkpoint "$sub" "connectivity_complete"
        log "OK" "[${sub}] Connectivity analysis checkpoint created"
    fi
}



# --- Stage 3: Connectivity Analysis (Continuation) ---

# Advanced tractography quality control and optimization
optimize_tractography_parameters() {
    local sub=$1
    local iteration=${2:-1}
    
    log "ML" "[${sub}] Optimizing tractography parameters (iteration $iteration)"
    
    # Analyze current tractography results for optimization
    $PYTHON_EXECUTABLE << 'EOF'
import nibabel as nib
import numpy as np
import sys

def analyze_tractography_quality(wmfod_file, mask_file, tracks_file):
    """Analyze tractography quality and suggest parameter adjustments"""
    
    try:
        # Load FOD data
        wmfod = nib.load(wmfod_file).get_fdata()
        mask = nib.load(mask_file).get_fdata()
        
        # Calculate FOD quality metrics
        fod_masked = wmfod[mask > 0]
        
        if len(fod_masked) == 0:
            return {"error": "Empty mask"}
        
        # FOD amplitude analysis
        fod_mean = np.mean(fod_masked)
        fod_std = np.std(fod_masked)
        fod_max = np.max(fod_masked)
        
        # Analyze tracks if available
        track_analysis = {}
        if tracks_file and sys.argv[4] != 'None':
            # Would analyze track file here - simplified for this example
            track_analysis['status'] = 'analyzed'
        
        recommendations = {}
        
        # Parameter recommendations based on FOD quality
        if fod_mean < 0.1:
            recommendations['angle_threshold'] = 50  # More permissive
            recommendations['min_length'] = 8
            recommendations['step_size'] = 1.0
            recommendations['reason'] = 'Low FOD amplitude detected'
        elif fod_mean > 0.3:
            recommendations['angle_threshold'] = 30  # More restrictive
            recommendations['min_length'] = 15
            recommendations['step_size'] = 0.5
            recommendations['reason'] = 'High FOD amplitude - can use stricter parameters'
        else:
            recommendations['angle_threshold'] = 35
            recommendations['min_length'] = 10
            recommendations['step_size'] = 0.75
            recommendations['reason'] = 'Standard parameters appropriate'
        
        # Adjust for noise level
        if fod_std / fod_mean > 0.8:  # High noise
            recommendations['angle_threshold'] += 10
            recommendations['reason'] += ' (high noise detected)'
        
        return {
            'fod_quality': {
                'mean': fod_mean,
                'std': fod_std,
                'max': fod_max,
                'snr': fod_mean / fod_std if fod_std > 0 else 0
            },
            'recommendations': recommendations
        }
        
    except Exception as e:
        return {"error": str(e)}

# Run analysis
wmfod_file = sys.argv[1] if len(sys.argv) > 1 else 'wmfod.mif'
mask_file = sys.argv[2] if len(sys.argv) > 2 else 'mask.mif'
tracks_file = sys.argv[3] if len(sys.argv) > 3 else None

result = analyze_tractography_quality(wmfod_file, mask_file, tracks_file)

if 'error' in result:
    print(f"Analysis failed: {result['error']}")
    sys.exit(1)

# Output recommendations
fod = result['fod_quality']
rec = result['recommendations']

print(f"FOD Quality Analysis:")
print(f"  Mean amplitude: {fod['mean']:.4f}")
print(f"  Std amplitude: {fod['std']:.4f}")
print(f"  Max amplitude: {fod['max']:.4f}")
print(f"  SNR estimate: {fod['snr']:.2f}")
print("")
print(f"Parameter Recommendations:")
print(f"  Angle threshold: {rec['angle_threshold']}°")
print(f"  Min length: {rec['min_length']} mm")
print(f"  Step size: {rec['step_size']} mm")
print(f"  Reason: {rec['reason']}")

# Save parameters for use in bash
with open('optimal_params.txt', 'w') as f:
    f.write(f"ANGLE={rec['angle_threshold']}\n")
    f.write(f"MIN_LENGTH={rec['min_length']}\n")
    f.write(f"STEP_SIZE={rec['step_size']}\n")
    f.write(f"REASON={rec['reason']}\n")

print("Optimal parameters saved to optimal_params.txt")
EOF

    local analysis_exit=$?
    
    if [ $analysis_exit -eq 0 ] && [ -f "optimal_params.txt" ]; then
        # Source the optimal parameters
        source optimal_params.txt
        
        log "ML" "[${sub}] Tractography optimization recommendations:"
        log "ML" "[${sub}]   Angle threshold: ${ANGLE}°"
        log "ML" "[${sub}]   Min length: ${MIN_LENGTH} mm"
        log "ML" "[${sub}]   Step size: ${STEP_SIZE} mm"
        log "ML" "[${sub}]   Reason: ${REASON}"
        
        # Export parameters for use in tractography
        export OPT_ANGLE=$ANGLE
        export OPT_MIN_LENGTH=$MIN_LENGTH
        export OPT_STEP_SIZE=$STEP_SIZE
        
        return 0
    else
        log "WARN" "[${sub}] Tractography optimization failed, using defaults"
        export OPT_ANGLE=35
        export OPT_MIN_LENGTH=10
        export OPT_STEP_SIZE=0.75
        return 1
    fi
}

# Enhanced multi-shell response function estimation
estimate_enhanced_response_functions() {
    local sub=$1
    
    log "ML" "[${sub}] Enhanced multi-shell response function estimation"
    
    # Analyze b-value distribution for optimal response estimation
    $PYTHON_EXECUTABLE << 'EOF'
import numpy as np
import sys

def analyze_bvalue_distribution(bval_file):
    """Analyze b-value distribution and recommend response estimation strategy"""
    
    try:
        bvals = np.loadtxt(bval_file)
        
        # Find unique b-values (rounded to nearest 100)
        unique_bvals = np.unique(np.round(bvals[bvals > 50], -2))
        
        # Count volumes per shell
        shell_counts = {}
        for bval in unique_bvals:
            count = np.sum((bvals >= bval - 50) & (bvals <= bval + 50))
            shell_counts[int(bval)] = count
        
        # Classify acquisition
        num_shells = len(unique_bvals)
        total_dwi = len(bvals[bvals > 50])
        
        strategy = {}
        
        if num_shells == 1:
            strategy['type'] = 'single_shell'
            strategy['method'] = 'tournier'
            strategy['lmax'] = 8 if total_dwi >= 45 else 6
        elif num_shells == 2:
            strategy['type'] = 'dual_shell'
            if all(count >= 20 for count in shell_counts.values()):
                strategy['method'] = 'dhollander'
            else:
                strategy['method'] = 'tournier'
            strategy['lmax'] = 8
        else:
            strategy['type'] = 'multi_shell'
            strategy['method'] = 'dhollander'
            strategy['lmax'] = 8 if total_dwi >= 60 else 6
        
        return {
            'shells': shell_counts,
            'num_shells': num_shells,
            'total_dwi': total_dwi,
            'strategy': strategy
        }
        
    except Exception as e:
        return {'error': str(e)}

# Analyze current b-values
result = analyze_bvalue_distribution('dwi.bval')

if 'error' in result:
    print(f"B-value analysis failed: {result['error']}")
    sys.exit(1)

print(f"B-value Analysis:")
print(f"  Shells detected: {result['shells']}")
print(f"  Number of shells: {result['num_shells']}")
print(f"  Total DWI volumes: {result['total_dwi']}")
print("")
print(f"Recommended strategy:")
print(f"  Type: {result['strategy']['type']}")
print(f"  Method: {result['strategy']['method']}")
print(f"  L-max: {result['strategy']['lmax']}")

# Save strategy
with open('response_strategy.txt', 'w') as f:
    f.write(f"TYPE={result['strategy']['type']}\n")
    f.write(f"METHOD={result['strategy']['method']}\n")
    f.write(f"LMAX={result['strategy']['lmax']}\n")

sys.exit(0)
EOF

    if [ $? -eq 0 ] && [ -f "response_strategy.txt" ]; then
        source response_strategy.txt
        
        log "ML" "[${sub}] Response function strategy: $TYPE using $METHOD (lmax=$LMAX)"
        
        # Apply the recommended strategy
        case $METHOD in
            "dhollander")
                log "INFO" "[${sub}] Using dhollander multi-tissue response estimation"
                if dwi2response dhollander dwi.mif wm.txt gm.txt csf.txt -mask mask.mif -quiet -force; then
                    # Validate response functions
                    if validate_response_functions "$sub"; then
                        log "OK" "[${sub}] Multi-tissue response functions validated"
                        echo "multi_tissue" > response_type.txt
                        return 0
                    fi
                fi
                log "WARN" "[${sub}] Dhollander failed, falling back to tournier"
                ;&  # Fall through to tournier
                
            "tournier")
                log "INFO" "[${sub}] Using tournier single-tissue response estimation"
                if dwi2response tournier dwi.mif wm.txt -mask mask.mif -quiet -force; then
                    if [ -f "wm.txt" ] && [ $(wc -l < wm.txt) -ge 1 ]; then
                        log "OK" "[${sub}] Single-tissue response function validated"
                        echo "single_tissue" > response_type.txt
                        export RESPONSE_LMAX=$LMAX
                        return 0
                    fi
                fi
                log "WARN" "[${sub}] Tournier failed, trying FA method"
                ;&  # Fall through to FA
                
            *)
                log "INFO" "[${sub}] Using FA-based response estimation"
                if dwi2response fa dwi.mif wm.txt -mask mask.mif -quiet -force; then
                    if [ -f "wm.txt" ] && [ $(wc -l < wm.txt) -ge 1 ]; then
                        log "OK" "[${sub}] FA-based response function validated"
                        echo "single_tissue" > response_type.txt
                        export RESPONSE_LMAX=6  # Conservative for FA method
                        return 0
                    fi
                fi
                ;;
        esac
    fi
    
    log "ERROR" "[${sub}] All enhanced response estimation methods failed"
    return 1
}

validate_response_functions() {
    local sub=$1
    
    log "INFO" "[${sub}] Validating response functions"
    
    $PYTHON_EXECUTABLE << 'EOF'
import numpy as np
import sys

def validate_response_function(rf_file, tissue_type):
    """Validate response function parameters"""
    
    try:
        rf_data = np.loadtxt(rf_file)
        
        # Basic validation
        if len(rf_data) == 0:
            return False, "Empty response function"
        
        # Check for reasonable values based on tissue type
        if tissue_type == 'wm':
            # White matter should have high anisotropy
            if len(rf_data) >= 3:
                fa_approx = rf_data[0] / (rf_data[0] + 2 * rf_data[1]) if (rf_data[0] + 2 * rf_data[1]) > 0 else 0
                if fa_approx < 0.3:
                    return False, f"WM FA too low: {fa_approx:.3f}"
                if fa_approx > 0.95:
                    return False, f"WM FA too high: {fa_approx:.3f}"
        
        elif tissue_type == 'gm':
            # Gray matter should have low anisotropy
            if len(rf_data) >= 3:
                fa_approx = rf_data[0] / (rf_data[0] + 2 * rf_data[1]) if (rf_data[0] + 2 * rf_data[1]) > 0 else 0
                if fa_approx > 0.3:
                    return False, f"GM FA too high: {fa_approx:.3f}"
        
        elif tissue_type == 'csf':
            # CSF should be isotropic
            if len(rf_data) >= 3:
                if rf_data[0] > rf_data[1] * 2:  # Should be roughly isotropic
                    return False, f"CSF too anisotropic"
        
        return True, "Valid"
        
    except Exception as e:
        return False, str(e)

# Validate each response function file
response_files = [
    ('wm.txt', 'wm'),
    ('gm.txt', 'gm'), 
    ('csf.txt', 'csf')
]

all_valid = True
validation_results = {}

for rf_file, tissue_type in response_files:
    try:
        valid, message = validate_response_function(rf_file, tissue_type)
        validation_results[tissue_type] = {'valid': valid, 'message': message}
        
        if valid:
            print(f"{tissue_type.upper()} response function: VALID")
        else:
            print(f"{tissue_type.upper()} response function: INVALID ({message})")
            if tissue_type == 'wm':  # WM is critical
                all_valid = False
                
    except FileNotFoundError:
        validation_results[tissue_type] = {'valid': False, 'message': 'File not found'}
        if tissue_type == 'wm':
            all_valid = False

print(f"\nOverall validation: {'PASSED' if all_valid else 'FAILED'}")
sys.exit(0 if all_valid else 1)
EOF

    return $?
}

# Enhanced FOD estimation with quality control
estimate_enhanced_fods() {
    local sub=$1
    
    log "ML" "[${sub}] Enhanced FOD estimation with quality control"
    
    local response_type=$(cat response_type.txt 2>/dev/null || echo "single_tissue")
    local lmax=${RESPONSE_LMAX:-8}
    
    case $response_type in
        "multi_tissue")
            log "INFO" "[${sub}] Computing multi-tissue FODs"
            
            # Multi-tissue CSD with enhanced parameters
            if dwi2fod msmt_csd dwi.mif -mask mask.mif \
                wm.txt wmfod.mif \
                gm.txt gmfod.mif \
                csf.txt csffod.mif \
                -lmax $lmax,0,0 \
                -nthreads $(get_optimal_threads 4) \
                -quiet -force; then
                
                # Validate multi-tissue FODs
                if validate_fod_quality "$sub" "wmfod.mif" "multi_tissue"; then
                    log "OK" "[${sub}] Multi-tissue FOD estimation successful"
                    return 0
                else
                    log "WARN" "[${sub}] Multi-tissue FOD validation failed, using single-tissue"
                fi
            else
                log "WARN" "[${sub}] Multi-tissue CSD failed, falling back to single-tissue"
            fi
            
            # Fallback to single-tissue if multi-tissue failed
            if [ ! -f "wmfod.mif" ] || ! validate_fod_quality "$sub" "wmfod.mif" "fallback_check"; then
                log "INFO" "[${sub}] Using single-tissue CSD as fallback"
                if ! dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -lmax $lmax -quiet -force; then
                    log "ERROR" "[${sub}] Single-tissue FOD estimation failed"
                    return 1
                fi
            fi
            ;;
            
        "single_tissue"|*)
            log "INFO" "[${sub}] Computing single-tissue FODs"
            
            # Adaptive lmax based on number of directions
            local num_dirs=$(mrinfo dwi.mif -size | awk '{print $4}')
            if [ "$num_dirs" -lt 45 ]; then
                lmax=6
                log "INFO" "[${sub}] Using lmax=6 for limited directions ($num_dirs)"
            fi
            
            if ! dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -lmax $lmax -quiet -force; then
                # Try with reduced lmax
                log "WARN" "[${sub}] CSD failed with lmax=$lmax, trying lmax=6"
                if ! dwi2fod csd dwi.mif wm.txt wmfod.mif -mask mask.mif -lmax 6 -quiet -force; then
                    log "ERROR" "[${sub}] FOD computation failed with all lmax values"
                    return 1
                fi
            fi
            ;;
    esac
    
    # Final FOD validation
    if validate_fod_quality "$sub" "wmfod.mif" "final"; then
        log "OK" "[${sub}] Enhanced FOD estimation completed successfully"
        return 0
    else
        log "ERROR" "[${sub}] Final FOD validation failed"
        return 1
    fi
}

validate_fod_quality() {
    local sub=$1
    local fod_file=$2
    local validation_type=${3:-"standard"}
    
    log "ML" "[${sub}] Validating FOD quality ($validation_type)"
    
    # Check file exists and is valid
    if [ ! -f "$fod_file" ]; then
        log "ERROR" "[${sub}] FOD file not found: $fod_file"
        return 1
    fi
    
    # Basic MRtrix validation
    if ! mrstats "$fod_file" -mask mask.mif -quiet >/dev/null 2>&1; then
        log "ERROR" "[${sub}] FOD file invalid or corrupted"
        return 1
    fi
    
    # Advanced validation using Python
    # Convert .mif to .nii.gz for nibabel compatibility
    local fod_nii="${fod_file%.mif}_validate_tmp.nii.gz"
    local mask_nii="mask_validate_tmp.nii.gz"
    mrconvert "$fod_file" "$fod_nii" -quiet -force 2>/dev/null || true
    mrconvert mask.mif "$mask_nii" -quiet -force 2>/dev/null || true
    _FOD_FILE="$fod_nii" _MASK_FILE="$mask_nii" _SUB="$sub" $PYTHON_EXECUTABLE - << 'PYEOF'
import os, sys
import nibabel as nib
import numpy as np

sub = os.environ['_SUB']
fod_file = os.environ['_FOD_FILE']
mask_file = os.environ['_MASK_FILE']

try:
    fod_data = nib.load(fod_file).get_fdata()
    mask_data = nib.load(mask_file).get_fdata()
    
    if fod_data.size == 0:
        print(f"[{sub}] ERROR: Empty FOD data"); sys.exit(1)
    
    masked_fod = fod_data[mask_data > 0]
    if len(masked_fod) == 0:
        print(f"[{sub}] ERROR: No FOD data in mask"); sys.exit(1)
    
    fod_mean = np.mean(masked_fod)
    fod_max = np.max(masked_fod)
    
    if fod_mean < 0.01:
        print(f"[{sub}] WARNING: Very low FOD amplitudes (mean: {fod_mean:.4f})")
    elif fod_mean > 1.0:
        print(f"[{sub}] WARNING: Unusually high FOD amplitudes (mean: {fod_mean:.4f})")
    
    if fod_max > 5.0:
        print(f"[{sub}] WARNING: Extreme FOD values detected (max: {fod_max:.4f})")
    
    print(f"[{sub}] FOD validation passed (mean: {fod_mean:.4f}, max: {fod_max:.4f})")
    
except Exception as e:
    print(f"[{sub}] FOD validation failed: {str(e)}")
    sys.exit(1)
PYEOF

    local fod_exit=$?
    rm -f "$fod_nii" "$mask_nii" 2>/dev/null
    return $fod_exit
}

# Complete the tractography with enhanced parameters from optimization
run_enhanced_tractography() {
    local sub=$1
    
    log "INFO" "[${sub}] Running enhanced tractography with optimized parameters"
    
    # Use optimized parameters if available
    local angle_threshold=${OPT_ANGLE:-45}
    local min_length=${OPT_MIN_LENGTH:-10}
    local step_size=${OPT_STEP_SIZE:-0.75}
    
    # Determine target streamlines based on data quality
    local target_tracks=10000000
    local max_attempts=3
    
    # Enhanced tractography with adaptive retry
    local track_success=false
    local attempt=0
    
    while [ $attempt -lt $max_attempts ] && [ "$track_success" = false ]; do
        attempt=$((attempt + 1))
        local current_target=$((target_tracks / attempt))
        
        log "INFO" "[${sub}] Tractography attempt $attempt: $current_target streamlines"
        
        if tckgen wmfod.mif tracks_${current_target}.tck \
            -act 5tt.mif \
            -backtrack \
            -seed_gmwmi gmwm_seed.mif \
            -select $current_target \
            -maxlength 250 \
            -minlength $min_length \
            -angle $angle_threshold \
            -step $step_size \
            -nthreads $(get_optimal_threads 4) \
            -quiet -force 2>/dev/null; then
            
            # Verify track count
            local actual_tracks=$(tckinfo tracks_${current_target}.tck | grep "count:" | awk '{print $2}' 2>/dev/null || echo "0")
            
            if [ "$actual_tracks" -gt 1000 ]; then
                mv tracks_${current_target}.tck tracks_10M.tck
                track_success=true
                log "OK" "[${sub}] Tractography successful: $actual_tracks streamlines"
            else
                log "WARN" "[${sub}] Insufficient streamlines generated: $actual_tracks"
                rm -f tracks_${current_target}.tck
            fi
        else
            log "WARN" "[${sub}] Tractography attempt $attempt failed"
        fi
        
        # Adjust parameters for retry
        if [ "$track_success" = false ] && [ $attempt -lt $max_attempts ]; then
            angle_threshold=$((angle_threshold + 10))
            min_length=$((min_length - 2))
            [ $min_length -lt 5 ] && min_length=5
        fi
    done
    
    return $([ "$track_success" = true ] && echo 0 || echo 1)
}

# Complete connectivity analysis with all enhancements
complete_connectivity_analysis() {
    local sub=$1
    
    update_progress "$sub" "connectivity" 85
    
    # Enhanced SIFT2 with validation
    log "INFO" "[${sub}] Running enhanced SIFT2 filtering"
    
    local sift_success=false
    local sift_methods=("standard" "relaxed" "fallback")
    
    for method in "${sift_methods[@]}"; do
        case $method in
            "standard")
                tcksift2 tracks_10M.tck wmfod.mif tracks_sift2.txt \
                    -act 5tt.mif -nthreads $(get_optimal_threads 4) -quiet -force
                ;;
            "relaxed")
                tcksift2 tracks_10M.tck wmfod.mif tracks_sift2.txt \
                    -act 5tt.mif -fd_scale_gm -nthreads $(get_optimal_threads 4) -quiet -force
                ;;
            "fallback")
                # Create uniform weights
                tckinfo tracks_10M.tck | grep "count:" | awk '{print $2}' | \
                xargs seq 1 | awk '{print 1}' > tracks_sift2.txt
                ;;
        esac
        
        if [ -f "tracks_sift2.txt" ] && [ $(wc -l < tracks_sift2.txt) -gt 0 ]; then
            sift_success=true
            log "OK" "[${sub}] SIFT2 successful using $method method"
            break
        fi
    done
    
    if [ "$sift_success" = false ]; then
        log "ERROR" "[${sub}] All SIFT2 methods failed"
        return 1
    fi
    
    update_progress "$sub" "connectivity" 90
    
    # Generate enhanced connectomes
    log "INFO" "[${sub}] Building comprehensive connectomes"
    
    # Primary connectome
    local connectome_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv"
    mkdir -p "$(dirname "$connectome_file")"
    
    if ! tck2connectome tracks_10M.tck nodes.mif "$connectome_file" \
        -tck_weights_in tracks_sift2.txt \
        -symmetric -zero_diagonal \
        -nthreads $(get_optimal_threads 4) -quiet; then
        log "ERROR" "[${sub}] Primary connectome generation failed"
        return 1
    fi
    
    # Generate additional connectome metrics as implemented in the original
    generate_enhanced_connectomes "$sub"
    
    update_progress "$sub" "connectivity" 95
    
    # Final connectivity QC with ML integration summary
    generate_enhanced_connectivity_qc "$sub"
    
    update_progress "$sub" "connectivity" 100
    create_connectivity_checkpoint "$sub"
    
    log "OK" "[${sub}] Enhanced connectivity analysis completed"
    return 0
}


# Complete tractography section using optimized parameters
complete_tractography_with_optimization() {
    local sub=$1
    
    # Use ML-optimized parameters if available
    local angle_threshold=${OPT_ANGLE:-45}
    local min_length=${OPT_MIN_LENGTH:-10}
    
    log "INFO" "[${sub}] Running tractography with optimized parameters"
    local start_time=$(date +%s)
    
    # Enhanced tractography with multiple attempts
    local target_tracks=10000000
    local track_success=false
    local attempts=0
    local max_attempts=3
    
    while [ $attempts -lt $max_attempts ] && [ "$track_success" = false ]; do
        attempts=$((attempts + 1))
        local current_target=$((target_tracks / attempts))
        
        log "INFO" "[${sub}] Tractography attempt $attempts: $current_target streamlines"
        
        if tckgen wmfod.mif tracks_${current_target}.tck \
            -act 5tt.mif \
            -backtrack \
            -seed_gmwmi gmwm_seed.mif \
            -select $current_target \
            -maxlength 250 \
            -minlength $min_length \
            -angle $angle_threshold \
            -nthreads $(get_optimal_threads 4) \
            -quiet -force 2>/dev/null; then
            
            local track_count=$(tckinfo tracks_${current_target}.tck | grep "count:" | awk '{print $2}' 2>/dev/null || echo "0")
            
            if [ "$track_count" -gt 1000 ]; then
                mv tracks_${current_target}.tck tracks_10M.tck
                track_success=true
                log "OK" "[${sub}] Tractography successful: $track_count streamlines"
            else
                log "WARN" "[${sub}] Insufficient streamlines: $track_count"
                rm -f tracks_${current_target}.tck
            fi
        else
            log "WARN" "[${sub}] Tractography attempt $attempts failed"
        fi
        
        # Adjust parameters for retry
        if [ "$track_success" = false ] && [ $attempts -lt $max_attempts ]; then
            angle_threshold=$((angle_threshold + 10))
            min_length=$((min_length - 2))
            [ $min_length -lt 5 ] && min_length=5
        fi
    done
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    log "INFO" "[${sub}] Tractography completed in $((duration / 60)) minutes"
    
    return $([ "$track_success" = true ] && echo 0 || echo 1)
}

# Enhanced SIFT2 and connectome generation
complete_enhanced_sift_and_connectome() {
    local sub=$1
    
    log "INFO" "[${sub}] Running enhanced SIFT2 filtering"
    
    # Try SIFT2 with different approaches
    local sift_success=false
    local sift_methods=("standard" "relaxed" "uniform")
    
    for method in "${sift_methods[@]}"; do
        case $method in
            "standard")
                if tcksift2 tracks_10M.tck wmfod.mif tracks_sift2.txt \
                    -act 5tt.mif -nthreads $(get_optimal_threads 4) -quiet -force 2>/dev/null; then
                    sift_success=true
                    break
                fi
                ;;
            "relaxed")
                if tcksift2 tracks_10M.tck wmfod.mif tracks_sift2.txt \
                    -act 5tt.mif -fd_scale_gm -nthreads $(get_optimal_threads 4) -quiet -force 2>/dev/null; then
                    sift_success=true
                    break
                fi
                ;;
            "uniform")
                # Create uniform weights as fallback
                tckinfo tracks_10M.tck | grep "count:" | awk '{print $2}' | \
                xargs seq 1 | awk '{print 1}' > tracks_sift2.txt
                sift_success=true
                log "WARN" "[${sub}] Using uniform track weights (SIFT2 failed)"
                break
                ;;
        esac
    done
    
    if [ "$sift_success" = false ]; then
        log "ERROR" "[${sub}] All SIFT2 methods failed"
        return 1
    fi
    
    # Generate primary connectome
    log "INFO" "[${sub}] Building structural connectome"
    local connectome_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_connectome_dk.csv"
    mkdir -p "$(dirname "$connectome_file")"
    
    if ! tck2connectome tracks_10M.tck nodes.mif "$connectome_file" \
        -tck_weights_in tracks_sift2.txt \
        -symmetric -zero_diagonal \
        -nthreads $(get_optimal_threads 4) -quiet; then
        log "ERROR" "[${sub}] Connectome generation failed"
        return 1
    fi
    
    # Generate additional connectome metrics if data available
    if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_fa.nii.gz" ] || [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
        generate_enhanced_connectomes "$sub"
    fi
    
    log "OK" "[${sub}] Enhanced connectome generation completed"
    return 0
}

# --- Stage 4: NODDI Estimation with ML Integration ---
run_noddi_estimation() {
    local sub=$1
    local workdir="${WORK_DIR}/${sub}/noddi"
    
    log "STAGE" "[${sub}] Starting NODDI estimation (Stage 4) with ML integration"
    monitor_resources "$sub" "noddi_start"
    update_progress "$sub" "noddi" 0
    
    # Check if already completed
    if [ -f "${EXTERNAL_MRTRIX}/${sub}/${sub}_ndi.nii.gz" ]; then
        log "INFO" "[${sub}] NODDI estimation already completed"
        return 0
    fi
    
    # System requirements check
    if ! check_disk_space "$(dirname "$BIDS_DIR")" 25; then
        log "ERROR" "[${sub}] Insufficient disk space for NODDI estimation"
        return 1
    fi
    
    if ! check_memory_usage 6; then
        log "WARN" "[${sub}] Limited memory for NODDI - processing may be slower"
    fi
    
    mkdir -p "$workdir/kernels"
    safe_cd "$workdir" || return 1
    
    update_progress "$sub" "noddi" 10
    
    # Select optimal DWI data (prefer ML-refined over basic)
    local dwi_file bvec_file bval_file mask_file data_source
    
    if [ -f "${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz" ]; then
        log "INFO" "[${sub}] Using ML-refined data for NODDI estimation"
        dwi_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi_refined.nii.gz"
        bvec_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bvec"
        bval_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_dwi.bval"
        mask_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_mask_enhanced.nii.gz"
        data_source="refined"
    else
        log "INFO" "[${sub}] Using standard preprocessed data for NODDI estimation"
        dwi_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi_preproc.nii.gz"
        bvec_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi.bvec"
        bval_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_dwi.bval"
        mask_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_mask.nii.gz"
        data_source="basic"
    fi
    
    # Verify all required files exist
    local missing_files=()
    for f in "$dwi_file" "$bvec_file" "$bval_file" "$mask_file"; do
        if [ ! -f "$f" ]; then
            missing_files+=("$(basename "$f")")
        fi
    done
    
    if [ ${#missing_files[@]} -gt 0 ]; then
        log "ERROR" "[${sub}] Missing required files: ${missing_files[*]}"
        safe_cd_return
        return 1
    fi
    
    update_progress "$sub" "noddi" 20
    
    # Create enhanced NODDI fitting script with comprehensive validation
    create_enhanced_noddi_script "$sub" "$data_source"
    
    update_progress "$sub" "noddi" 30
    
    # Run NODDI fitting with monitoring
    log "INFO" "[${sub}] Running NODDI fitting with validation (15-45 minutes estimated)"
    local noddi_start_time=$(date +%s)
    
    $PYTHON_EXECUTABLE "${workdir}/enhanced_noddi_fit.py" "$sub" "$workdir" \
        "$dwi_file" "$bvec_file" "$bval_file" "$mask_file" "$data_source" \
        &> "${workdir}/noddi_comprehensive_log.txt"
    
    local noddi_exit_code=$?
    local noddi_end_time=$(date +%s)
    local noddi_duration=$((noddi_end_time - noddi_start_time))
    
    update_progress "$sub" "noddi" 80
    
    if [ $noddi_exit_code -eq 0 ]; then
        log "OK" "[${sub}] NODDI fitting completed in $((noddi_duration / 60)) minutes"
        
        # Process and move results
        if process_noddi_results "$sub"; then
            update_progress "$sub" "noddi" 90
            
            # Generate comprehensive NODDI QC
            generate_comprehensive_noddi_qc "$sub" "$data_source"
            
            update_progress "$sub" "noddi" 100
            create_checkpoint "$sub" "noddi_complete"
            
            log "OK" "[${sub}] NODDI estimation completed successfully"
            safe_cd_return
            cleanup_work_dir "$sub"
            monitor_resources "$sub" "noddi_end"
            return 0
        else
            log "ERROR" "[${sub}] NODDI results processing failed"
        fi
    else
        log "ERROR" "[${sub}] NODDI fitting failed after $((noddi_duration / 60)) minutes"
        
        # Analyze failure
        if [ -f "${workdir}/noddi_comprehensive_log.txt" ]; then
            log "ERROR" "[${sub}] Last 10 lines of NODDI log:"
            tail -n 10 "${workdir}/noddi_comprehensive_log.txt" >&2
            
            # Check for common errors
            analyze_noddi_failure "$sub"
        fi
    fi
    
    safe_cd_return
    return 1
}

create_enhanced_noddi_script() {
    local sub=$1
    local data_source=${2:-basic}

    # Ensure WORK_DIR is set (fall back to DERIV_DIR if not)
    local base_workdir="${WORK_DIR:-${DERIV_DIR}/work}"
    local workdir="${base_workdir}/${sub}/noddi"
    mkdir -p "$workdir"
    
    local script_path="${workdir}/enhanced_noddi_fit.py"
    
    cat > "$script_path" << 'PYTHON_EOF'
import amico, os, sys, warnings, datetime
import numpy as np
from amico import util
warnings.filterwarnings("ignore")

def comprehensive_noddi_validation(ae, sub, data_source):
    """Comprehensive NODDI validation with ML data integration awareness"""
    
    print(f"[{sub}] Performing comprehensive NODDI validation...")
    print(f"[{sub}] Data source: {data_source}")
    
    validation_results = {}
    validation_results['data_source'] = data_source
    
    try:
        # Get fitted parameters
        params = ae.get_params()
        
        # 1. Parameter range validation
        param_validation = validate_parameter_ranges(params, sub)
        validation_results.update(param_validation)
        
        # 2. Model fit quality assessment
        fit_validation = assess_model_fit_quality(params, sub)
        validation_results.update(fit_validation)
        
        # 3. Acquisition adequacy check
        acquisition_validation = validate_acquisition_adequacy(ae, sub)
        validation_results.update(acquisition_validation)
        
        # 4. ML data source specific validation
        if data_source == 'refined':
            ml_validation = validate_ml_refined_results(params, sub)
            validation_results.update(ml_validation)
        
        # 5. Overall quality score
        overall_score = calculate_overall_quality_score(validation_results)
        validation_results['overall_quality_score'] = overall_score
        
        return validation_results
        
    except Exception as e:
        print(f"[{sub}] Validation failed: {str(e)}")
        validation_results['validation_error'] = str(e)
        return validation_results

def validate_parameter_ranges(params, sub):
    """Validate NODDI parameters are in physiologically reasonable ranges"""
    
    results = {}
    
    try:
        if 'NDI' in params:
            ndi = params['NDI']
            ndi_valid = np.logical_and(ndi >= 0, ndi <= 1)
            ndi_reasonable = np.logical_and(ndi >= 0.05, ndi <= 0.9)
            
            results['ndi_valid_percent'] = np.sum(ndi_valid) / ndi.size * 100
            results['ndi_reasonable_percent'] = np.sum(ndi_reasonable) / ndi.size * 100
            results['ndi_mean'] = np.nanmean(ndi[ndi_valid])
            results['ndi_std'] = np.nanstd(ndi[ndi_valid])
            
            if results['ndi_valid_percent'] < 85:
                print(f"[{sub}] WARNING: Only {results['ndi_valid_percent']:.1f}% NDI values in valid range")
        
        if 'ODI' in params:
            odi = params['ODI']
            odi_valid = np.logical_and(odi >= 0, odi <= 1)
            
            results['odi_valid_percent'] = np.sum(odi_valid) / odi.size * 100
            results['odi_mean'] = np.nanmean(odi[odi_valid])
            results['odi_std'] = np.nanstd(odi[odi_valid])
            
            if results['odi_valid_percent'] < 85:
                print(f"[{sub}] WARNING: Only {results['odi_valid_percent']:.1f}% ODI values in valid range")
        
        if 'FWF' in params:
            fwf = params['FWF']
            fwf_valid = np.logical_and(fwf >= 0, fwf <= 1)
            
            results['fwf_valid_percent'] = np.sum(fwf_valid) / fwf.size * 100
            results['fwf_mean'] = np.nanmean(fwf[fwf_valid])
            results['fwf_std'] = np.nanstd(fwf[fwf_valid])
        
        print(f"[{sub}] Parameter range validation completed")
        
    except Exception as e:
        print(f"[{sub}] Parameter validation failed: {str(e)}")
        results['param_validation_error'] = str(e)
    
    return results

def assess_model_fit_quality(params, sub):
    """Assess quality of NODDI model fit"""
    
    results = {}
    
    try:
        if 'NDI' in params and 'ODI' in params:
            ndi = params['NDI']
            odi = params['ODI']
            
            # Check for unreasonable parameter combinations
            both_zero = np.sum((ndi < 0.01) & (odi < 0.01))
            both_max = np.sum((ndi > 0.99) & (odi > 0.99))
            
            total_voxels = ndi.size
            results['suspicious_voxels_percent'] = (both_zero + both_max) / total_voxels * 100
            
            # Parameter correlation analysis
            valid_mask = (ndi >= 0) & (ndi <= 1) & (odi >= 0) & (odi <= 1)
            if np.sum(valid_mask) > 100:
                results['ndi_odi_correlation'] = np.corrcoef(ndi[valid_mask], odi[valid_mask])[0,1]
            else:
                results['ndi_odi_correlation'] = np.nan
            
            # Check for fitting convergence issues
            results['extreme_ndi_voxels'] = np.sum((ndi < 0.001) | (ndi > 0.999)) / total_voxels * 100
            results['extreme_odi_voxels'] = np.sum((odi < 0.001) | (odi > 0.999)) / total_voxels * 100
            
            if results['suspicious_voxels_percent'] > 15:
                print(f"[{sub}] WARNING: {results['suspicious_voxels_percent']:.1f}% voxels have suspicious parameters")
        
        results['fit_assessment_completed'] = True
        
    except Exception as e:
        print(f"[{sub}] Fit quality assessment failed: {str(e)}")
        results['fit_assessment_error'] = str(e)
    
    return results

def validate_acquisition_adequacy(ae, sub):
    """Validate DWI acquisition adequacy for NODDI"""
    
    results = {}
    
    try:
        # Get acquisition scheme
        scheme = ae.get_scheme()
        if hasattr(scheme, 'b'):
            b_values = scheme.b
        else:
            # Fallback - estimate from loaded data
            b_values = np.array([0] * 5 + [1000] * 30 + [2000] * 60)  # Typical scheme
        
        # Analyze b-value distribution
        unique_shells = np.unique(np.round(b_values[b_values > 100], -2))
        
        results['total_volumes'] = len(b_values)
        results['unique_shells'] = unique_shells.tolist()
        results['num_shells'] = len(unique_shells)
        
        # NODDI-specific requirements
        b0_volumes = np.sum(b_values < 100)
        low_b_volumes = np.sum((b_values >= 800) & (b_values <= 1200))
        high_b_volumes = np.sum(b_values >= 1800)
        
        results['b0_volumes'] = b0_volumes
        results['low_b_volumes'] = low_b_volumes
        results['high_b_volumes'] = high_b_volumes
        
        # Quality thresholds
        results['adequate_b0'] = b0_volumes >= 1
        results['adequate_low_b'] = low_b_volumes >= 15
        results['adequate_high_b'] = high_b_volumes >= 30
        results['noddi_adequate'] = all([results['adequate_b0'], results['adequate_low_b'], results['adequate_high_b']])
        
        if not results['noddi_adequate']:
            warnings = []
            if not results['adequate_b0']:
                warnings.append("insufficient b=0 volumes")
            if not results['adequate_low_b']:
                warnings.append(f"insufficient low-b volumes ({low_b_volumes} < 15)")
            if not results['adequate_high_b']:
                warnings.append(f"insufficient high-b volumes ({high_b_volumes} < 30)")
            
            print(f"[{sub}] ACQUISITION WARNING: {'; '.join(warnings)}")
        else:
            print(f"[{sub}] Acquisition adequate for NODDI")
        
    except Exception as e:
        print(f"[{sub}] Acquisition validation failed: {str(e)}")
        results['acquisition_validation_error'] = str(e)
    
    return results

def validate_ml_refined_results(params, sub):
    """Additional validation for ML-refined data"""
    
    results = {}
    results['ml_data_used'] = True
    
    try:
        if 'NDI' in params and 'ODI' in params:
            ndi = params['NDI']
            odi = params['ODI']
            
            # ML-refined data should show improved parameter maps
            # Check for smoother parameter distributions
            ndi_valid = ndi[(ndi >= 0) & (ndi <= 1)]
            odi_valid = odi[(odi >= 0) & (odi <= 1)]
            
            if len(ndi_valid) > 0 and len(odi_valid) > 0:
                # Calculate coefficient of variation as smoothness proxy
                results['ndi_cv'] = np.std(ndi_valid) / np.mean(ndi_valid) if np.mean(ndi_valid) > 0 else np.inf
                results['odi_cv'] = np.std(odi_valid) / np.mean(odi_valid) if np.mean(odi_valid) > 0 else np.inf
                
                # ML-refined data should have reasonable variability
                if results['ndi_cv'] > 2.0:
                    print(f"[{sub}] NOTE: High NDI variability despite ML refinement (CV={results['ndi_cv']:.2f})")
                if results['odi_cv'] > 2.0:
                    print(f"[{sub}] NOTE: High ODI variability despite ML refinement (CV={results['odi_cv']:.2f})")
            
            print(f"[{sub}] ML-refined data validation completed")
        
    except Exception as e:
        print(f"[{sub}] ML validation failed: {str(e)}")
        results['ml_validation_error'] = str(e)
    
    return results

def calculate_overall_quality_score(validation_results):
    """Calculate overall NODDI quality score (0-100)"""
    
    score = 0
    max_score = 100
    
    try:
        # Parameter validity (40 points)
        ndi_valid = validation_results.get('ndi_valid_percent', 0)
        odi_valid = validation_results.get('odi_valid_percent', 0)
        
        if ndi_valid >= 90:
            score += 20
        elif ndi_valid >= 75:
            score += 15
        elif ndi_valid >= 60:
            score += 10
        
        if odi_valid >= 90:
            score += 20
        elif odi_valid >= 75:
            score += 15
        elif odi_valid >= 60:
            score += 10
        
        # Acquisition adequacy (30 points)
        if validation_results.get('noddi_adequate', False):
            score += 30
        elif validation_results.get('adequate_high_b', False):
            score += 15
        
        # Fit quality (30 points)
        suspicious_percent = validation_results.get('suspicious_voxels_percent', 100)
        if suspicious_percent < 5:
            score += 30
        elif suspicious_percent < 10:
            score += 20
        elif suspicious_percent < 20:
            score += 10
        
        return min(score, max_score)
        
    except:
        return 0

def save_comprehensive_validation_report(validation_results, sub, work_dir, data_source):
    """Save comprehensive validation report"""
    
    try:
        report_file = os.path.join(work_dir, f'{sub}_noddi_comprehensive_validation.txt')
        
        with open(report_file, 'w') as f:
            f.write(f"Comprehensive NODDI Validation Report for {sub}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Data source: {data_source}\n")
            f.write(f"Overall Quality Score: {validation_results.get('overall_quality_score', 'N/A')}/100\n\n")
            
            # Parameter validation section
            f.write("1. PARAMETER VALIDATION\n")
            f.write("-" * 25 + "\n")
            f.write(f"NDI valid voxels: {validation_results.get('ndi_valid_percent', 'N/A'):.1f}%\n")
            f.write(f"NDI mean ± std: {validation_results.get('ndi_mean', 'N/A'):.3f} ± {validation_results.get('ndi_std', 'N/A'):.3f}\n")
            f.write(f"ODI valid voxels: {validation_results.get('odi_valid_percent', 'N/A'):.1f}%\n")
            f.write(f"ODI mean ± std: {validation_results.get('odi_mean', 'N/A'):.3f} ± {validation_results.get('odi_std', 'N/A'):.3f}\n")
            f.write(f"FWF mean ± std: {validation_results.get('fwf_mean', 'N/A'):.3f} ± {validation_results.get('fwf_std', 'N/A'):.3f}\n\n")
            
            # Model fit quality section
            f.write("2. MODEL FIT QUALITY\n")
            f.write("-" * 20 + "\n")
            f.write(f"Suspicious voxels: {validation_results.get('suspicious_voxels_percent', 'N/A'):.1f}%\n")
            f.write(f"NDI-ODI correlation: {validation_results.get('ndi_odi_correlation', 'N/A'):.3f}\n")
            f.write(f"Extreme NDI voxels: {validation_results.get('extreme_ndi_voxels', 'N/A'):.1f}%\n")
            f.write(f"Extreme ODI voxels: {validation_results.get('extreme_odi_voxels', 'N/A'):.1f}%\n\n")
            
            # Acquisition adequacy section
            f.write("3. ACQUISITION ADEQUACY\n")
            f.write("-" * 23 + "\n")
            f.write(f"Total volumes: {validation_results.get('total_volumes', 'N/A')}\n")
            f.write(f"B-value shells: {validation_results.get('unique_shells', 'N/A')}\n")
            f.write(f"B=0 volumes: {validation_results.get('b0_volumes', 'N/A')}\n")
            f.write(f"Low-b volumes: {validation_results.get('low_b_volumes', 'N/A')}\n")
            f.write(f"High-b volumes: {validation_results.get('high_b_volumes', 'N/A')}\n")
            f.write(f"NODDI adequate: {validation_results.get('noddi_adequate', 'Unknown')}\n\n")
            
            # ML-specific validation if applicable
            if data_source == 'refined':
                f.write("4. ML REFINEMENT VALIDATION\n")
                f.write("-" * 27 + "\n")
                f.write(f"NDI coefficient of variation: {validation_results.get('ndi_cv', 'N/A'):.3f}\n")
                f.write(f"ODI coefficient of variation: {validation_results.get('odi_cv', 'N/A'):.3f}\n")
                f.write("ML-refined data used for enhanced NODDI estimation\n\n")
            
            # Quality assessment
            f.write("5. QUALITY ASSESSMENT\n")
            f.write("-" * 21 + "\n")
            score = validation_results.get('overall_quality_score', 0)
            if score >= 80:
                f.write("Quality Rating: EXCELLENT\n")
                f.write("Recommendation: Proceed with confidence\n")
            elif score >= 65:
                f.write("Quality Rating: GOOD\n")
                f.write("Recommendation: Suitable for most analyses\n")
            elif score >= 50:
                f.write("Quality Rating: ACCEPTABLE\n")
                f.write("Recommendation: Use with caution, verify results\n")
            else:
                f.write("Quality Rating: POOR\n")
                f.write("Recommendation: Manual review required\n")
        
        print(f"[{sub}] Comprehensive validation report saved: {report_file}")
        
    except Exception as e:
        print(f"[{sub}] Failed to save validation report: {str(e)}")

# Main NODDI fitting execution
if __name__ == "__main__":
    if len(sys.argv) < 7:
        print("Usage: python script.py <sub> <work_dir> <dwi_file> <bvec_file> <bval_file> <mask_file> <data_source>")
        sys.exit(1)
    
    sub = sys.argv[1]
    work_dir = sys.argv[2]
    dwi_file = sys.argv[3]
    bvec_file = sys.argv[4]
    bval_file = sys.argv[5]
    mask_file = sys.argv[6]
    data_source = sys.argv[7]
    
    print(f"[{sub}] Starting enhanced NODDI fitting with comprehensive validation")
    print(f"[{sub}] Data source: {data_source}")
    
    try:
        # Generate AMICO scheme file
        scheme_file = os.path.join(work_dir, f'{sub}.scheme')
        util.fsl2scheme(bval_file, bvec_file, scheme_file)
        
        # Initialize AMICO
        ae = amico.Evaluation(work_dir, sub)
        ae.load_data(dwi_filename=dwi_file, scheme_filename=scheme_file, mask_filename=mask_file, b0_thr=20)
        
        # Set NODDI model
        ae.set_model("NODDI")
        ae.generate_kernels(regenerate=True)
        ae.load_kernels()
        
        print(f"[{sub}] Fitting NODDI model...")
        ae.fit()
        
        # Comprehensive validation
        validation_results = comprehensive_noddi_validation(ae, sub, data_source)
        
        # Save results
        ae.save_results()
        
        # Save comprehensive validation report
        save_comprehensive_validation_report(validation_results, sub, work_dir, data_source)
        
        # Final status
        quality_score = validation_results.get('overall_quality_score', 0)
        
        if quality_score >= 65:
            print(f"[{sub}] NODDI fitting completed successfully (Quality: {quality_score}/100)")
        elif quality_score >= 50:
            print(f"[{sub}] NODDI fitting completed with warnings (Quality: {quality_score}/100)")
        else:
            print(f"[{sub}] NODDI fitting completed but quality concerns exist (Quality: {quality_score}/100)")
        
    except Exception as e:
        print(f"[{sub}] NODDI fitting failed: {str(e)}", file=sys.stderr)
        sys.exit(1)
PYTHON_EOF

    chmod +x "$script_path"
    export NODDI_SCRIPT="$script_path"
    log "ML" "[${sub}] Enhanced NODDI script created at: $script_path"
}

process_noddi_results() {
    local sub=$1
    
    # Ensure WORK_DIR is set (fall back to DERIV_DIR if not)
    local base_workdir="${WORK_DIR:-${DERIV_DIR}/work}"
    local workdir="${base_workdir}/${sub}/noddi"   
   
    log "INFO" "[${sub}] Processing NODDI results (workdir: ${workdir})"

    local amico_output="${workdir}/${sub}/AMICO/NODDI"
    
    if [ ! -d "$amico_output" ]; then
        log "ERROR" "[${sub}] AMICO output directory not found: $amico_output"
        return 1
    fi
    
 # Move NODDI parameter maps to external storage
    local files_moved=0
    local expected_files=("NDI" "ODI" "FWF")

    for metric in "${expected_files[@]}"; do
        local source_file="${amico_output}/fit_${metric}.nii.gz"
        local dest_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_$(echo "$metric" | tr '[:upper:]' '[:lower:]').nii.gz"

        if [ -f "$source_file" ]; then
            mkdir -p "$(dirname "$dest_file")"
            if mv "$source_file" "$dest_file"; then
                ((files_moved++))
                log "INFO" "[${sub}] Moved ${metric} parameter map to ${dest_file}"
            else
                log "WARN" "[${sub}] Failed to move ${metric} parameter map"
            fi
        else
            log "WARN" "[${sub}] NODDI output not found: fit_${metric}.nii.gz"
        fi
    done

    if [ $files_moved -eq 0 ]; then
        log "ERROR" "[${sub}] No NODDI output files were successfully moved"
        return 1
    elif [ $files_moved -lt ${#expected_files[@]} ]; then
        log "WARN" "[${sub}] Only ${files_moved}/${#expected_files[@]} NODDI files moved successfully"
        return 0  # Partial success
    else
        log "OK" "[${sub}] All ${files_moved} NODDI parameter maps moved successfully"
        return 0
    fi
}

generate_comprehensive_noddi_qc() {
    local sub=$1
    local data_source=$2
    local workdir="${WORK_DIR:-${DERIV_DIR}/work}/${sub}/noddi"
    local qc_file="${EXTERNAL_QC}/${sub}_noddi_comprehensive_qc.txt"
    local validation_file="${workdir}/${sub}_noddi_comprehensive_validation.txt"
    
    mkdir -p "${EXTERNAL_QC}"
    
    {
        echo "Comprehensive NODDI Analysis Report for ${sub}"
        echo "=============================================="
        echo "Generated on: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        echo ""
        
        echo "PROCESSING CONFIGURATION:"
        echo "========================"
        echo "Data source: $data_source"
        if [ "$data_source" = "refined" ]; then
            echo "  ✓ Using ML-refined DWI data"
            echo "  ✓ Enhanced bias correction applied"
            echo "  ✓ Intensity normalization applied"
            echo "  ✓ Enhanced brain masking applied"
            
            if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
                echo "  ✓ ML registration refinements applied"
                echo "  ML method: ${ML_REGISTRATION_METHOD:-auto}"
            fi
        else
            echo "  → Using standard preprocessed data"
            echo "  → Consider refined data for optimal results"
        fi
        echo ""
        
        # Include comprehensive validation results
        if [ -f "$validation_file" ]; then
            echo "COMPREHENSIVE VALIDATION RESULTS:"
            echo "================================"
            cat "$validation_file"
            echo ""
            
            # Copy validation file to external QC storage
            cp "$validation_file" "${EXTERNAL_QC}/${sub}_noddi_validation.txt"
        else
            echo "VALIDATION RESULTS: Validation file not found"
            echo ""
        fi
        
        # Parameter statistics with enhanced analysis
        echo "ENHANCED PARAMETER STATISTICS:"
        echo "============================="
        
        # Determine mask to use for statistics
        local mask_file="${EXTERNAL_POSTHOC}/${sub}/${sub}_mask_enhanced.nii.gz"
        if [ ! -f "$mask_file" ]; then
            mask_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_mask.nii.gz"
        fi
        
        for metric in ndi odi fwf; do
            local param_file="${EXTERNAL_MRTRIX}/${sub}/${sub}_${metric}.nii.gz"
            
            if [ -f "$param_file" ]; then
                echo ""
                echo "${metric^^} ($([ "$metric" = "ndi" ] && echo "Neurite Density Index" || [ "$metric" = "odi" ] && echo "Orientation Dispersion Index" || echo "Free Water Fraction")):"
                echo "$(printf '%*s' ${#metric} '')$([ "$metric" = "ndi" ] && echo "                         " || [ "$metric" = "odi" ] && echo "                              " || echo "                  ")"
                
                if [ -f "$mask_file" ]; then
                    # Basic statistics
                    mrstats "$param_file" -mask "$mask_file" 2>/dev/null || echo "  Error computing basic statistics"
                    
                    # Enhanced range analysis with Python
                    $PYTHON_EXECUTABLE -c "
import nibabel as nib
import numpy as np
try:
    data = nib.load('$param_file').get_fdata()
    mask = nib.load('$mask_file').get_fdata()
    
    masked_data = data[mask > 0]
    
    if len(masked_data) > 0:
        # Percentile analysis
        p5, p25, p50, p75, p95 = np.percentile(masked_data, [5, 25, 50, 75, 95])
        
        print(f'  Percentiles: 5th={p5:.3f}, 25th={p25:.3f}, 50th={p50:.3f}, 75th={p75:.3f}, 95th={p95:.3f}')
        
        # Range validation for specific parameters
        if '$metric' == 'ndi':
            outside_range = np.sum((masked_data < 0) | (masked_data > 1)) / len(masked_data) * 100
            print(f'  Values outside [0,1]: {outside_range:.1f}%')
            
            # Tissue-specific ranges
            wm_like = np.sum((masked_data >= 0.3) & (masked_data <= 0.8)) / len(masked_data) * 100
            print(f'  White matter-like values [0.3-0.8]: {wm_like:.1f}%')
            
        elif '$metric' == 'odi':
            outside_range = np.sum((masked_data < 0) | (masked_data > 1)) / len(masked_data) * 100
            print(f'  Values outside [0,1]: {outside_range:.1f}%')
            
            low_disp = np.sum(masked_data <= 0.3) / len(masked_data) * 100
            high_disp = np.sum(masked_data >= 0.7) / len(masked_data) * 100
            print(f'  Low dispersion [0-0.3]: {low_disp:.1f}%, High dispersion [0.7-1]: {high_disp:.1f}%')
            
        elif '$metric' == 'fwf':
            outside_range = np.sum((masked_data < 0) | (masked_data > 1)) / len(masked_data) * 100
            print(f'  Values outside [0,1]: {outside_range:.1f}%')
            
            csf_like = np.sum(masked_data >= 0.8) / len(masked_data) * 100
            print(f'  CSF-like values [0.8-1]: {csf_like:.1f}%')
    else:
        print('  No valid data in mask')
        
except Exception as e:
    print(f'  Enhanced analysis failed: {e}')
" 2>/dev/null || echo "  Enhanced analysis not available"
                else
                    echo "  No mask available for statistics"
                fi
            else
                echo ""
                echo "${metric^^}: Parameter file not found"
            fi
        done
        
        # ML-specific analysis if applicable
        if [ "$data_source" = "refined" ]; then
            echo ""
            echo "ML INTEGRATION IMPACT ANALYSIS:"
            echo "==============================="
            echo "Benefits of using ML-refined data for NODDI:"
            echo "  ✓ Reduced motion artifacts → More accurate parameter estimation"
            echo "  ✓ Enhanced bias correction → Improved signal uniformity"
            echo "  ✓ Better brain masking → More reliable tissue modeling"
            
            if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
                echo "  ✓ ML registration → Better cross-modal alignment"
            fi
            
            # Compare with basic data if available
            local basic_ndi="${EXTERNAL_MRTRIX}/${sub}/${sub}_ndi_basic.nii.gz"
            if [ -f "$basic_ndi" ]; then
                echo ""
                echo "Comparison with basic preprocessing (if available):"
                echo "  → Check parameter map smoothness and consistency"
                echo "  → ML-refined data typically shows reduced noise"
            fi
        fi
        
        # Processing performance summary
        echo ""
        echo "PROCESSING PERFORMANCE:"
        echo "======================"
        
        if [ -f "${workdir}/noddi_comprehensive_log.txt" ]; then
            # Extract timing information from log
            local fitting_time=$(grep -o "fitting.*completed.*minutes" "${workdir}/noddi_comprehensive_log.txt" | tail -1 || echo "Time not recorded")
            echo "NODDI fitting time: $fitting_time"
        fi
        
        # Resource usage if available
        if [ -f "${LOG_DIR}/resource_monitor.log" ]; then
            local noddi_entries=$(grep -c "noddi" "${LOG_DIR}/resource_monitor.log" 2>/dev/null || echo "0")
            echo "Resource monitoring entries: $noddi_entries"
        fi
        
        # Quality-based recommendations
        echo ""
        echo "RECOMMENDATIONS:"
        echo "==============="
        
        # Extract quality score from validation
        local quality_score="N/A"
        if [ -f "$validation_file" ]; then
            quality_score=$(grep "Overall Quality Score:" "$validation_file" | cut -d: -f2 | awk '{print $1}' | cut -d/ -f1 2>/dev/null || echo "N/A")
        fi
        
        if [ "$quality_score" != "N/A" ]; then
            echo "Overall Quality Score: $quality_score/100"
            
            if (( $(echo "$quality_score >= 80" | bc -l 2>/dev/null || echo 0) )); then
                echo "✓ EXCELLENT quality - Proceed with microstructure analysis with confidence"
                echo "✅ Recommended for publication-quality research"
                echo "✅ Suitable for clinical applications"
            elif (( $(echo "$quality_score >= 65" | bc -l 2>/dev/null || echo 0) )); then
                                echo "✓ GOOD quality - Suitable for most research applications"
                echo "✅ Appropriate for group-level analyses"
                echo "→ Consider additional quality checks for critical applications"
            elif (( $(echo "$quality_score >= 50" | bc -l 2>/dev/null || echo 0) )); then
                echo "⚠ ACCEPTABLE quality - Use with caution"
                echo "→ Verify results with visual inspection"
                echo "→ Consider exclusion criteria for group analyses"
                echo "→ May be suitable for exploratory analyses"
            else
                echo "⚠ POOR quality - Manual review strongly recommended"
                echo "→ Check acquisition parameters and preprocessing quality"
                echo "→ Consider reprocessing with different parameters"
                echo "→ May not be suitable for quantitative analysis"
            fi
        else
            echo "Quality score: Not available"
            echo "→ Manual review of parameter maps recommended"
        fi
        
        # Data source specific recommendations
        echo ""
        if [ "$data_source" = "refined" ]; then
            echo "✅ Using optimal data source (ML-refined)"
            echo "→ Parameter maps should show improved quality vs. basic preprocessing"
        else
            echo "💡 OPTIMIZATION SUGGESTION:"
            echo "→ Consider using refined DWI data for improved NODDI estimation"
            echo "→ Run: --posthoc-refinement flag for enhanced preprocessing"
            if [ "${USE_ML_REGISTRATION:-false}" = false ]; then
                echo "→ Enable ML registration: --use-ml-registration for better accuracy"
            fi
        fi
        
        # Technical recommendations
        echo ""
        echo "TECHNICAL CONSIDERATIONS:"
        echo "========================"
        
        # Check acquisition adequacy from validation
        if [ -f "$validation_file" ] && grep -q "NODDI adequate: False" "$validation_file"; then
            echo "⚠ ACQUISITION LIMITATION DETECTED:"
            echo "→ DWI acquisition may not be optimal for NODDI"
            echo "→ Results should be interpreted with caution"
            echo "→ Consider multi-shell acquisition for future scans"
        else
            echo "✅ Acquisition appears adequate for NODDI modeling"
        fi
        
        # Parameter interpretation guidance
        echo ""
        echo "PARAMETER INTERPRETATION GUIDE:"
        echo "==============================="
        echo "NDI (Neurite Density Index):"
        echo "  • Range: 0-1 (0 = no neurites, 1 = maximum density)"
        echo "  • Typical WM values: 0.3-0.8"
        echo "  • Higher values indicate greater axonal/dendritic density"
        echo ""
        echo "ODI (Orientation Dispersion Index):"
        echo "  • Range: 0-1 (0 = perfectly aligned, 1 = isotropic)"
        echo "  • Corpus callosum: ~0.1-0.3 (low dispersion)"
        echo "  • Cortical GM: ~0.7-1.0 (high dispersion)"
        echo ""
        echo "FWF (Free Water Fraction):"
        echo "  • Range: 0-1 (proportion of free water)"
        echo "  • CSF regions: ~0.9-1.0"
        echo "  • Healthy WM: typically <0.3"
        echo "  • Elevated values may indicate pathology or partial volume"
        
        echo ""
        echo "NEXT STEPS:"
        echo "=========="
        echo "1. Visual inspection of parameter maps"
        echo "2. ROI-based analysis using anatomical atlases"
        echo "3. Statistical comparison across groups/conditions"
        echo "4. Correlation with other imaging modalities"
        echo "5. Consider tract-specific analysis using tractography"
        
        if [ "$quality_score" != "N/A" ] && (( $(echo "$quality_score < 65" | bc -l 2>/dev/null || echo 0) )); then
            echo ""
            echo "QUALITY IMPROVEMENT SUGGESTIONS:"
            echo "==============================="
            echo "For future processing consider:"
            echo "→ Enable ML registration (--use-ml-registration)"
            echo "→ Use post-hoc refinement (automatic in this pipeline)"
            echo "→ Verify acquisition parameters match NODDI requirements"
            echo "→ Check for motion artifacts in raw data"
        fi
        
        echo ""
        echo "OUTPUT FILES:"
        echo "============"
        echo "Parameter maps (in ${EXTERNAL_MRTRIX}/):"
        echo "  • ${sub}_ndi.nii.gz - Neurite Density Index"
        echo "  • ${sub}_odi.nii.gz - Orientation Dispersion Index" 
        echo "  • ${sub}_fwf.nii.gz - Free Water Fraction"
        echo ""
        echo "Quality control files (in ${EXTERNAL_QC}/):"
        echo "  • ${sub}_noddi_comprehensive_qc.txt - This report"
        echo "  • ${sub}_noddi_validation.txt - Detailed validation metrics"
        
        echo ""
        echo "Report completed: $(date)"
        echo "Pipeline version: $SCRIPT_VERSION"
        
    } > "$qc_file"
    
    log "OK" "[${sub}] Comprehensive NODDI QC report generated: $qc_file"
}

analyze_noddi_failure() {
    local sub=$1
    local workdir="${WORK_DIR:-${DERIV_DIR}/work}/${sub}/noddi"
    local log_file="${workdir}/noddi_comprehensive_log.txt"
    
    if [ ! -f "$log_file" ]; then
        return 1
    fi
    
    log "INFO" "[${sub}] Analyzing NODDI failure"
    
    # Common failure patterns
    local error_patterns=(
        "out of memory|memory.*error|killed"
        "scheme.*error|gradient.*error"
        "mask.*error|empty.*mask"
        "fitting.*failed|convergence.*failed"
        "amico.*error|import.*error"
    )
    
    local error_messages=(
        "Memory exhaustion"
        "Gradient scheme error"
        "Brain mask error"
        "Model fitting convergence failure"
        "AMICO/Python environment error"
    )
    
    # Check for specific errors
    for i in "${!error_patterns[@]}"; do
        if grep -qi "${error_patterns[$i]}" "$log_file"; then
            log "WARN" "[${sub}] Detected: ${error_messages[$i]}"
            
            case $i in
                0) log "INFO" "[${sub}] Suggestion: Reduce memory usage or increase available RAM";;
                1) log "INFO" "[${sub}] Suggestion: Check DWI gradient files (bvec/bval)";;
                2) log "INFO" "[${sub}] Suggestion: Verify brain mask quality";;
                3) log "INFO" "[${sub}] Suggestion: Check DWI data quality and acquisition parameters";;
                4) log "INFO" "[${sub}] Suggestion: Verify AMICO installation and Python environment";;
            esac
        fi
    done
    
    # Generate failure report
    local failure_report="${EXTERNAL_QC}/${sub}_noddi_failure_analysis.txt"
    {
        echo "NODDI Failure Analysis for ${sub}"
        echo "================================="
        echo "Generated: $(date)"
        echo ""
        echo "Failure Analysis:"
        
        for i in "${!error_patterns[@]}"; do
            if grep -qi "${error_patterns[$i]}" "$log_file"; then
                echo "- ${error_messages[$i]}: DETECTED"
            fi
        done
        
        echo ""
        echo "Last 50 lines of NODDI log:"
        tail -n 50 "$log_file"
        
    } > "$failure_report"
    
    log "INFO" "[${sub}] NODDI failure analysis saved: $failure_report"
}

# --- Main Processing Function ---
process_subject() {
    local sub=$1
    local start_time=$(date +%s)
    
    log "INFO" "=========================================="
    log "INFO" "Processing subject: ${sub}"
    log "INFO" "=========================================="
    
    # --- Dry-run mode ---
    if [ "$DRY_RUN" = true ]; then
        log "INFO" "[DRY-RUN] Subject $sub — checking what would run:"
        
        if ! check_checkpoint "$sub" "preprocessing_complete"; then
            dry_run_skip "Stage 1 (synb0 + preprocessing + eddy)" "$sub"
        else
            log "INFO" "[DRY-RUN] Stage 1: already checkpointed — would skip"
        fi
        if ! check_checkpoint "$sub" "posthoc_complete"; then
            dry_run_skip "Stage 2 (post-hoc refinement)" "$sub"
        else
            log "INFO" "[DRY-RUN] Stage 2: already checkpointed — would skip"
        fi
        if [ "$RUN_CONNECTOME" = true ] && ! check_checkpoint "$sub" "connectivity_complete"; then
            dry_run_skip "Stage 3 (connectivity)" "$sub"
        else
            log "INFO" "[DRY-RUN] Stage 3: checkpointed or skipped"
        fi
        if ! check_checkpoint "$sub" "noddi_complete"; then
            dry_run_skip "Stage 4 (NODDI)" "$sub"
        else
            log "INFO" "[DRY-RUN] Stage 4: already checkpointed — would skip"
        fi
        return 0
    fi
    
    # --- Acquire lock (prevents duplicate processing) ---
    if ! acquire_subject_lock "$sub"; then
        return 1
    fi
    # Ensure lock is released on any exit path
    trap 'release_subject_lock' RETURN
    
    # Verify required input files
    if [ ! -f "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.nii.gz" ]; then
        log "ERROR" "[${sub}] DWI data not found"
        return 1
    fi
    
    if [ ! -f "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bvec" ] || [ ! -f "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.bval" ]; then
        log "ERROR" "[${sub}] DWI gradient files (bvec/bval) not found"
        return 1
    fi
    
    # Pre-processing disk check
    if ! check_disk_before_stage "$sub" "pipeline_start" 50; then return 1; fi
    
    # ---- Stage 1: Basic preprocessing with ML enhancements ----
    if ! check_checkpoint "$sub" "preprocessing_complete"; then
        log "STAGE" "[${sub}] Stage 1: Basic preprocessing with ML integration"
        
        # Synb0 (advisory — failure is non-fatal)
        if ! check_checkpoint "$sub" "synb0_complete"; then
            if ! timed_stage "synb0_${sub}" run_synb0 "$sub"; then
                log "WARN" "[${sub}] Synb0-DisCo failed, continuing without distortion correction"
            fi
        fi
        
        # Basic preprocessing (fatal)
        if ! check_checkpoint "$sub" "basic_preproc_complete"; then
            check_disk_before_stage "$sub" "basic_preprocessing" 30 || return 1
            if ! timed_stage "preproc_${sub}" run_basic_preprocessing "$sub"; then
                log "ERROR" "[${sub}] Basic preprocessing failed"
                return 1
            fi
            create_checkpoint "$sub" "basic_preproc_complete"
        fi
        
        # Eddy + bias correction (fatal)
        if ! check_checkpoint "$sub" "eddy_complete"; then
            check_disk_before_stage "$sub" "eddy_correction" 30 || return 1
            if ! timed_stage "eddy_${sub}" run_eddy_and_bias_correction "$sub"; then
                log "ERROR" "[${sub}] Motion/distortion correction failed"
                return 1
            fi
            create_checkpoint "$sub" "eddy_complete"
        fi
    else
        log "INFO" "[${sub}] Stage 1 already completed"
    fi
    
    # ---- Stage 2: Post-hoc refinement (advisory) ----
    if ! check_checkpoint "$sub" "posthoc_complete"; then
        log "STAGE" "[${sub}] Stage 2: Post-hoc refinement with ML enhancements"
        check_disk_before_stage "$sub" "posthoc_refinement" 20 || return 1
        
        if ! timed_stage "posthoc_${sub}" run_posthoc_refinement "$sub"; then
            log "WARN" "[${sub}] Post-hoc refinement failed, continuing with basic preprocessing"
        else
            generate_posthoc_summary "$sub"
        fi
    else
        log "INFO" "[${sub}] Stage 2 already completed"
    fi
    
    # ---- Stage 3: Connectivity analysis (advisory) ----
    if [ "$RUN_CONNECTOME" = true ] && ! check_checkpoint "$sub" "connectivity_complete"; then
        log "STAGE" "[${sub}] Stage 3: Connectivity analysis with ML integration"
        check_disk_before_stage "$sub" "connectivity" 40 || return 1
        
        if ! timed_stage "connectivity_${sub}" run_connectivity_analysis "$sub"; then
            log "WARN" "[${sub}] Connectivity analysis failed"
        else
            create_connectivity_checkpoint "$sub"
        fi
    elif [ "$RUN_CONNECTOME" = true ]; then
        log "INFO" "[${sub}] Stage 3 already completed"
    else
        log "INFO" "[${sub}] Stage 3: Skipping connectivity analysis (--skip-connectome)"
    fi
    
    # ---- Stage 4: NODDI estimation (advisory) ----
    if ! check_checkpoint "$sub" "noddi_complete"; then
        log "STAGE" "[${sub}] Stage 4: NODDI estimation with ML data integration"
        check_disk_before_stage "$sub" "noddi" 20 || return 1
        
        if ! timed_stage "noddi_${sub}" run_noddi_estimation "$sub"; then
            log "WARN" "[${sub}] NODDI estimation failed"
        fi
    else
        log "INFO" "[${sub}] Stage 4 already completed"
    fi
    
    # Final cleanup and reporting
    cleanup_work_dir "$sub"
    
    # Calculate total processing time
    local end_time=$(date +%s)
    local total_time=$((end_time - start_time))
    local hours=$((total_time / 3600))
    local minutes=$(( (total_time % 3600) / 60 ))
    
    log "OK" "[${sub}] All processing completed in ${hours}h ${minutes}m"
    
    # Final system resource check
    check_disk_space "$(dirname "$BIDS_DIR")" 20
    monitor_resources "$sub" "processing_complete"
    
    return 0
}


# ============================================================================
# Generate HTML summary report with per-subject status
# ============================================================================
_generate_html_report() {
    local txt_report=$1
    local n_total=$2
    local n_successful=$3
    local n_failed=$4
    local total_hours=$5
    local total_minutes=$6
    local failed_list=$7
    
    local html_report="${EXTERNAL_QC}/pipeline_report.html"
    local success_rate=$(( n_total > 0 ? n_successful * 100 / n_total : 0 ))
    
    cat > "$html_report" << 'HTMLEOF'
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>DTI Pipeline Report</title>
<style>
body{font-family:-apple-system,BlinkMacSystemFont,sans-serif;max-width:1100px;margin:0 auto;padding:20px;background:#f5f6fa;color:#2d3436}
h1{color:#2d3436;border-bottom:3px solid #0984e3;padding-bottom:10px}
h2{color:#636e72;margin-top:30px}
.card{background:#fff;border-radius:8px;padding:20px;margin:15px 0;box-shadow:0 2px 6px rgba(0,0,0,.08)}
.stat-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:15px}
.stat{text-align:center;padding:20px}
.stat .value{font-size:2.5em;font-weight:700}
.stat .label{color:#636e72;font-size:.9em}
.success{color:#00b894} .fail{color:#d63031} .warn{color:#fdcb6e;color:#e17055}
table{width:100%;border-collapse:collapse;margin:10px 0}
th,td{padding:10px 14px;text-align:left;border-bottom:1px solid #dfe6e9}
th{background:#f8f9fa;font-weight:600}
.badge{display:inline-block;padding:3px 10px;border-radius:12px;font-size:.8em;font-weight:600}
.badge-ok{background:#00b89422;color:#00b894} .badge-fail{background:#d6303122;color:#d63031}
.badge-skip{background:#636e7222;color:#636e72}
.ts{color:#b2bec3;font-size:.85em}
</style>
</head>
<body>
HTMLEOF

    {
        echo "<h1>DTI Processing Pipeline Report</h1>"
        echo "<p class='ts'>Generated: $(date) &mdash; Pipeline v${SCRIPT_VERSION}</p>"
        
        # Summary cards
        echo "<div class='card'><div class='stat-grid'>"
        echo "<div class='stat'><div class='value'>${n_total}</div><div class='label'>Total Subjects</div></div>"
        echo "<div class='stat'><div class='value success'>${n_successful}</div><div class='label'>Successful</div></div>"
        echo "<div class='stat'><div class='value fail'>${n_failed}</div><div class='label'>Failed</div></div>"
        echo "<div class='stat'><div class='value'>${success_rate}%</div><div class='label'>Success Rate</div></div>"
        echo "<div class='stat'><div class='value'>${total_hours}h ${total_minutes}m</div><div class='label'>Total Time</div></div>"
        echo "</div></div>"
        
        # Configuration
        echo "<div class='card'><h2>Configuration</h2>"
        echo "<table>"
        echo "<tr><td>ML Registration</td><td>${USE_ML_REGISTRATION:-false}</td></tr>"
        echo "<tr><td>ML Method</td><td>${ML_REGISTRATION_METHOD:-auto}</td></tr>"
        echo "<tr><td>GPU Available</td><td>${GPU_AVAILABLE:-false}</td></tr>"
        echo "<tr><td>Connectivity</td><td>${RUN_CONNECTOME}</td></tr>"
        echo "<tr><td>Threads</td><td>${OMP_NUM_THREADS}</td></tr>"
        echo "</table></div>"
        
        # Per-subject table
        echo "<div class='card'><h2>Per-Subject Status</h2>"
        echo "<table><tr><th>Subject</th><th>Stage 1</th><th>Stage 2</th><th>Stage 3</th><th>Stage 4</th><th>Overall</th></tr>"
        
        for sub_dir in "${BIDS_DIR}"/sub-*; do
            [ -d "$sub_dir" ] || continue
            local sub
            sub=$(basename "$sub_dir")
            
            local s1="skip" s2="skip" s3="skip" s4="skip" overall="skip"
            
            if check_checkpoint "$sub" "preprocessing_complete" 2>/dev/null || \
               check_checkpoint "$sub" "eddy_complete" 2>/dev/null; then
                s1="ok"
            fi
            if check_checkpoint "$sub" "posthoc_complete" 2>/dev/null; then
                s2="ok"
            fi
            if check_checkpoint "$sub" "connectivity_complete" 2>/dev/null; then
                s3="ok"
            elif [ "$RUN_CONNECTOME" != true ]; then
                s3="skip"
            fi
            if check_checkpoint "$sub" "noddi_complete" 2>/dev/null; then
                s4="ok"
            fi
            
            # Check if in failed list
            if echo " $failed_list " | grep -q " $sub "; then
                overall="fail"
            elif [ "$s1" = "ok" ]; then
                overall="ok"
            fi
            
            local badge_s1 badge_s2 badge_s3 badge_s4 badge_all
            for stage_var in s1 s2 s3 s4 overall; do
                local val="${!stage_var}"
                case "$val" in
                    ok)   eval "badge_${stage_var}=\"<span class='badge badge-ok'>PASS</span>\"" ;;
                    fail) eval "badge_${stage_var}=\"<span class='badge badge-fail'>FAIL</span>\"" ;;
                    *)    eval "badge_${stage_var}=\"<span class='badge badge-skip'>—</span>\"" ;;
                esac
            done
            
            echo "<tr><td><strong>$sub</strong></td><td>$badge_s1</td><td>$badge_s2</td><td>$badge_s3</td><td>$badge_s4</td><td>$badge_overall</td></tr>"
        done
        
        echo "</table></div>"
        
        # Timing from JSONL log
        if [ -f "${JSONL_LOG}" ]; then
            echo "<div class='card'><h2>Stage Timing</h2><table><tr><th>Stage</th><th>Duration</th></tr>"
            grep '"TIMING"' "$JSONL_LOG" 2>/dev/null | while IFS= read -r line; do
                local stage_name duration_s
                stage_name=$(echo "$line" | grep -o '"stage":"[^"]*"' | cut -d'"' -f4)
                duration_s=$(echo "$line" | grep -o '"duration_s":[0-9]*' | cut -d: -f2)
                if [ -n "$stage_name" ] && [ -n "$duration_s" ]; then
                    echo "<tr><td>${stage_name}</td><td>${duration_s}s ($((duration_s/60))m)</td></tr>"
                fi
            done
            echo "</table></div>"
        fi
        
        # Output locations
        echo "<div class='card'><h2>Output Locations</h2>"
        echo "<table>"
        echo "<tr><td>DTI Metrics</td><td><code>${EXTERNAL_MRTRIX}/</code></td></tr>"
        echo "<tr><td>NODDI Parameters</td><td><code>${EXTERNAL_MRTRIX}/</code></td></tr>"
        echo "<tr><td>FreeSurfer</td><td><code>${EXTERNAL_FS}/</code></td></tr>"
        echo "<tr><td>QC Reports</td><td><code>${EXTERNAL_QC}/</code></td></tr>"
        echo "<tr><td>Event Log</td><td><code>${JSONL_LOG:-N/A}</code></td></tr>"
        echo "</table></div>"
        
        echo "</body></html>"
    } >> "$html_report"
    
    log "INFO" "HTML summary report: $html_report"
}

# --- Main Execution ---
main() {
    local pipeline_start_time=$(date +%s)
    
    # Parse arguments and setup environment
    parse_arguments "$@" || exit 1
    setup_environment || exit 1
    
    log "INFO" "================================================="
    log "INFO" "Integrated DTI Processing Pipeline v${SCRIPT_VERSION}"
    log "INFO" "ML-Enhanced Edition with Comprehensive Validation"
    log "INFO" "================================================="

    # Display processing configuration summary
    log "INFO" ""
    log "INFO" "Processing Configuration Summary:"
    log "INFO" "  ML Registration: ${USE_ML_REGISTRATION:-false}"
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "INFO" "    Method: ${ML_REGISTRATION_METHOD:-auto}"
        log "INFO" "    GPU Available: ${GPU_AVAILABLE:-false}"
        log "INFO" "    Quick Mode: ${ML_QUICK_MODE:-true}"
    fi
    log "INFO" "  Connectivity Analysis: $RUN_CONNECTOME"
    log "INFO" "  Post-hoc Refinement: Enabled"
    log "INFO" "  NODDI Estimation: Enabled"
    log "INFO" "  Cleanup: $CLEANUP"
    log "INFO" "  Threads: $OMP_NUM_THREADS"

    # NOTE: Parallel processing (GNU parallel / xargs -P) is not currently used.
    # If you enable it in the future, these functions rely on dynamic scoping
    # (shared globals like WORK_DIR, BIDS_DIR, etc.) and will NOT work correctly
    # via export -f into subshells. You would need to refactor them to accept
    # all context via arguments or environment variables first.
    
    # Determine subjects to process
    local subjects=()
    if [ -n "$SINGLE_SUBJECT" ]; then
        subjects=("$SINGLE_SUBJECT")
    else
        mapfile -t subjects < <(find "$BIDS_DIR" -maxdepth 1 -name "sub-*" -type d -exec basename {} \; | sort)
    fi
    
    local n_total=${#subjects[@]}
    
    if [ $n_total -eq 0 ]; then
        log "ERROR" "No subjects found in $BIDS_DIR"
        exit 1
    fi
    
    log "INFO" "Found ${n_total} subject(s) to process"
    
    # Pre-flight input validation (runs on all subjects before processing begins)
    validate_all_inputs "${subjects[@]}"
    
    # Dry-run banner
    if [ "$DRY_RUN" = true ]; then
        log "INFO" ""
        log "INFO" "================================================="
        log "INFO" "DRY-RUN MODE — no data will be processed"
        log "INFO" "================================================="
    fi
    
    # Process subjects sequentially
    local n_processed=0
    local n_successful=0
    local n_failed=0
    local failed_subjects=()
    
    for sub in "${subjects[@]}"; do
        n_processed=$((n_processed + 1))
        
        log "INFO" "=========================================="
        log "INFO" "Subject ${n_processed}/${n_total}: ${sub}"
        log "INFO" "=========================================="
        
        # Pre-flight checks
        if [ ! -f "${BIDS_DIR}/${sub}/dwi/${sub}_dwi.nii.gz" ]; then
            log "ERROR" "[${sub}] DWI data not found"
            failed_subjects+=("$sub")
            n_failed=$((n_failed + 1))
            continue
        fi
        
        # Check available space before processing
        if ! check_disk_space "$(dirname "$BIDS_DIR")" 50; then
            log "ERROR" "[${sub}] Insufficient disk space (need 50GB free)"
            failed_subjects+=("$sub")
            n_failed=$((n_failed + 1))
            continue
        fi
        
        # Process the subject
        local subject_start_time=$(date +%s)
        
        if process_subject "$sub"; then
            local subject_end_time=$(date +%s)
            local subject_duration=$((subject_end_time - subject_start_time))
            local subject_hours=$((subject_duration / 3600))
            local subject_minutes=$(( (subject_duration % 3600) / 60 ))
            
            n_successful=$((n_successful + 1))
            log "OK" "[${sub}] Processing completed successfully in ${subject_hours}h ${subject_minutes}m"
        else
            failed_subjects+=("$sub")
            n_failed=$((n_failed + 1))
            log "ERROR" "[${sub}] Processing failed"
        fi
        
        # Brief pause between subjects for system stability
        if [ $n_processed -lt $n_total ]; then
            log "INFO" "Pausing briefly before next subject..."
            sleep 3
        fi
        
        # Intermediate cleanup if enabled
        if [ "$CLEANUP" = true ]; then
            cleanup_aggressive "$sub"
        fi
    done
    
    # Calculate total pipeline duration
    local pipeline_end_time=$(date +%s)
    local total_duration=$((pipeline_end_time - pipeline_start_time))
    local total_hours=$((total_duration / 3600))
    local total_minutes=$(( (total_duration % 3600) / 60 ))
    
    # Generate comprehensive pipeline summary
    log "INFO" "================================================="
    log "INFO" "PIPELINE COMPLETION SUMMARY"
    log "INFO" "================================================="
    log "INFO" "Processing Results:"
    log "INFO" "  Total subjects: ${n_total}"
    log "INFO" "  Successful: ${n_successful}"
    log "INFO" "  Failed: ${n_failed}"
    log "INFO" "  Success rate: $(( n_total > 0 ? n_successful * 100 / n_total : 0 ))%"
    log "INFO" "  Total processing time: ${total_hours}h ${total_minutes}m"
    
    if [ $n_successful -gt 0 ]; then
        local avg_time_per_subject=$((total_duration / n_successful))
        local avg_hours=$((avg_time_per_subject / 3600))
        local avg_minutes=$(( (avg_time_per_subject % 3600) / 60 ))
        log "INFO" "  Average time per successful subject: ${avg_hours}h ${avg_minutes}m"
    fi
    
    # Stage completion counts (always computed so final report can reference them)
    local stage1_count=$(find "${EXTERNAL_MRTRIX}" -name "*_dwi_preproc.nii.gz" 2>/dev/null | wc -l)
    local stage2_count=$(find "${EXTERNAL_POSTHOC}" -name "*_dwi_refined.nii.gz" 2>/dev/null | wc -l)
    local stage3_count=$(find "${EXTERNAL_FS}" -name "scripts" -type d 2>/dev/null | wc -l)
    local stage4_count=$(find "${EXTERNAL_MRTRIX}" -name "*_ndi.nii.gz" 2>/dev/null | wc -l)
    
    # Stage completion summary
    if [ $n_successful -gt 0 ]; then
        log "INFO" ""
        log "INFO" "Stage Completion Analysis:"
        
        log "INFO" "  Stage 1 (Basic preprocessing): ${stage1_count}/${n_total} subjects"
        log "INFO" "  Stage 2 (Post-hoc refinement): ${stage2_count}/${n_total} subjects"
        
        if [ "$RUN_CONNECTOME" = true ]; then
            log "INFO" "  Stage 3 (Connectivity analysis): ${stage3_count}/${n_total} subjects"
        else
            log "INFO" "  Stage 3 (Connectivity analysis): Skipped by user"
        fi
        
        log "INFO" "  Stage 4 (NODDI estimation): ${stage4_count}/${n_total} subjects"
        
        # ML integration summary
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            log "INFO" ""
            log "INFO" "ML Integration Results:"
            
            local ml_reports=$(find "${EXTERNAL_QC}" -name "*_ml_registration_report.txt" 2>/dev/null | wc -l)
            local synthmorph_reports=$(find "${EXTERNAL_QC}" -name "*_synthmorph_quality.txt" 2>/dev/null | wc -l)
            local ants_reports=$(find "${EXTERNAL_QC}" -name "*_ants_quality.txt" 2>/dev/null | wc -l)
            
            log "INFO" "  ML registration attempts: $ml_reports subjects"
            log "INFO" "  SynthMorph registrations: $synthmorph_reports subjects"
            log "INFO" "  Enhanced ANTs registrations: $ants_reports subjects"
            log "INFO" "  Method: ${ML_REGISTRATION_METHOD:-auto}"
            log "INFO" "  GPU utilized: ${GPU_AVAILABLE:-false}"
        fi
        
        # Quality summary
        log "INFO" ""
        log "INFO" "Quality Assessment Summary:"
        
        local noddi_validations=$(find "${EXTERNAL_QC}" -name "*_noddi_validation.txt" 2>/dev/null | wc -l)
        local connectivity_reports=$(find "${EXTERNAL_QC}" -name "*_connectivity_comprehensive.txt" 2>/dev/null | wc -l)
        
        log "INFO" "  NODDI validations: ${noddi_validations} subjects"
        log "INFO" "  Connectivity QC reports: ${connectivity_reports} subjects"
        
        # High-quality results count
        local excellent_noddi=0
        local good_noddi=0
        
        for validation_file in "${EXTERNAL_QC}"/*_noddi_validation.txt; do
            if [ -f "$validation_file" ]; then
                if grep -q "Quality.*EXCELLENT" "$validation_file"; then
                    ((excellent_noddi++))
                elif grep -q "Quality.*GOOD" "$validation_file"; then
                    ((good_noddi++))
                fi
            fi
        done
        
        if [ $noddi_validations -gt 0 ]; then
            log "INFO" "  NODDI Excellent quality: ${excellent_noddi} subjects"
            log "INFO" "  NODDI Good quality: ${good_noddi} subjects"
        fi
    fi
    
    # Failed subjects analysis
    if [ $n_failed -gt 0 ]; then
        log "INFO" ""
        log "WARN" "Failed Subjects Analysis:"
        log "WARN" "  Failed subjects: ${failed_subjects[*]}"
        log "WARN" "  Check individual logs in: ${LOG_DIR}/"
        log "WARN" "  Common failure points:"
        log "WARN" "    - Insufficient disk space"
        log "WARN" "    - Missing or corrupted input files"
        log "WARN" "    - FreeSurfer reconstruction failures"
        log "WARN" "    - NODDI fitting convergence issues"
        log "WARN" "    - ML dependency issues"
        
        # Generate failure summary report
        local failure_summary="${EXTERNAL_QC}/pipeline_failure_summary.txt"
        {
            echo "Pipeline Failure Summary"
            echo "======================="
            echo "Generated: $(date)"
            echo "Pipeline version: $SCRIPT_VERSION"
            echo ""
            echo "Failed subjects: ${failed_subjects[*]}"
            echo "Total failed: $n_failed out of $n_total"
            echo ""
            echo "For troubleshooting, check:"
            echo "1. Individual subject logs in: ${LOG_DIR}/"
            echo "2. Disk space availability"
            echo "3. Input data integrity"
            echo "4. System dependencies"
            
            if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
                echo "5. ML registration dependencies"
            fi
            
        } > "$failure_summary"
        
        log "INFO" "  Failure summary saved: $failure_summary"
    fi
    
    # Storage utilization report
    log "INFO" ""
    log "INFO" "Storage Utilization:"
    
    local c_used="N/A"
    local e_used="N/A" 
    local f_used="N/A"
    
    if command -v du &>/dev/null; then
        # Current BIDS drive usage
        if [ -d "${DERIV_DIR}" ]; then
            c_used=$(du -sh "${DERIV_DIR}" 2>/dev/null | cut -f1 || echo "N/A")
        fi
        
        # External storage usage
        if [ -d "${STORAGE_FAST}/derivatives" ]; then
            e_used=$(du -sh "${STORAGE_FAST}/derivatives" 2>/dev/null | cut -f1 || echo "N/A")
        fi
        
        if [ -d "${STORAGE_LARGE}/derivatives" ]; then
            f_used=$(du -sh "${STORAGE_LARGE}/derivatives" 2>/dev/null | cut -f1 || echo "N/A")
        fi
    fi
    
    log "INFO" "  BIDS drive (processing): $c_used"
    log "INFO" "  fast storage (outputs): $e_used"
    log "INFO" "  large storage (FreeSurfer): $f_used"
    
    # Free space remaining
    local c_free=$(df -h "$(dirname "$BIDS_DIR")" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    local e_free=$(df -h "$STORAGE_FAST" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    local f_free=$(df -h "$STORAGE_LARGE" 2>/dev/null | tail -1 | awk '{print $4}' || echo "N/A")
    
    log "INFO" "  Free space - C: $c_free, E: $e_free, F: $f_free"
    
    # Final recommendations
    log "INFO" ""
    log "INFO" "Post-Processing Recommendations:"
    
    if [ $n_successful -gt 0 ]; then
        log "INFO" "✅ Data Analysis Ready:"
        log "INFO" "  → DTI metrics: ${EXTERNAL_MRTRIX}/*_fa.nii.gz, *_md.nii.gz, etc."
        log "INFO" "  → NODDI parameters: ${EXTERNAL_MRTRIX}/*_ndi.nii.gz, *_odi.nii.gz, *_fwf.nii.gz"
        
        if [ "$RUN_CONNECTOME" = true ]; then
            log "INFO" "  → Connectomes: ${EXTERNAL_MRTRIX}/*_connectome_*.csv"
        fi
        
        log "INFO" "  → Quality reports: ${EXTERNAL_QC}/*_qc.txt"
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            log "INFO" "✅ ML-Enhanced Processing Applied"
            log "INFO" "  → Check ML quality reports for registration accuracy"
            log "INFO" "  → Enhanced data should show improved quality metrics"
        fi
        
        log "INFO" ""
        log "INFO" "Next Steps:"
        log "INFO" "1. Review quality control reports"
        log "INFO" "2. Perform visual inspection of parameter maps"
        log "INFO" "3. Extract ROI-based measurements"
        log "INFO" "4. Conduct statistical analyses"
        log "INFO" "5. Create publication-quality visualizations"
    fi
    
    if [ $n_failed -gt 0 ]; then
        log "WARN" ""
        log "WARN" "Failed Subjects Recovery:"
        log "WARN" "1. Review failure logs and fix underlying issues"
        log "WARN" "2. Re-run pipeline for failed subjects: --subject <sub-ID>"
        log "WARN" "3. Consider parameter adjustments for problematic cases"
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            log "WARN" "4. For ML failures, try --ml-method traditional or disable ML"
        fi
    fi
    
    # Performance analysis
    if [ $n_successful -gt 1 ]; then
        log "INFO" ""
        log "INFO" "Performance Analysis:"
        log "INFO" "  Processing rate: $([ "$total_duration" -gt 0 ] && echo "scale=2; $n_successful * 24 / ($total_duration / 3600)" | bc -l 2>/dev/null || echo "N/A") subjects/day"
        
        if [ "${USE_ML_REGISTRATION:-false}" = true ] && [ "${GPU_AVAILABLE:-false}" = true ]; then
            log "INFO" "  GPU acceleration: UTILIZED"
        elif [ "${USE_ML_REGISTRATION:-false}" = true ]; then
            log "INFO" "  GPU acceleration: NOT AVAILABLE (CPU-only ML processing)"
        fi
    fi
    
    # Final cleanup
    if [ "$CLEANUP" = true ]; then
        log "INFO" ""
        log "INFO" "Performing final cleanup..."
        
        # Clean up any remaining work files
        if [ -d "$WORK_DIR" ]; then
            find "$WORK_DIR" -name "*.tmp" -delete 2>/dev/null || true
            find "$WORK_DIR" -name "*.mif" -size +100M -delete 2>/dev/null || true
        fi
        
        # Compress old logs
        find "$LOG_DIR" -name "*.log" -mtime +1 -exec gzip {} \; 2>/dev/null || true
        
        log "INFO" "Cleanup completed"
    fi
    
    # Generate final pipeline report
    local final_report="${EXTERNAL_QC}/pipeline_final_report.txt"
    {
        echo "Integrated DTI Processing Pipeline - Final Report"
        echo "================================================"
        echo "Pipeline Version: $SCRIPT_VERSION"
        echo "Generated: $(date)"
        echo ""
        echo "PROCESSING SUMMARY:"
        echo "  Total subjects processed: $n_total"
        echo "  Successful: $n_successful"
        echo "  Failed: $n_failed"
        echo "  Success rate: $(( n_total > 0 ? n_successful * 100 / n_total : 0 ))%"
        echo "  Total processing time: ${total_hours}h ${total_minutes}m"
        echo ""
        echo "CONFIGURATION:"
        echo "  ML Registration: ${USE_ML_REGISTRATION:-false}"
        echo "  ML Method: ${ML_REGISTRATION_METHOD:-auto}"
        echo "  GPU Available: ${GPU_AVAILABLE:-false}"
        echo "  Connectivity Analysis: $RUN_CONNECTOME"
        echo "  Threads Used: $OMP_NUM_THREADS"
        echo ""
        echo "STAGE COMPLETION:"
        echo "  Stage 1 (Basic preprocessing): ${stage1_count}/${n_total}"
        echo "  Stage 2 (Post-hoc refinement): ${stage2_count}/${n_total}"
        echo "  Stage 3 (Connectivity): ${stage3_count}/${n_total}"
        echo "  Stage 4 (NODDI): ${stage4_count}/${n_total}"
        echo ""
        echo "STORAGE USAGE:"
        echo "  BIDS drive used: $c_used (free: $c_free)"
        echo "  fast storage used: $e_used (free: $e_free)" 
        echo "  large storage used: $f_used (free: $f_free)"
        echo ""
        
        if [ $n_failed -gt 0 ]; then
            echo "FAILED SUBJECTS:"
            echo "  ${failed_subjects[*]}"
            echo ""
        fi
        
        echo "OUTPUT LOCATIONS:"
        echo "  DTI metrics: ${EXTERNAL_MRTRIX}/"
        echo "  NODDI parameters: ${EXTERNAL_MRTRIX}/"
        echo "  Connectomes: ${EXTERNAL_MRTRIX}/"
        echo "  FreeSurfer: ${EXTERNAL_FS}/"
        echo "  Quality reports: ${EXTERNAL_QC}/"
        echo ""
        echo "PIPELINE COMPLETED: $(date)"
        
    } > "$final_report"
    
    log "INFO" "Final pipeline report saved: $final_report"
    
    # Generate HTML summary report (if not dry-run)
    if [ "$DRY_RUN" != true ] && [ $n_total -gt 0 ]; then
        _generate_html_report "$final_report" "$n_total" "$n_successful" "$n_failed"             "${total_hours}" "${total_minutes}" "${failed_subjects[*]:-}"
    fi
    
    # Final status message
    log "INFO" "================================================="
    if [ $n_failed -eq 0 ]; then
        log "OK" "🎉 PIPELINE COMPLETED SUCCESSFULLY!"
        log "OK" "All ${n_successful} subjects processed without errors"
    elif [ $n_successful -gt 0 ]; then
        log "OK" "✅ PIPELINE COMPLETED WITH PARTIAL SUCCESS"
        log "OK" "${n_successful} subjects successful, ${n_failed} subjects failed"
        log "WARN" "Review failure logs and consider reprocessing failed subjects"
    else
        log "ERROR" "❌ PIPELINE COMPLETED WITH ALL SUBJECTS FAILED"
        log "ERROR" "Review system requirements and input data quality"
    fi
    
    if [ "${USE_ML_REGISTRATION:-false}" = true ]; then
        log "INFO" "🧠 ML-enhanced processing was utilized for improved accuracy"
    fi
    
    log "INFO" "📊 Results ready for analysis in external storage locations"
    log "INFO" "📋 Comprehensive QC reports available for quality assessment"
    log "INFO" "================================================="
    
    # Return appropriate exit code
    return $([ $n_failed -gt 0 ] && echo 1 || echo 0)
}

# Script execution with error handling
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    # Enable comprehensive error handling
    set -euo pipefail
    
    # Set up signal handlers for clean shutdown
    trap 'log "WARN" "Pipeline interrupted by user"; cleanup_on_exit' INT TERM
    
    # Execute main function with all arguments
    main "$@"
    exit_code=$?
    
    # Final exit message
    if [ $exit_code -eq 0 ]; then
        echo ""
        echo "✅ Integrated DTI Processing Pipeline completed successfully!"
        echo "📁 Check output directories for processed data and quality reports"
    else
        echo ""
        echo "⚠️  Pipeline completed with some failures - check logs for details"
        echo "🔧 Consider reprocessing failed subjects or adjusting parameters"
    fi
    
    exit $exit_code
fi

# End of Integrated DTI Processing Pipeline v1.3-ml-enhanced